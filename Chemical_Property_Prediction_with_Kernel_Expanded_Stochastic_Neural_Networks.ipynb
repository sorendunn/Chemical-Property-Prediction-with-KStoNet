{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpI4XqTC9BgT"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOYmxVqnItBo",
        "outputId": "92a45381-6ca9-416c-a53b-b8d8675659d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'A-Kernel-Expanded-Stochastic-Neural-Network'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 11 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (11/11), 20.03 KiB | 6.68 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sylydya/A-Kernel-Expanded-Stochastic-Neural-Network.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jvr_bP5iKFF5"
      },
      "outputs": [],
      "source": [
        "!cp /content/A-Kernel-Expanded-Stochastic-Neural-Network/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "madiz1MDKQ8v",
        "outputId": "863f1ba7-306f-4b29-ba66-a89a8825cef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-08 23:17:15--  https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.nvidia.com/downloads/compute/cuda/9.0/prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb [following]\n",
            "--2023-12-08 23:17:15--  https://developer.nvidia.com/downloads/compute/cuda/9.0/prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
            "Reusing existing connection to developer.nvidia.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb?H92hr_Nkt9_Y75O54jw--8tRJvKuP3LNk1_QM9uOX8G5x64P32RbaNxZhQKZT6sNwVQFIGCr8FXHx90D0lZJsmdo9hsaVaOEqul5tJ5PcPyEh0RHGxBmMjJt4vlBLUh_x-kBXgC6SkQPmXJlho2hnTAsk4NdJ4gtkl-S0rIGf-2J2_YtKLJSFG-x-d8bIUMuuXvBFus60RT0xYM6tEJi [following]\n",
            "--2023-12-08 23:17:15--  https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb?H92hr_Nkt9_Y75O54jw--8tRJvKuP3LNk1_QM9uOX8G5x64P32RbaNxZhQKZT6sNwVQFIGCr8FXHx90D0lZJsmdo9hsaVaOEqul5tJ5PcPyEh0RHGxBmMjJt4vlBLUh_x-kBXgC6SkQPmXJlho2hnTAsk4NdJ4gtkl-S0rIGf-2J2_YtKLJSFG-x-d8bIUMuuXvBFus60RT0xYM6tEJi\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1216133170 (1.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu170 100%[===================>]   1.13G   376MB/s    in 3.1s    \n",
            "\n",
            "2023-12-08 23:17:19 (376 MB/s) - ‘cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb’ saved [1216133170/1216133170]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj-4PZN054bN",
        "outputId": "9eb59e30-b986-493d-b814-7b5636014512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A-Kernel-Expanded-Stochastic-Neural-Network\t    knn.py\t\t\tprocess_data.py\n",
            "confidence_interval.py\t\t\t\t    kstonet_covertype_split.py\tREADME.md\n",
            "cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb  kstonet_cross_validate.py\tsample_data\n",
            "drive\t\t\t\t\t\t    kstonet_parallel.py\n",
            "knn_cross_validate.py\t\t\t\t    kstonet.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hinUCwaJ5CqD",
        "outputId": "6624f0bd-f835-4075-cec2-9ba7b7795b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package cuda-repo-ubuntu1704-9-0-local.\n",
            "(Reading database ... 120899 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1704-9-0-local (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1704-9-0-local (9.0.176-1) ...\n",
            "\n",
            "The public CUDA GPG key does not appear to be installed.\n",
            "To install the key, run this command:\n",
            "sudo apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAGb9L-HOVT0",
        "outputId": "1ec54cdd-08e2-4df7-df7f-9a429f9a2504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7fa2af80.pub\n"
          ]
        }
      ],
      "source": [
        "!ls /var/cuda-repo-9-0-local | grep .pub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4KCWXYC5MhE",
        "outputId": "307204c0-f701-492e-d7df-35a8705e3c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbcmTiK86HZF",
        "outputId": "38a721ba-4378-4fa4-f8e1-070547acdf5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "\r0% [1 InRelease 0 B]\r                    \rIgn:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  Packages [15.8 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [633 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,036 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,304 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [28.5 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,512 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,572 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,282 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,576 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
            "Fetched 9,337 kB in 2s (5,713 kB/s)\n",
            "Reading package lists... Done\n",
            "W: file:/var/cuda-repo-9-0-local/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSUjGu6VQAaY",
        "outputId": "6e716c65-6af1-4eca-f025-14649a3e2b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libcuda-9.0-1' for regex 'cuda-9.0'\n",
            "Note, selecting 'cuda-9-0' for regex 'cuda-9.0'\n",
            "The following additional packages will be installed:\n",
            "  cpp-12 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0 cuda-drivers\n",
            "  cuda-drivers-545 cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "  dctrl-tools default-jre default-jre-headless dkms fakeroot fonts-dejavu-core\n",
            "  fonts-dejavu-extra freeglut3 freeglut3-dev gcc-12 keyboard-configuration\n",
            "  libasan8 libatk-wrapper-java libatk-wrapper-java-jni libegl-dev libfakeroot\n",
            "  libfontenc1 libgcc-12-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n",
            "  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n",
            "  libice-dev libjansson4 liblocale-gettext-perl libnvidia-cfg1-545\n",
            "  libnvidia-common-545 libnvidia-compute-545 libnvidia-decode-545\n",
            "  libnvidia-encode-545 libnvidia-extra-545 libnvidia-fbc1-545 libnvidia-gl-545\n",
            "  libopengl-dev libsm-dev libtsan2 libudev1 libxcvt0 libxfixes-dev libxfont2\n",
            "  libxi-dev libxkbfile1 libxmu-dev libxmu-headers libxt-dev libxtst6\n",
            "  libxxf86dga1 nvidia-compute-utils-545 nvidia-dkms-545 nvidia-driver-545\n",
            "  nvidia-kernel-common-545 nvidia-kernel-source-545 nvidia-modprobe\n",
            "  nvidia-prime nvidia-settings nvidia-utils-545 openjdk-11-jre python3-xkit\n",
            "  screen-resolution-extra systemd-hwe-hwdb udev x11-utils x11-xkb-utils xcvt\n",
            "  xfonts-base xfonts-encodings xfonts-utils xserver-common xserver-xorg-core\n",
            "  xserver-xorg-video-nvidia-545\n",
            "Suggested packages:\n",
            "  gcc-12-locales cpp-12-doc debtags menu gcc-12-multilib gcc-12-doc libice-doc\n",
            "  libsm-doc libxt-doc mesa-utils xfs | xserver xfonts-100dpi | xfonts-75dpi\n",
            "  xfonts-scalable\n",
            "Recommended packages:\n",
            "  libnvidia-compute-545:i386 libnvidia-decode-545:i386\n",
            "  libnvidia-encode-545:i386 libnvidia-fbc1-545:i386 libnvidia-gl-545:i386\n",
            "The following NEW packages will be installed:\n",
            "  cpp-12 cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0 cuda-drivers\n",
            "  cuda-drivers-545 cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "  dctrl-tools default-jre default-jre-headless dkms fakeroot fonts-dejavu-core\n",
            "  fonts-dejavu-extra freeglut3 freeglut3-dev gcc-12 keyboard-configuration\n",
            "  libasan8 libatk-wrapper-java libatk-wrapper-java-jni libegl-dev libfakeroot\n",
            "  libfontenc1 libgcc-12-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n",
            "  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n",
            "  libice-dev libjansson4 liblocale-gettext-perl libnvidia-cfg1-545\n",
            "  libnvidia-common-545 libnvidia-compute-545 libnvidia-decode-545\n",
            "  libnvidia-encode-545 libnvidia-extra-545 libnvidia-fbc1-545 libnvidia-gl-545\n",
            "  libopengl-dev libsm-dev libtsan2 libxcvt0 libxfixes-dev libxfont2 libxi-dev\n",
            "  libxkbfile1 libxmu-dev libxmu-headers libxt-dev libxtst6 libxxf86dga1\n",
            "  nvidia-compute-utils-545 nvidia-dkms-545 nvidia-driver-545\n",
            "  nvidia-kernel-common-545 nvidia-kernel-source-545 nvidia-modprobe\n",
            "  nvidia-prime nvidia-settings nvidia-utils-545 openjdk-11-jre python3-xkit\n",
            "  screen-resolution-extra systemd-hwe-hwdb udev x11-utils x11-xkb-utils xcvt\n",
            "  xfonts-base xfonts-encodings xfonts-utils xserver-common xserver-xorg-core\n",
            "  xserver-xorg-video-nvidia-545\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 110 newly installed, 0 to remove and 28 not upgraded.\n",
            "Need to get 344 MB/1,440 MB of archives.\n",
            "After this operation, 3,264 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-9-0-local  cuda-license-9-0 9.0.176-1 [22.0 kB]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  cuda-misc-headers-9-0 9.0.176-1 [684 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-common-545 545.23.08-0ubuntu1 [19.4 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-545 545.23.08-0ubuntu1 [48.8 MB]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  cuda-core-9-0 9.0.176-1 [16.9 MB]\n",
            "Get:6 file:/var/cuda-repo-9-0-local  cuda-cudart-9-0 9.0.176-1 [106 kB]\n",
            "Get:7 file:/var/cuda-repo-9-0-local  cuda-driver-dev-9-0 9.0.176-1 [10.9 kB]\n",
            "Get:8 file:/var/cuda-repo-9-0-local  cuda-cudart-dev-9-0 9.0.176-1 [767 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblocale-gettext-perl amd64 1.07-4build3 [17.1 kB]\n",
            "Get:10 file:/var/cuda-repo-9-0-local  cuda-command-line-tools-9-0 9.0.176-1 [25.4 MB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 keyboard-configuration all 1.205ubuntu3 [206 kB]\n",
            "Get:12 file:/var/cuda-repo-9-0-local  cuda-nvrtc-9-0 9.0.176-1 [6,348 kB]\n",
            "Get:13 file:/var/cuda-repo-9-0-local  cuda-nvrtc-dev-9-0 9.0.176-1 [9,334 B]\n",
            "Get:14 file:/var/cuda-repo-9-0-local  cuda-cusolver-9-0 9.0.176-1 [26.2 MB]\n",
            "Get:15 file:/var/cuda-repo-9-0-local  cuda-cusolver-dev-9-0 9.0.176-1 [5,317 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-decode-545 545.23.08-0ubuntu1 [1,712 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-encode-545 545.23.08-0ubuntu1 [93.2 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-fbc1-545 545.23.08-0ubuntu1 [53.0 kB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-gl-545 545.23.08-0ubuntu1 [150 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cpp-12 amd64 12.3.0-1ubuntu1~22.04 [10.8 MB]\n",
            "Get:21 file:/var/cuda-repo-9-0-local  cuda-cublas-9-0 9.0.176-1 [25.0 MB]\n",
            "Get:22 file:/var/cuda-repo-9-0-local  cuda-cublas-dev-9-0 9.0.176-1 [49.4 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libasan8 amd64 12.3.0-1ubuntu1~22.04 [2,442 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtsan2 amd64 12.3.0-1ubuntu1~22.04 [2,477 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgcc-12-dev amd64 12.3.0-1ubuntu1~22.04 [2,618 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gcc-12 amd64 12.3.0-1ubuntu1~22.04 [21.7 MB]\n",
            "Get:27 file:/var/cuda-repo-9-0-local  cuda-cufft-9-0 9.0.176-1 [84.1 MB]\n",
            "Get:28 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-compute-utils-545 545.23.08-0ubuntu1 [194 kB]\n",
            "Get:29 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-source-545 545.23.08-0ubuntu1 [43.4 MB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 dctrl-tools amd64 2.24-3build2 [66.9 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dkms all 2.8.7-2ubuntu2.2 [70.1 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.11 [78.0 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.11 [1,557 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjansson4 amd64 2.13.1-1.1build3 [32.4 kB]\n",
            "Get:35 file:/var/cuda-repo-9-0-local  cuda-cufft-dev-9-0 9.0.176-1 [73.7 MB]\n",
            "Get:36 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-common-545 545.23.08-0ubuntu1 [39.6 MB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:38 file:/var/cuda-repo-9-0-local  cuda-curand-9-0 9.0.176-1 [38.8 MB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-dkms-545 545.23.08-0ubuntu1 [33.4 kB]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-extra-545 545.23.08-0ubuntu1 [260 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-utils-545 545.23.08-0ubuntu1 [382 kB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-cfg1-545 545.23.08-0ubuntu1 [102 kB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  xserver-xorg-video-nvidia-545 545.23.08-0ubuntu1 [1,523 kB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-driver-545 545.23.08-0ubuntu1 [479 kB]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-modprobe 545.23.08-0ubuntu1 [21.3 kB]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-settings 545.23.08-0ubuntu1 [946 kB]\n",
            "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers-545 545.23.08-1 [2,634 B]\n",
            "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers 545.23.08-1 [2,502 B]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:59 file:/var/cuda-repo-9-0-local  cuda-curand-dev-9-0 9.0.176-1 [57.9 MB]\n",
            "Get:60 file:/var/cuda-repo-9-0-local  cuda-cusparse-9-0 9.0.176-1 [25.2 MB]\n",
            "Get:61 file:/var/cuda-repo-9-0-local  cuda-cusparse-dev-9-0 9.0.176-1 [25.3 MB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxmu-headers all 2:1.1.3-3 [54.1 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxmu-dev amd64 2:1.1.3-3 [54.6 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfixes-dev amd64 1:6.0.0-1 [12.2 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxi-dev amd64 2:1.8-1build1 [193 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:72 file:/var/cuda-repo-9-0-local  cuda-npp-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.21+9-0ubuntu1~22.04 [214 kB]\n",
            "Get:75 file:/var/cuda-repo-9-0-local  cuda-npp-dev-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:76 file:/var/cuda-repo-9-0-local  cuda-nvgraph-9-0 9.0.176-1 [6,081 kB]\n",
            "Get:77 file:/var/cuda-repo-9-0-local  cuda-nvgraph-dev-9-0 9.0.176-1 [5,658 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcvt0 amd64 0.1.1-3 [5,494 B]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-xorg-core amd64 2:21.1.4-2ubuntu1.7~22.04.2 [1,477 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-xkit all 0.5.0ubuntu5 [18.5 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu jammy/main amd64 screen-resolution-extra all 0.18.2 [4,396 B]\n",
            "Get:88 file:/var/cuda-repo-9-0-local  cuda-samples-9-0 9.0.176-1 [75.9 MB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
            "Get:90 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
            "Get:91 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:92 file:/var/cuda-repo-9-0-local  cuda-documentation-9-0 9.0.176-1 [53.1 MB]\n",
            "Get:93 file:/var/cuda-repo-9-0-local  cuda-libraries-dev-9-0 9.0.176-1 [2,596 B]\n",
            "Get:94 file:/var/cuda-repo-9-0-local  cuda-nvml-dev-9-0 9.0.176-1 [47.6 kB]\n",
            "Get:95 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:96 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:97 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:98 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:99 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:100 http://archive.ubuntu.com/ubuntu jammy/main amd64 nvidia-prime all 0.8.17.1 [9,956 B]\n",
            "Get:101 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.3 [2,908 B]\n",
            "Get:102 http://archive.ubuntu.com/ubuntu jammy/main amd64 xcvt amd64 0.1.1-3 [7,140 B]\n",
            "Get:103 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:104 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:105 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:106 file:/var/cuda-repo-9-0-local  cuda-visual-tools-9-0 9.0.176-1 [398 MB]\n",
            "Get:107 file:/var/cuda-repo-9-0-local  cuda-toolkit-9-0 9.0.176-1 [2,836 B]\n",
            "Get:108 file:/var/cuda-repo-9-0-local  cuda-libraries-9-0 9.0.176-1 [2,566 B]\n",
            "Get:109 file:/var/cuda-repo-9-0-local  cuda-runtime-9-0 9.0.176-1 [2,526 B]\n",
            "Get:110 file:/var/cuda-repo-9-0-local  cuda-demo-suite-9-0 9.0.176-1 [3,880 kB]\n",
            "Get:111 file:/var/cuda-repo-9-0-local  cuda-9-0 9.0.176-1 [2,552 B]\n",
            "Fetched 344 MB in 9s (39.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 111.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package liblocale-gettext-perl.\n",
            "(Reading database ... 120958 files and directories currently installed.)\n",
            "Preparing to unpack .../0-liblocale-gettext-perl_1.07-4build3_amd64.deb ...\n",
            "Unpacking liblocale-gettext-perl (1.07-4build3) ...\n",
            "Selecting previously unselected package keyboard-configuration.\n",
            "Preparing to unpack .../1-keyboard-configuration_1.205ubuntu3_all.deb ...\n",
            "Unpacking keyboard-configuration (1.205ubuntu3) ...\n",
            "Selecting previously unselected package cpp-12.\n",
            "Preparing to unpack .../2-cpp-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libasan8:amd64.\n",
            "Preparing to unpack .../3-libasan8_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libtsan2:amd64.\n",
            "Preparing to unpack .../4-libtsan2_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libgcc-12-dev:amd64.\n",
            "Preparing to unpack .../5-libgcc-12-dev_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package gcc-12.\n",
            "Preparing to unpack .../6-gcc-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package dctrl-tools.\n",
            "Preparing to unpack .../7-dctrl-tools_2.24-3build2_amd64.deb ...\n",
            "Unpacking dctrl-tools (2.24-3build2) ...\n",
            "Selecting previously unselected package dkms.\n",
            "Preparing to unpack .../8-dkms_2.8.7-2ubuntu2.2_all.deb ...\n",
            "Unpacking dkms (2.8.7-2ubuntu2.2) ...\n",
            "Preparing to unpack .../9-libudev1_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.11) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 121318 files and directories currently installed.)\n",
            "Preparing to unpack .../000-udev_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package libjansson4:amd64.\n",
            "Preparing to unpack .../001-libjansson4_2.13.1-1.1build3_amd64.deb ...\n",
            "Unpacking libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Selecting previously unselected package cuda-license-9-0.\n",
            "Preparing to unpack .../002-cuda-license-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-license-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-9-0.\n",
            "Preparing to unpack .../003-cuda-misc-headers-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-core-9-0.\n",
            "Preparing to unpack .../004-cuda-core-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-core-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-9-0.\n",
            "Preparing to unpack .../005-cuda-cudart-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-9-0.\n",
            "Preparing to unpack .../006-cuda-driver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-9-0.\n",
            "Preparing to unpack .../007-cuda-cudart-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-9-0.\n",
            "Preparing to unpack .../008-cuda-command-line-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "Preparing to unpack .../009-freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "Preparing to unpack .../010-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../011-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../012-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../013-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../014-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../015-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../016-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../017-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../018-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../019-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../020-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libice-dev:amd64.\n",
            "Preparing to unpack .../021-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n",
            "Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Selecting previously unselected package libsm-dev:amd64.\n",
            "Preparing to unpack .../022-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n",
            "Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../023-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package freeglut3-dev:amd64.\n",
            "Preparing to unpack .../024-freeglut3-dev_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libxmu-headers.\n",
            "Preparing to unpack .../025-libxmu-headers_2%3a1.1.3-3_all.deb ...\n",
            "Unpacking libxmu-headers (2:1.1.3-3) ...\n",
            "Selecting previously unselected package libxmu-dev:amd64.\n",
            "Preparing to unpack .../026-libxmu-dev_2%3a1.1.3-3_amd64.deb ...\n",
            "Unpacking libxmu-dev:amd64 (2:1.1.3-3) ...\n",
            "Selecting previously unselected package libxfixes-dev:amd64.\n",
            "Preparing to unpack .../027-libxfixes-dev_1%3a6.0.0-1_amd64.deb ...\n",
            "Unpacking libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Selecting previously unselected package libxi-dev:amd64.\n",
            "Preparing to unpack .../028-libxi-dev_2%3a1.8-1build1_amd64.deb ...\n",
            "Unpacking libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-9-0.\n",
            "Preparing to unpack .../029-cuda-nvrtc-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-9-0.\n",
            "Preparing to unpack .../030-cuda-nvrtc-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-9-0.\n",
            "Preparing to unpack .../031-cuda-cusolver-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-9-0.\n",
            "Preparing to unpack .../032-cuda-cusolver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-9-0.\n",
            "Preparing to unpack .../033-cuda-cublas-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-9-0.\n",
            "Preparing to unpack .../034-cuda-cublas-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-9-0.\n",
            "Preparing to unpack .../035-cuda-cufft-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-9-0.\n",
            "Preparing to unpack .../036-cuda-cufft-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-9-0.\n",
            "Preparing to unpack .../037-cuda-curand-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-9-0.\n",
            "Preparing to unpack .../038-cuda-curand-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-9-0.\n",
            "Preparing to unpack .../039-cuda-cusparse-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-9-0.\n",
            "Preparing to unpack .../040-cuda-cusparse-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-9-0.\n",
            "Preparing to unpack .../041-cuda-npp-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-9-0.\n",
            "Preparing to unpack .../042-cuda-npp-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-9-0.\n",
            "Preparing to unpack .../043-cuda-nvgraph-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-9-0.\n",
            "Preparing to unpack .../044-cuda-nvgraph-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-samples-9-0.\n",
            "Preparing to unpack .../045-cuda-samples-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-samples-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-documentation-9-0.\n",
            "Preparing to unpack .../046-cuda-documentation-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-9-0.\n",
            "Preparing to unpack .../047-cuda-libraries-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-9-0.\n",
            "Preparing to unpack .../048-cuda-nvml-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../049-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../050-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../051-openjdk-11-jre_11.0.21+9-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.21+9-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../052-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-visual-tools-9-0.\n",
            "Preparing to unpack .../053-cuda-visual-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-9-0.\n",
            "Preparing to unpack .../054-cuda-toolkit-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package libnvidia-common-545.\n",
            "Preparing to unpack .../055-libnvidia-common-545_545.23.08-0ubuntu1_all.deb ...\n",
            "Unpacking libnvidia-common-545 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-545:amd64.\n",
            "Preparing to unpack .../056-libnvidia-compute-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-decode-545:amd64.\n",
            "Preparing to unpack .../057-libnvidia-decode-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-decode-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-encode-545:amd64.\n",
            "Preparing to unpack .../058-libnvidia-encode-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-encode-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-fbc1-545:amd64.\n",
            "Preparing to unpack .../059-libnvidia-fbc1-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-fbc1-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-gl-545:amd64.\n",
            "Preparing to unpack .../060-libnvidia-gl-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "dpkg-query: no packages found matching libnvidia-gl-450\n",
            "Unpacking libnvidia-gl-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-compute-utils-545.\n",
            "Preparing to unpack .../061-nvidia-compute-utils-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-compute-utils-545 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-source-545.\n",
            "Preparing to unpack .../062-nvidia-kernel-source-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-source-545 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-common-545.\n",
            "Preparing to unpack .../063-nvidia-kernel-common-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-common-545 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-dkms-545.\n",
            "Preparing to unpack .../064-nvidia-dkms-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-dkms-545 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-extra-545:amd64.\n",
            "Preparing to unpack .../065-libnvidia-extra-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-extra-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-utils-545.\n",
            "Preparing to unpack .../066-nvidia-utils-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-utils-545 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-cfg1-545:amd64.\n",
            "Preparing to unpack .../067-libnvidia-cfg1-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-cfg1-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../068-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../069-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../070-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package libxcvt0:amd64.\n",
            "Preparing to unpack .../071-libxcvt0_0.1.1-3_amd64.deb ...\n",
            "Unpacking libxcvt0:amd64 (0.1.1-3) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../072-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../073-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package xserver-xorg-core.\n",
            "Preparing to unpack .../074-xserver-xorg-core_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xserver-xorg-video-nvidia-545.\n",
            "Preparing to unpack .../075-xserver-xorg-video-nvidia-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking xserver-xorg-video-nvidia-545 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-driver-545.\n",
            "Preparing to unpack .../076-nvidia-driver-545_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-driver-545 (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-modprobe.\n",
            "Preparing to unpack .../077-nvidia-modprobe_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-modprobe (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package python3-xkit.\n",
            "Preparing to unpack .../078-python3-xkit_0.5.0ubuntu5_all.deb ...\n",
            "Unpacking python3-xkit (0.5.0ubuntu5) ...\n",
            "Selecting previously unselected package screen-resolution-extra.\n",
            "Preparing to unpack .../079-screen-resolution-extra_0.18.2_all.deb ...\n",
            "Unpacking screen-resolution-extra (0.18.2) ...\n",
            "Selecting previously unselected package nvidia-settings.\n",
            "Preparing to unpack .../080-nvidia-settings_545.23.08-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-settings (545.23.08-0ubuntu1) ...\n",
            "Selecting previously unselected package cuda-drivers-545.\n",
            "Preparing to unpack .../081-cuda-drivers-545_545.23.08-1_amd64.deb ...\n",
            "Unpacking cuda-drivers-545 (545.23.08-1) ...\n",
            "Selecting previously unselected package cuda-drivers.\n",
            "Preparing to unpack .../082-cuda-drivers_545.23.08-1_amd64.deb ...\n",
            "Unpacking cuda-drivers (545.23.08-1) ...\n",
            "Selecting previously unselected package cuda-libraries-9-0.\n",
            "Preparing to unpack .../083-cuda-libraries-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-runtime-9-0.\n",
            "Preparing to unpack .../084-cuda-runtime-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-9-0.\n",
            "Preparing to unpack .../085-cuda-demo-suite-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-9-0.\n",
            "Preparing to unpack .../086-cuda-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package libfakeroot:amd64.\n",
            "Preparing to unpack .../087-libfakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fakeroot.\n",
            "Preparing to unpack .../088-fakeroot_1.28-1ubuntu1_amd64.deb ...\n",
            "Unpacking fakeroot (1.28-1ubuntu1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../089-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../090-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../091-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../092-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../093-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../094-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package nvidia-prime.\n",
            "Preparing to unpack .../095-nvidia-prime_0.8.17.1_all.deb ...\n",
            "Unpacking nvidia-prime (0.8.17.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../096-systemd-hwe-hwdb_249.11.3_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.3) ...\n",
            "Selecting previously unselected package xcvt.\n",
            "Preparing to unpack .../097-xcvt_0.1.1-3_amd64.deb ...\n",
            "Unpacking xcvt (0.1.1-3) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../098-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../099-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../100-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Setting up libxmu-headers (2:1.1.3-3) ...\n",
            "Setting up cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Setting up nvidia-prime (0.8.17.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libnvidia-compute-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.21+9-0ubuntu1~22.04) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libnvidia-decode-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Setting up libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
            "Setting up libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Setting up nvidia-modprobe (545.23.08-0ubuntu1) ...\n",
            "Setting up libnvidia-extra-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Setting up fakeroot (1.28-1ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
            "Setting up libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Setting up nvidia-kernel-source-545 (545.23.08-0ubuntu1) ...\n",
            "Setting up nvidia-utils-545 (545.23.08-0ubuntu1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libnvidia-common-545 (545.23.08-0ubuntu1) ...\n",
            "Setting up libnvidia-gl-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Setting up libnvidia-fbc1-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up nvidia-compute-utils-545 (545.23.08-0ubuntu1) ...\n",
            "Warning: The home dir /nonexistent you specified can't be accessed: No such file or directory\n",
            "Adding system user `nvidia-persistenced' (UID 104) ...\n",
            "Adding new group `nvidia-persistenced' (GID 107) ...\n",
            "Adding new user `nvidia-persistenced' (UID 104) with group `nvidia-persistenced' ...\n",
            "Not creating home directory `/nonexistent'.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/nvidia-persistenced.service → /lib/systemd/system/nvidia-persistenced.service.\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up udev (249.11-0ubuntu3.11) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.3) ...\n",
            "Setting up libnvidia-cfg1-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Setting up cuda-license-9-0 (9.0.176-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-9.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Setting up libxcvt0:amd64 (0.1.1-3) ...\n",
            "Setting up libnvidia-encode-545:amd64 (545.23.08-0ubuntu1) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-npp-9-0 (9.0.176-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Setting up libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Setting up python3-xkit (0.5.0ubuntu5) ...\n",
            "Setting up nvidia-kernel-common-545 (545.23.08-0ubuntu1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up liblocale-gettext-perl (1.07-4build3) ...\n",
            "Setting up dctrl-tools (2.24-3build2) ...\n",
            "Setting up cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-core-9-0 (9.0.176-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-9-0 (9.0.176-1) ...\n",
            "Setting up libxmu-dev:amd64 (2:1.1.3-3) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Setting up xcvt (0.1.1-3) ...\n",
            "Setting up cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Setting up screen-resolution-extra (0.18.2) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up nvidia-settings (545.23.08-0ubuntu1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up keyboard-configuration (1.205ubuntu3) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Configuring keyboard-configuration\n",
            "----------------------------------\n",
            "\n",
            "The layout of keyboards varies per country, with some countries having multiple\n",
            "common layouts. Please select the country of origin for the keyboard of this\n",
            "computer.\n",
            "\n",
            "  1. Afghani\n",
            "  2. Albanian\n",
            "  3. Amharic\n",
            "  4. Arabic\n",
            "  5. Arabic (Morocco)\n",
            "  6. Arabic (Syria)\n",
            "  7. Armenian\n",
            "  8. A user-defined custom Layout\n",
            "  9. Azerbaijani\n",
            "  10. Bambara\n",
            "  11. Bangla\n",
            "  12. Belarusian\n",
            "  13. Belgian\n",
            "  14. Berber (Algeria, Latin)\n",
            "  15. Bosnian\n",
            "  16. Braille\n",
            "  17. Bulgarian\n",
            "  18. Burmese\n",
            "  19. Chinese\n",
            "  20. Croatian\n",
            "  21. Czech\n",
            "  22. Danish\n",
            "  23. Dhivehi\n",
            "  24. Dutch\n",
            "  25. Dzongkha\n",
            "  26. English (Australian)\n",
            "  27. English (Cameroon)\n",
            "  28. English (Ghana)\n",
            "  29. English (Nigeria)\n",
            "  30. English (South Africa)\n",
            "  31. English (UK)\n",
            "  32. English (US)\n",
            "  33. Esperanto\n",
            "  34. Estonian\n",
            "  35. Faroese\n",
            "  36. Filipino\n",
            "  37. Finnish\n",
            "  38. French\n",
            "  39. French (Canada)\n",
            "  40. French (Democratic Republic of the Congo)\n",
            "  41. French (Togo)\n",
            "  42. Georgian\n",
            "  43. German\n",
            "  44. German (Austria)\n",
            "  45. Greek\n",
            "  46. Hebrew\n",
            "  47. Hungarian\n",
            "  48. Icelandic\n",
            "  49. Indian\n",
            "  50. Indonesian (Javanese)\n",
            "  51. Indonesian (Latin)\n",
            "  52. Iraqi\n",
            "  53. Irish\n",
            "  54. Italian\n",
            "  55. Japanese\n",
            "  56. Japanese (PC-98)\n",
            "  57. Kazakh\n",
            "  58. Khmer (Cambodia)\n",
            "  59. Korean\n",
            "  60. Kyrgyz\n",
            "  61. Lao\n",
            "  62. Latvian\n",
            "  63. Lithuanian\n",
            "  64. Macedonian\n",
            "  65. Malay (Jawi, Arabic Keyboard)\n",
            "  66. Maltese\n",
            "  67. Maori\n",
            "  68. Moldavian\n",
            "  69. Mongolian\n",
            "  70. Montenegrin\n",
            "  71. Nepali\n",
            "  72. NKo (AZERTY)\n",
            "  73. Norwegian\n",
            "  74. Persian\n",
            "  75. Polish\n",
            "  76. Portuguese\n",
            "  77. Portuguese (Brazil)\n",
            "  78. Romanian\n",
            "  79. Russian\n",
            "  80. Serbian\n",
            "  81. Sinhala (phonetic)\n",
            "  82. Slovak\n",
            "  83. Slovenian\n",
            "  84. Spanish\n",
            "  85. Spanish (Latin American)\n",
            "  86. Swahili (Kenya)\n",
            "  87. Swahili (Tanzania)\n",
            "  88. Swedish\n",
            "  89. Switzerland\n",
            "  90. Taiwanese\n",
            "  91. Tajik\n",
            "  92. Thai\n",
            "  93. Tswana\n",
            "  94. Turkish\n",
            "  95. Turkmen\n",
            "  96. Ukrainian\n",
            "  97. Urdu (Pakistan)\n",
            "  98. Uzbek\n",
            "  99. Vietnamese\n",
            "  100. Wolof\n",
            "\u001b[4mCountry of origin for the keyboard: \u001b[m\u001b[1m32\n",
            "\u001b[m\u001b[m\n",
            "Please select the layout matching the keyboard for this machine.\n",
            "\n",
            "  1. English (US)\n",
            "  2. English (US) - Cherokee\n",
            "  3. English (US) - English (classic Dvorak)\n",
            "  4. English (US) - English (Colemak)\n",
            "  5. English (US) - English (Colemak-DH)\n",
            "  6. English (US) - English (Colemak-DH ISO)\n",
            "  7. English (US) - English (Dvorak)\n",
            "  8. English (US) - English (Dvorak, alt. intl.)\n",
            "  9. English (US) - English (Dvorak, intl., with dead keys)\n",
            "  10. English (US) - English (Dvorak, left-handed)\n",
            "  11. English (US) - English (Dvorak, right-handed)\n",
            "  12. English (US) - English (intl., with AltGr dead keys)\n",
            "  13. English (US) - English (Macintosh)\n",
            "  14. English (US) - English (Norman)\n",
            "  15. English (US) - English (programmer Dvorak)\n",
            "  16. English (US) - English (the divide/multiply toggle the layout)\n",
            "  17. English (US) - English (US, alt. intl.)\n",
            "  18. English (US) - English (US, euro on 5)\n",
            "  19. English (US) - English (US, intl., with dead keys)\n",
            "  20. English (US) - English (US, Symbolic)\n",
            "  21. English (US) - English (Workman)\n",
            "  22. English (US) - English (Workman, intl., with dead keys)\n",
            "  23. English (US) - Hawaiian\n",
            "  24. English (US) - Russian (US, phonetic)\n",
            "  25. English (US) - Serbo-Croatian (US)\n",
            "\u001b[4mKeyboard layout: \u001b[m\u001b[1m1\n",
            "\u001b[m\u001b[m\n",
            "Your console font configuration will be updated the next time your system\n",
            "boots. If you want to update it now, run 'setupcon' from a virtual console.\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Setting up xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Setting up cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Setting up xserver-xorg-video-nvidia-545 (545.23.08-0ubuntu1) ...\n",
            "Setting up dkms (2.8.7-2ubuntu2.2) ...\n",
            "Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n",
            "Setting up cuda-samples-9-0 (9.0.176-1) ...\n",
            "Setting up nvidia-dkms-545 (545.23.08-0ubuntu1) ...\n",
            "\n",
            "A modprobe blacklist file has been created at /etc/modprobe.d to prevent Nouveau\n",
            "from loading. This can be reverted by deleting the following file:\n",
            "/etc/modprobe.d/nvidia-graphics-drivers.conf\n",
            "\n",
            "A new initrd image has also been created. To revert, please regenerate your\n",
            "initrd by running the following command after deleting the modprobe.d file:\n",
            "`/usr/sbin/initramfs -u`\n",
            "\n",
            "*****************************************************************************\n",
            "*** Reboot your computer and verify that the NVIDIA graphics driver can   ***\n",
            "*** be loaded.                                                            ***\n",
            "*****************************************************************************\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Loading new nvidia-545.23.08 DKMS files...\n",
            "It is likely that 5.15.120+ belongs to a chroot's host\n",
            "Building for 5.15.0-89-generic\n",
            "Building for architecture x86_64\n",
            "Building initial module for 5.15.0-89-generic\n",
            "Done.\n",
            "\n",
            "nvidia.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-89-generic/updates/dkms/\n",
            "\n",
            "nvidia-modeset.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-89-generic/updates/dkms/\n",
            "\n",
            "nvidia-drm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-89-generic/updates/dkms/\n",
            "\n",
            "nvidia-peermem.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-89-generic/updates/dkms/\n",
            "\n",
            "nvidia-uvm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/5.15.0-89-generic/updates/dkms/\n",
            "\n",
            "depmod...\n",
            "Setting up cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Setting up nvidia-driver-545 (545.23.08-0ubuntu1) ...\n",
            "Setting up cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-drivers-545 (545.23.08-1) ...\n",
            "Setting up cuda-drivers (545.23.08-1) ...\n",
            "Setting up cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-9-0 (9.0.176-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install cuda-9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6XW8EiIQkFx",
        "outputId": "11cb7e1a-1034-44a9-fbc2-c8cd13a7921b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting thundersvm\n",
            "  Downloading thundersvm-0.3.12-py3-none-any.whl (507 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/507.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/507.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.4/507.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from thundersvm) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->thundersvm) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->thundersvm) (3.2.0)\n",
            "Installing collected packages: thundersvm\n",
            "Successfully installed thundersvm-0.3.12\n"
          ]
        }
      ],
      "source": [
        "!pip install thundersvm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CEaw0GkjPYO-"
      },
      "outputs": [],
      "source": [
        "from thundersvm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpnlgzLR6Fkc",
        "outputId": "81c44aa1-19a0-4c00-d966-903c074d3a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive/MyDrive/result/hiv/0/undersampled-dataset/’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir drive/MyDrive/result/hiv/0/undersampled-dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Ni7Zs36QUS",
        "outputId": "3fb6b427-07e3-4250-a1a1-9f01deab3c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive/MyDrive/result/hiv/0/full-dataset/’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir drive/MyDrive/result/hiv/0/full-dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRFuScd16TBk",
        "outputId": "f266f3dc-1ced-4d44-e347-6aaec5654537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive/MyDrive/result/hiv/0/pca-100-undersampled-dataset/’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir drive/MyDrive/result/hiv/0/pca-100-undersampled-dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXposJCM6Xfj",
        "outputId": "8c228155-2a3f-48d9-f1d5-7818823a3144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘drive/MyDrive/result/hiv/0/pca-100-full-dataset/’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir drive/MyDrive/result/hiv/0/pca-100-full-dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsIiwZGo6aS2",
        "outputId": "a255b690-972d-4283-c77e-aa5a7daba465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'drive/MyDrive/result/hiv/0/20*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv drive/MyDrive/result/hiv/0/20* drive/MyDrive/result/hiv/0/full-dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHemwg3gDfIk",
        "outputId": "de1ff021-a9be-456a-9e3d-9512e5ece4df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.2.dev20231208230844-py3-none-any.whl (915 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.4/915.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.12)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.11.4)\n",
            "Collecting rdkit (from deepchem)\n",
            "  Downloading rdkit-2023.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.3.post1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->deepchem) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: rdkit, deepchem\n",
            "Successfully installed deepchem-2.7.2.dev20231208230844 rdkit-2023.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre deepchem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wdNlx2OaXNN",
        "outputId": "8998f8d5-f9aa-4afc-fbbe-de0e76a8dc09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for SPS. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for AvgIpc. Feature removed!\n",
            "WARNING:deepchem.models:Skipped loading some PyTorch models, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ],
      "source": [
        "import deepchem as dc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-kI-NSp1YTg"
      },
      "source": [
        "## Editing Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duYjcDe61ju9",
        "outputId": "569e258e-fd57-45b1-99ff-bc664c01e009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting kstonet.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile kstonet.py\n",
        "import argparse\n",
        "import os\n",
        "import errno\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import torch.utils.data\n",
        "import time\n",
        "\n",
        "from thundersvm import SVR\n",
        "# from sklearn.svm import SVR\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from process_data import preprocess_data\n",
        "\n",
        "# Basic Setting\n",
        "parser = argparse.ArgumentParser(description='Running K-StoNet')\n",
        "parser.add_argument('--seed', default=0, type=int, help='set seed')\n",
        "parser.add_argument('--data_name', default = 'Boston', type = str, help='specify the name of the data')\n",
        "parser.add_argument('--base_path', default='./result/', type=str, help='base path for saving result')\n",
        "\n",
        "parser.add_argument('--model_path', default='test_run/', type=str, help='folder name for saving model')\n",
        "parser.add_argument('--cross_validate', default=0, type = int, help='specify which fold of 5 fold cross validation')\n",
        "parser.add_argument('--regression_flag', default=True, type=int, help='true for regression and false for classification')\n",
        "parser.add_argument('--normalize_y_flag', default=False, type=int, help='whether to normalize target value or not')\n",
        "parser.add_argument('--confidence_interval_flag', default=False, type=int, help='whether to store result to compute confidence interval')\n",
        "\n",
        "# model\n",
        "parser.add_argument('--layer', default=1, type=int, help='number of hidden layer')\n",
        "parser.add_argument('--unit', default = [10], type=int, nargs='+', help='number of hidden unit in each layer')\n",
        "parser.add_argument('--sigma', default=[0.001], type=float, nargs='+', help='variance of each layer for the hidden variable model')\n",
        "parser.add_argument('--C', default=5.0, type=float, help='C in SVR')\n",
        "parser.add_argument('--epsilon', default=0.01, type=float, help='epsilon in SVR')\n",
        "\n",
        "\n",
        "\n",
        "# Training Setting\n",
        "parser.add_argument('--nepoch', default=50, type=int, help='total number of training epochs')\n",
        "parser.add_argument('--MH_step', default=25, type=int, help='SGLD step for imputation')\n",
        "parser.add_argument('--lr', default=[0.00005], type=float, nargs='+', help='step size in imputation')\n",
        "parser.add_argument('--alpha', default=0.1, type=float, help='momentum parameter for HMC')\n",
        "parser.add_argument('--temperature', default=[1], type=float, nargs='+', help='temperature parameter for HMC')\n",
        "parser.add_argument('--lasso', default=[0.0001], type=float, nargs='+', help='lambda parameter for LASSO')\n",
        "parser.add_argument('--p_gamma', default=None, type=float, help='inverse of gamma in the kernel')\n",
        "parser.add_argument('--kernel', default='rbf', type=str,  help='kernel function of svr')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_hidden, hidden_dim, output_dim):\n",
        "        super(Net, self).__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.fc_list = []\n",
        "        for i in range(num_hidden - 1):\n",
        "            self.fc_list.append(nn.Linear(hidden_dim[i], hidden_dim[i+1]))\n",
        "            self.add_module('fc'+str(i+2), self.fc_list[-1])\n",
        "        self.fc_list.append(nn.Linear(hidden_dim[-1], output_dim))\n",
        "        self.add_module('fc' + str(num_hidden + 1), self.fc_list[-1])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(x)\n",
        "        for i in range(self.num_hidden - 1):\n",
        "            x = torch.tanh(self.fc_list[i](x))\n",
        "        x = self.fc_list[-1](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def main():\n",
        "    import pickle\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    data_name = args.data_name\n",
        "    cross_validate_index = args.cross_validate\n",
        "\n",
        "    num_hidden = args.layer\n",
        "    hidden_dim = args.unit\n",
        "\n",
        "    regression_flag = args.regression_flag\n",
        "    normalize_y_flag = args.normalize_y_flag\n",
        "\n",
        "    num_epochs = args.nepoch\n",
        "\n",
        "    x_train, y_train, x_test, y_test = preprocess_data(data_name, cross_validate_index, seed = args.seed)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    ntrain = x_train.shape[0]\n",
        "    ntest = x_test.shape[0]\n",
        "    dim = x_train.shape[1]\n",
        "\n",
        "\n",
        "    sse = nn.MSELoss(reduction='sum')\n",
        "    if regression_flag:\n",
        "        output_dim = 1\n",
        "        loss_func = nn.MSELoss()\n",
        "        loss_func_sum = nn.MSELoss(reduction='sum')\n",
        "        train_loss_path = np.zeros(num_epochs)\n",
        "        test_loss_path = np.zeros(num_epochs)\n",
        "        if normalize_y_flag:\n",
        "            y_train_mean = y_train.mean()\n",
        "            y_train_std = y_train.std()\n",
        "            y_train = (y_train - y_train_mean) / y_train_std\n",
        "    else:\n",
        "        output_dim = int((y_test.max() + 1).item())\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        loss_func_sum = nn.CrossEntropyLoss(reduction='sum')\n",
        "        train_loss_path = np.zeros(num_epochs)\n",
        "        test_loss_path = np.zeros(num_epochs)\n",
        "        train_accuracy_path = np.zeros(num_epochs)\n",
        "        test_accuracy_path = np.zeros(num_epochs)\n",
        "        train_auc_roc_path = np.zeros(num_epochs)\n",
        "        test_auc_roc_path = np.zeros(num_epochs)\n",
        "    time_used_path = np.zeros(num_epochs)\n",
        "\n",
        "    net = Net(num_hidden, hidden_dim, output_dim)\n",
        "    net.to(device)\n",
        "\n",
        "    PATH = args.base_path + data_name + '/' + str(cross_validate_index) + '/' + args.model_path\n",
        "    if not os.path.isdir(PATH):\n",
        "        try:\n",
        "            os.makedirs(PATH)\n",
        "        except OSError as exc:  # Python >2.5\n",
        "            if exc.errno == errno.EEXIST and os.path.isdir(PATH):\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    proposal_lr = args.lr\n",
        "    sigma_list = args.sigma\n",
        "    temperature = args.temperature\n",
        "    lasso_lambda = args.lasso\n",
        "\n",
        "    if len(proposal_lr) == 1 and num_hidden > 1:\n",
        "        temp_proposal_lr = proposal_lr[0]\n",
        "        proposal_lr = []\n",
        "        for i in range(num_hidden):\n",
        "            proposal_lr.append(temp_proposal_lr)\n",
        "\n",
        "    if len(sigma_list) == 1 and num_hidden > 1:\n",
        "        temp_sigma_list = sigma_list[0]\n",
        "        sigma_list = []\n",
        "        for i in range(num_hidden):\n",
        "            sigma_list.append(temp_sigma_list)\n",
        "\n",
        "    if len(temperature) == 1 and num_hidden > 1:\n",
        "        temp_temperature = temperature[0]\n",
        "        temperature = []\n",
        "        for i in range(num_hidden):\n",
        "            temperature.append(temp_temperature)\n",
        "\n",
        "    if len(lasso_lambda) == 1 and num_hidden > 1:\n",
        "        temp_lasso_lambda = lasso_lambda[0]\n",
        "        lasso_lambda = []\n",
        "        for i in range(num_hidden):\n",
        "            lasso_lambda.append(temp_lasso_lambda)\n",
        "\n",
        "    C = args.C\n",
        "    epsilon = args.epsilon\n",
        "\n",
        "    temp_init = nn.Linear(dim, hidden_dim[0])\n",
        "    temp_init.to(device)\n",
        "    svr_output_init = temp_init(x_train)\n",
        "\n",
        "    if args.p_gamma is None:\n",
        "        p_gamma = x_train.shape[1]\n",
        "    else:\n",
        "        p_gamma = args.p_gamma\n",
        "\n",
        "\n",
        "    kernel = args.kernel\n",
        "    with torch.no_grad():\n",
        "        svr_list = []\n",
        "        for i in range(hidden_dim[0]):\n",
        "            temp = SVR(C=C, epsilon=epsilon, gamma=1.0/p_gamma, kernel = kernel)\n",
        "            svr_list.append(temp)\n",
        "\n",
        "    svr_out_test = np.zeros([ntest, hidden_dim[0]])\n",
        "\n",
        "    svr_out_train = svr_output_init.data.cpu().numpy()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        start_time = time.process_time()\n",
        "\n",
        "        hidden_list = []\n",
        "        momentum_list = []\n",
        "        with torch.no_grad():\n",
        "            hidden_list.append(torch.FloatTensor(svr_out_train).to(device))\n",
        "            momentum_list.append(torch.zeros_like(hidden_list[-1]))\n",
        "            for i in range(num_hidden - 1):\n",
        "                hidden_list.append(net.fc_list[i](torch.tanh(hidden_list[-1])))\n",
        "                momentum_list.append(torch.zeros_like(hidden_list[-1]))\n",
        "\n",
        "        for i in range(hidden_list.__len__()):\n",
        "            hidden_list[i].requires_grad = True\n",
        "\n",
        "        MH_step = args.MH_step\n",
        "        alpha = args.alpha\n",
        "\n",
        "        forward_hidden = torch.FloatTensor(svr_out_train).to(device)\n",
        "\n",
        "        for repeat in range(MH_step):\n",
        "            for layer_index in reversed(range(num_hidden)):\n",
        "                if hidden_list[layer_index].grad is not None:\n",
        "                    hidden_list[layer_index].grad.zero_()\n",
        "\n",
        "                if layer_index == num_hidden -1:\n",
        "                    hidden_likelihood = -loss_func_sum(net.fc_list[layer_index](torch.tanh(hidden_list[layer_index])), y_train) / sigma_list[layer_index]\n",
        "                else:\n",
        "                    hidden_likelihood = -sse(net.fc_list[layer_index](torch.tanh(hidden_list[layer_index])), hidden_list[layer_index + 1]) / sigma_list[layer_index]\n",
        "                if layer_index == 0:\n",
        "                    hidden_likelihood = hidden_likelihood - C * torch.where((hidden_list[layer_index] - forward_hidden).abs() - epsilon > 0, (hidden_list[layer_index] - forward_hidden).abs() - epsilon, torch.zeros_like(hidden_list[0])).sum()\n",
        "                else:\n",
        "                    hidden_likelihood = hidden_likelihood - sse(net.fc_list[layer_index - 1](torch.tanh(hidden_list[layer_index - 1])), hidden_list[layer_index]) / sigma_list[layer_index - 1]\n",
        "                hidden_likelihood.backward()\n",
        "                step_proposal_lr = proposal_lr[layer_index]\n",
        "                with torch.no_grad():\n",
        "                    momentum_list[layer_index] = (1 - alpha) * momentum_list[layer_index] + step_proposal_lr / 2 * hidden_list[\n",
        "                        layer_index].grad + torch.FloatTensor(\n",
        "                        hidden_list[layer_index].shape).to(device).normal_().mul(\n",
        "                        np.sqrt(alpha * step_proposal_lr * temperature[layer_index]))\n",
        "                    hidden_list[layer_index].data += momentum_list[layer_index]\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(hidden_dim[0]):\n",
        "                svr_list[i].fit(x_train.cpu(), hidden_list[0][:, i].cpu().detach())\n",
        "\n",
        "        for layer_index in range(num_hidden):\n",
        "            if layer_index == num_hidden - 1:\n",
        "                if regression_flag:\n",
        "                    clf = Lasso(alpha=lasso_lambda[layer_index], max_iter=-1).fit(\n",
        "                        torch.tanh(hidden_list[layer_index]).cpu().detach(), y_train.cpu())\n",
        "                    net.fc_list[layer_index].weight.data = torch.FloatTensor(clf.coef_).reshape(\n",
        "                        net.fc_list[layer_index].weight.shape).to(device)\n",
        "                    net.fc_list[layer_index].bias.data = torch.FloatTensor(np.array(clf.intercept_)).reshape(\n",
        "                        net.fc_list[layer_index].bias.shape).to(device)\n",
        "                else:\n",
        "                    clf = LogisticRegression(penalty='l1', C=1/lasso_lambda[layer_index], solver='saga', max_iter=10000, multi_class='multinomial', n_jobs=-1).fit(torch.tanh(hidden_list[layer_index]).cpu().detach(), y_train.cpu())\n",
        "                    if output_dim == 2:\n",
        "                        net.fc_list[layer_index].weight.data = torch.FloatTensor(np.vstack([-clf.coef_, clf.coef_])).reshape(\n",
        "                            net.fc_list[layer_index].weight.shape).to(device)\n",
        "                        net.fc_list[layer_index].bias.data = torch.FloatTensor(np.array([-clf.intercept_, clf.intercept_])).reshape(\n",
        "                            net.fc_list[layer_index].bias.shape).to(device)\n",
        "                    else:\n",
        "                        net.fc_list[layer_index].weight.data = torch.FloatTensor(clf.coef_).reshape(\n",
        "                            net.fc_list[layer_index].weight.shape).to(device)\n",
        "                        net.fc_list[layer_index].bias.data = torch.FloatTensor(clf.intercept_).reshape(\n",
        "                            net.fc_list[layer_index].bias.shape).to(device)\n",
        "\n",
        "            else:\n",
        "\n",
        "                clf = Lasso(alpha=lasso_lambda[layer_index], max_iter=-1).fit(\n",
        "                    torch.tanh(hidden_list[layer_index]).cpu().detach(), hidden_list[layer_index + 1].cpu().detach())\n",
        "                net.fc_list[layer_index].weight.data = torch.FloatTensor(clf.coef_).reshape(net.fc_list[layer_index].weight.shape).to(device)\n",
        "                net.fc_list[layer_index].bias.data = torch.FloatTensor(clf.intercept_).reshape(net.fc_list[layer_index].bias.shape).to(device)\n",
        "\n",
        "        end_time = time.process_time()\n",
        "        time_used_path[epoch] = end_time - start_time\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if regression_flag:\n",
        "                print('epoch: ', epoch)\n",
        "                for i in range(hidden_dim[0]):\n",
        "                    svr_out_train[:, i] = svr_list[i].predict(x_train.cpu())\n",
        "\n",
        "                output = net(torch.FloatTensor(svr_out_train).to(device))\n",
        "                train_loss = loss_func(output, y_train)\n",
        "\n",
        "                if normalize_y_flag:\n",
        "                    train_loss = train_loss * y_train_std * y_train_std\n",
        "                train_loss_path[epoch] = train_loss\n",
        "                print(\"train loss: \", train_loss)\n",
        "\n",
        "                for i in range(hidden_dim[0]):\n",
        "                    svr_out_test[:, i] = svr_list[i].predict(x_test.cpu())\n",
        "                output = net(torch.FloatTensor(svr_out_test).to(device))\n",
        "\n",
        "                if normalize_y_flag:\n",
        "                    output = output * y_train_std + y_train_mean\n",
        "                test_loss = loss_func(output, y_test)\n",
        "                test_loss_path[epoch] = test_loss\n",
        "                print(\"test loss: \", test_loss)\n",
        "\n",
        "            else:\n",
        "                print('epoch:', epoch)\n",
        "\n",
        "                # Calculate SVR outputs and metrics for the training set\n",
        "                for i in range(hidden_dim[0]):\n",
        "                    svr_out_train[:, i] = svr_list[i].predict(x_train.cpu())\n",
        "\n",
        "                output = net(torch.FloatTensor(svr_out_train).to(device))\n",
        "                train_loss = loss_func(output, y_train)\n",
        "\n",
        "                # Assuming that the second column of `output` corresponds to the positive class probability\n",
        "                train_probabilities = torch.softmax(output, dim=1)[:, 1].cpu().detach().numpy()\n",
        "                train_prediction = output.data.max(1)[1]\n",
        "                train_accuracy = train_prediction.eq(y_train.data).sum().item() / ntrain\n",
        "                train_auc_roc = roc_auc_score(y_train.cpu().numpy(), train_probabilities)\n",
        "\n",
        "                train_loss_path[epoch] = train_loss\n",
        "                train_accuracy_path[epoch] = train_accuracy\n",
        "                train_auc_roc_path[epoch] = train_auc_roc\n",
        "\n",
        "                # Calculate SVR outputs and metrics for the test set\n",
        "                for i in range(hidden_dim[0]):\n",
        "                    svr_out_test[:, i] = svr_list[i].predict(x_test.cpu())\n",
        "\n",
        "                output = net(torch.FloatTensor(svr_out_test).to(device))\n",
        "                test_loss = loss_func(output, y_test)\n",
        "\n",
        "                # Assuming that the second column of `output` corresponds to the positive class probability\n",
        "                test_probabilities = torch.softmax(output, dim=1)[:, 1].cpu().detach().numpy()\n",
        "                test_prediction = output.data.max(1)[1]\n",
        "                test_accuracy = test_prediction.eq(y_test.data).sum().item() / ntest\n",
        "                test_auc_roc = roc_auc_score(y_test.cpu().numpy(), test_probabilities)\n",
        "\n",
        "                test_loss_path[epoch] = test_loss\n",
        "                test_accuracy_path[epoch] = test_accuracy\n",
        "                test_auc_roc_path[epoch] = test_auc_roc\n",
        "\n",
        "                print(\"train_loss:\", train_loss,\n",
        "                      \"train_accuracy:\", test_accuracy,\n",
        "                      'train AUC-ROC:', train_auc_roc,\n",
        "                      \"test loss:\", test_loss,\n",
        "                      'test accuracy:', test_accuracy,\n",
        "                      'test AUC-ROC:', test_auc_roc)\n",
        "\n",
        "\n",
        "        torch.save(net.state_dict(), PATH + 'model' + str(epoch) + '.pt')\n",
        "\n",
        "        for i in range(hidden_dim[0]):\n",
        "            filename = PATH + 'model_svr' + str(epoch) + '_' + str(i) + '.pt'\n",
        "            f = open(filename, 'wb')\n",
        "            pickle.dump(svr_list[i], f, protocol=4)\n",
        "            f.close()\n",
        "\n",
        "        if args.confidence_interval_flag:\n",
        "            filename = PATH + 'hidden_state' + str(epoch) + '.pt'\n",
        "            f = open(filename, 'wb')\n",
        "            pickle.dump(hidden_list, f, protocol=4)\n",
        "            f.close()\n",
        "\n",
        "        time_used_path[epoch] = end_time - start_time\n",
        "\n",
        "        if regression_flag:\n",
        "            filename = PATH + 'result.txt'\n",
        "            f = open(filename, 'wb')\n",
        "            pickle.dump([train_loss_path, test_loss_path, time_used_path], f)\n",
        "            f.close()\n",
        "        else:\n",
        "            filename = PATH + 'result.txt'\n",
        "            f = open(filename, 'wb')\n",
        "            pickle.dump([train_loss_path, test_loss_path, train_accuracy_path, test_accuracy_path, test_auc_roc_path, train_auc_roc_path, time_used_path], f)\n",
        "            f.close()\n",
        "        if args.confidence_interval_flag:\n",
        "            filename = PATH + 'data.txt'\n",
        "            f = open(filename, 'wb')\n",
        "            pickle.dump(\n",
        "                [x_train, x_test, y_train, y_test], f)\n",
        "            f.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7MaoJtRaA9q",
        "outputId": "600e41a5-4ab2-4386-b0b2-d5bb361d869f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing run_in_parallel.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_in_parallel.sh\n",
        "#!/bin/bash\n",
        "\n",
        "# Function to run the command with a specific solve_index\n",
        "run_command() {\n",
        "  local idx=$1\n",
        "  echo \"Running with solve_index ${idx}\"\n",
        "  python kstonet_covertype_split.py --layer 1 --data_name hiv --unit 50 --load_epoch -1 --solve_index $idx\n",
        "  echo \"Process with solve_index ${idx} finished\"\n",
        "}\n",
        "\n",
        "# Loop over the required solve_index range (0 to 49)\n",
        "for i in $(seq 0 49); do\n",
        "   # Run the run_command function in the background\n",
        "   run_command $i &\n",
        "\n",
        "   # Get the number of currently running jobs\n",
        "   num_jobs=`jobs -p | wc -l`\n",
        "\n",
        "   # If there are 3 or more jobs running, wait for one to finish\n",
        "   while [ $num_jobs -ge 10 ]; do\n",
        "      sleep 1\n",
        "      num_jobs=`jobs -p | wc -l`\n",
        "   done\n",
        "done\n",
        "\n",
        "# Wait for all background jobs to complete before ending the script\n",
        "wait\n",
        "echo \"All processes are complete.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhAboy3tcOTU"
      },
      "source": [
        "Adding processesing for HIV dataset to process_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0rDgJFjcL-d",
        "outputId": "286b95f7-83f3-473d-a7d7-9c4671732622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting process_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile process_data.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "\n",
        "def compute_distance(X, Y):\n",
        "    n_1, n_2 = X.size(0), Y.size(0)\n",
        "\n",
        "    norms_1 = torch.sum(X ** 2, dim=1, keepdim=True)\n",
        "    norms_2 = torch.sum(Y ** 2, dim=1, keepdim=True)\n",
        "    norms = (norms_1.expand(n_1, n_2) + norms_2.transpose(0, 1).expand(n_1, n_2))\n",
        "    distances_squared = norms - 2 * X @ Y.T\n",
        "\n",
        "    return torch.abs(distances_squared)\n",
        "\n",
        "def RBF_kernel(x, y , gamma):\n",
        "    distance = compute_distance(x, y)\n",
        "    kernel = torch.exp(-gamma * distance)\n",
        "    return kernel\n",
        "\n",
        "\n",
        "def preprocess_data(data_name, cross_validate_index, seed = 1):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if data_name == 'global_optimal':\n",
        "        # device = torch.device(\"cpu\")\n",
        "        TotalP = 5\n",
        "        NTrain = 5000\n",
        "        n_unit = 5\n",
        "        second_weight = np.mat(np.random.normal(0,1,[n_unit, 1]))\n",
        "        second_bias = np.random.normal(0,1,1)\n",
        "\n",
        "        x_train = np.mat(np.zeros([NTrain, TotalP]))\n",
        "        sigma = 1.0\n",
        "        for i in range(NTrain):\n",
        "            ee = np.sqrt(sigma) * np.random.normal(0, 1)\n",
        "            for j in range(TotalP):\n",
        "                x_train[i, j] = (ee + np.sqrt(sigma) * np.random.normal(0, 1)) / np.sqrt(2.0)\n",
        "\n",
        "        C = 5\n",
        "        epsilon = 0.01\n",
        "        p_gamma = 1 / (x_train.shape[1] * x_train.var())\n",
        "\n",
        "        x_train_hidden = np.mat(np.random.normal(0, 1, [NTrain, n_unit]))\n",
        "\n",
        "        svr_input_train = RBF_kernel(torch.FloatTensor(x_train), torch.FloatTensor(x_train), p_gamma)\n",
        "        svr_input_train = svr_input_train.numpy()\n",
        "\n",
        "        dual_coef = np.mat(np.zeros([NTrain, n_unit]))\n",
        "        dual_bias = np.zeros([n_unit])\n",
        "\n",
        "        for i in range(n_unit):\n",
        "            temp_svr = SVR(C=C, epsilon=epsilon)\n",
        "            temp_svr.fit(x_train, x_train_hidden[:, i])\n",
        "\n",
        "            dual_coef[temp_svr.support_, i] = temp_svr.dual_coef_\n",
        "            dual_bias[i] = temp_svr.intercept_\n",
        "\n",
        "        x_train_forward_hidden = np.matmul(svr_input_train, dual_coef) + dual_bias\n",
        "\n",
        "        y_train = np.matmul(x_train_forward_hidden, second_weight) + second_bias + np.random.normal(0, 1, [NTrain, 1])\n",
        "\n",
        "        NTest = 5000\n",
        "        x_test = np.mat(np.zeros([NTest, TotalP]))\n",
        "        for i in range(NTest):\n",
        "            ee = np.sqrt(sigma) * np.random.normal(0, 1)\n",
        "            for j in range(TotalP):\n",
        "                x_test[i, j] = (ee + np.sqrt(sigma) * np.random.normal(0, 1)) / np.sqrt(2.0)\n",
        "\n",
        "        svr_input_test = RBF_kernel(torch.FloatTensor(x_test), torch.FloatTensor(x_train), p_gamma)\n",
        "\n",
        "        svr_input_test = svr_input_test.numpy()\n",
        "\n",
        "        x_test_forward_hidden = np.matmul(svr_input_test, dual_coef) + dual_bias\n",
        "\n",
        "        y_test = np.matmul(x_test_forward_hidden, second_weight) + second_bias + np.random.normal(0, 1, [NTest, 1])\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "\n",
        "    if data_name == 'measurement_error':\n",
        "        TotalP = 5\n",
        "        NTrain = 500\n",
        "        x_train = np.matrix(np.zeros([NTrain, TotalP]))\n",
        "        y_train = np.matrix(np.zeros([NTrain, 1]))\n",
        "\n",
        "        sigma = 1.0\n",
        "        for i in range(NTrain):\n",
        "            ee = np.sqrt(sigma) * np.random.normal(0, 1)\n",
        "            for j in range(TotalP):\n",
        "                x_train[i, j] = (ee + np.sqrt(sigma) * np.random.normal(0, 1)) / np.sqrt(2.0)\n",
        "            x0 = x_train[i, 0]\n",
        "            x1 = x_train[i, 1]\n",
        "            x2 = x_train[i, 2]\n",
        "            x3 = x_train[i, 3]\n",
        "            x4 = x_train[i, 4]\n",
        "\n",
        "            y_train[i, 0] = 5 * x1 / (1 + x0 * x0) + 5 * np.sin(x2 * x3) + 2 * x4 + np.random.normal(0, 1)\n",
        "\n",
        "            x_train[i, 0] = x_train[i, 0] + np.random.normal(0, 0.5)\n",
        "            x_train[i, 1] = x_train[i, 1] + np.random.normal(0, 0.5)\n",
        "            x_train[i, 2] = x_train[i, 2] + np.random.normal(0, 0.5)\n",
        "            x_train[i, 3] = x_train[i, 3] + np.random.normal(0, 0.5)\n",
        "            x_train[i, 4] = x_train[i, 4] + np.random.normal(0, 0.5)\n",
        "\n",
        "        NTest = 500\n",
        "        x_test = np.matrix(np.zeros([NTest, TotalP]))\n",
        "        y_test = np.matrix(np.zeros([NTest, 1]))\n",
        "\n",
        "        for i in range(NTest):\n",
        "            ee = np.sqrt(sigma) * np.random.normal(0, 1)\n",
        "            for j in range(TotalP):\n",
        "                x_test[i, j] = (ee + np.sqrt(sigma) * np.random.normal(0, 1)) / np.sqrt(2.0)\n",
        "            x0 = x_test[i, 0]\n",
        "            x1 = x_test[i, 1]\n",
        "            x2 = x_test[i, 2]\n",
        "            x3 = x_test[i, 3]\n",
        "            x4 = x_test[i, 4]\n",
        "\n",
        "            y_test[i, 0] = 5 * x1 / (1 + x0 * x0) + 5 * np.sin(x2 * x3) + 2 * x4 + np.random.normal(0, 1)\n",
        "\n",
        "            x_test[i, 0] = x_test[i, 0] + np.random.normal(0, 0.5)\n",
        "            x_test[i, 1] = x_test[i, 1] + np.random.normal(0, 0.5)\n",
        "            x_test[i, 2] = x_test[i, 2] + np.random.normal(0, 0.5)\n",
        "            x_test[i, 3] = x_test[i, 3] + np.random.normal(0, 0.5)\n",
        "            x_test[i, 4] = x_test[i, 4] + np.random.normal(0, 0.5)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "    if data_name == 'full_row_rank':\n",
        "        TotalP = 1000\n",
        "        a = 1\n",
        "        b = 1\n",
        "        W1 = np.matrix(np.random.choice([-2, -1, 1, 2], size=TotalP * 5, replace=True).reshape([TotalP, 5]))\n",
        "        W2 = np.matrix(np.random.choice([-2, -1, 1, 2], size=5 * 5, replace=True).reshape([5, 5]))\n",
        "        W3 = np.matrix(np.random.choice([-2, -1, 1, 2], size=5 * 1, replace=True).reshape([5, 1]))\n",
        "        NTrain = 1000\n",
        "        x_train = np.matrix(np.zeros([NTrain, TotalP]))\n",
        "        y_train = np.matrix(np.zeros([NTrain, 1]))\n",
        "        sigma = 1.0\n",
        "        for i in range(NTrain):\n",
        "            if i % 1000 == 0:\n",
        "                print(\"x_train generate = \", i)\n",
        "            ee = np.sqrt(sigma) * np.random.normal(0, 1)\n",
        "            for j in range(TotalP):\n",
        "                x_train[i, j] = (a * ee + b * np.sqrt(sigma) * np.random.normal(0, 1)) / np.sqrt(a * a + b * b)\n",
        "\n",
        "        temp = np.tanh(x_train * W1)\n",
        "        temp = np.tanh(temp * W2)\n",
        "        y_train = temp * W3 + np.random.normal(0, 1, size=y_train.shape)\n",
        "\n",
        "        NTest = 1000\n",
        "        x_test = np.matrix(np.zeros([NTest, TotalP]))\n",
        "        y_test = np.matrix(np.zeros([NTest, 1]))\n",
        "\n",
        "        sigma = 1.0\n",
        "        for i in range(NTest):\n",
        "            if i % 1000 == 0:\n",
        "                print(\"x_test generate = \", i)\n",
        "            ee = np.sqrt(sigma) * np.random.normal(0, 1)\n",
        "            for j in range(TotalP):\n",
        "                x_test[i, j] = (a * ee + b * np.sqrt(sigma) * np.random.normal(0, 1)) / np.sqrt(a * a + b * b)\n",
        "\n",
        "        temp = np.tanh(x_test * W1)\n",
        "        temp = np.tanh(temp * W2)\n",
        "        y_test = temp * W3 + np.random.normal(0, 1, size=y_test.shape)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "\n",
        "    elif data_name == 'parkinson':\n",
        "        temp = pd.read_table('./data/prakinson_telemonitoring/parkinsons_updrs.data', sep=',')\n",
        "\n",
        "        temp = np.mat(temp)\n",
        "        x_data = temp[:, 6:]\n",
        "        y_data = temp[:, 5]\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.2).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        y_train_mean = np.mean(y_train)\n",
        "        y_train_std = np.std(y_train)\n",
        "\n",
        "        y_train = (y_train - y_train_mean) / y_train_std\n",
        "\n",
        "        y_test = (y_test - y_train_mean) / y_train_std\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "    elif data_name == 'qsar':\n",
        "\n",
        "        temp = pd.read_csv('./data/qsar/qsar_androgen_receptor.csv', sep=';', header=None)\n",
        "        temp = np.mat(temp)\n",
        "        x_data = temp[:, 0:-1].astype('float64')\n",
        "        y_data = temp[:, -1]\n",
        "\n",
        "        y_data = (y_data == 'positive')\n",
        "\n",
        "        y_data = np.array(y_data.astype('int')).reshape(y_data.shape[0])\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.2).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.LongTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.LongTensor(y_test).to(device)\n",
        "\n",
        "    elif data_name == 'MNIST':\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "        train_set = datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
        "        test_set = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "        x_train = train_set.data.type(torch.FloatTensor).div(255).sub(0.1307).div(0.3081).reshape(\n",
        "            [train_set.data.shape[0], -1])\n",
        "        x_test = test_set.data.type(torch.FloatTensor).div(255).sub(0.1307).div(0.3081).reshape(\n",
        "            [test_set.data.shape[0], -1])\n",
        "        y_train = train_set.targets\n",
        "        y_test = test_set.targets\n",
        "\n",
        "        x_train = x_train.to(device)\n",
        "        y_train = y_train.to(device)\n",
        "        x_test = x_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "\n",
        "    elif data_name == \"CoverType\":\n",
        "\n",
        "        device = torch.device(\"cpu\")\n",
        "        df = pd.read_csv('./data/CoverType/covtype.data', sep=',', header=None)\n",
        "\n",
        "        y = df[54]\n",
        "        X = df.drop(54, axis=1)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, stratify=y, train_size=0.5)\n",
        "\n",
        "\n",
        "        x_train = np.array(X_train).astype('float64')\n",
        "        x_test = np.array(X_test).astype('float64')\n",
        "        y_train = np.array(y_train)\n",
        "        y_test = np.array(y_test)\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train[:, 0:10] = (x_train[:, 0:10] - np.full(x_train[:, 0:10].shape, x_train_mean[0:10])) / np.full(\n",
        "            x_train[:, 0:10].shape,\n",
        "            x_train_std[0:10])\n",
        "\n",
        "        x_test[:, 0:10] = (x_test[:, 0:10] - np.full(x_test[:, 0:10].shape, x_train_mean[0:10])) / np.full(\n",
        "            x_test[:, 0:10].shape, x_train_std[0:10])\n",
        "\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_train = torch.LongTensor(y_train).to(device)\n",
        "        y_train = y_train - 1\n",
        "        y_test = torch.LongTensor(y_test).to(device)\n",
        "        y_test = y_test - 1\n",
        "    elif data_name == \"Boston\":\n",
        "        temp = np.loadtxt('./data/Boston/housing.data')\n",
        "        x_data = temp[:, 0:-1]\n",
        "        y_data = temp[:, -1].reshape([temp.shape[0], 1])\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "    elif data_name == 'Concrete':\n",
        "        temp = pd.read_excel('data/Concrete/Concrete_Data.xls')\n",
        "\n",
        "        temp = np.mat(temp)\n",
        "        x_data = temp[:, :8]\n",
        "        y_data = temp[:, 8]\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "    elif data_name == 'Energy':\n",
        "        temp = pd.read_excel('data/Energy/ENB2012_data.xlsx')\n",
        "\n",
        "        temp = np.mat(temp)\n",
        "        x_data = temp[:768, :8]\n",
        "        y_data = temp[:768, 9]\n",
        "\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "    elif data_name == 'Wine':\n",
        "        temp = pd.read_csv('data/Wine/winequality-red.csv', sep=';')\n",
        "\n",
        "        temp = np.mat(temp)\n",
        "        x_data = temp[:, 0:11]\n",
        "        y_data = temp[:, 11]\n",
        "\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "    elif data_name == 'Yacht':\n",
        "\n",
        "        temp = np.loadtxt('data/Yacht/yacht_hydrodynamics.data')\n",
        "        x_data = temp[:, 0:6]\n",
        "        y_data = temp[:, 6].reshape([temp.shape[0], 1])\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "    elif data_name == 'kin8nm':\n",
        "        temp = pd.read_csv('data/kin8nm/dataset_2175_kin8nm.csv')\n",
        "\n",
        "        temp = np.mat(temp)\n",
        "        x_data = temp[:, 0:8]\n",
        "        y_data = temp[:, 8]\n",
        "\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "    elif data_name == 'Naval':\n",
        "\n",
        "        temp = np.loadtxt('data/Naval/data.txt')\n",
        "        x_data = temp[:, 0:16]\n",
        "        y_data = temp[:, 16].reshape([temp.shape[0], 1])\n",
        "\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "    elif data_name == 'CCPP':\n",
        "        temp = pd.read_excel('data/CCPP/Folds5x2_pp.xlsx')\n",
        "\n",
        "        temp = np.mat(temp)\n",
        "        x_data = temp[:, :4]\n",
        "        y_data = temp[:, 4]\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "    elif data_name == 'Protein':\n",
        "        temp = pd.read_csv('data/Protein/CASP.csv')\n",
        "\n",
        "        temp = np.matrix(temp)\n",
        "        x_data = temp[:, 1:10]\n",
        "        y_data = temp[:, 0]\n",
        "\n",
        "        permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "        size_test = np.round(x_data.shape[0] * 0.1).astype(int)\n",
        "        divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "        lower_bound = cross_validate_index * size_test\n",
        "        upper_bound = (cross_validate_index + 1) * size_test\n",
        "        test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "        index_train = permutation[[not _ for _ in test_index]]\n",
        "        index_test = permutation[test_index]\n",
        "\n",
        "        x_train = x_data[index_train, :]\n",
        "        y_train = y_data[index_train]\n",
        "\n",
        "        x_test = x_data[index_test, :]\n",
        "        y_test = y_data[index_test]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "    elif data_name == 'Year':\n",
        "        temp = pd.read_table('data/Year/YearPredictionMSD.txt', header = None, sep = ',')\n",
        "\n",
        "        temp = np.matrix(temp)\n",
        "        x_data = temp[:, 1:]\n",
        "        y_data = temp[:, 0]\n",
        "\n",
        "        x_train = x_data[0:463715, :]\n",
        "        y_train = y_data[0:463715]\n",
        "\n",
        "        x_test = x_data[463715:, :]\n",
        "        y_test = y_data[463715:]\n",
        "\n",
        "        x_train_std = np.std(x_train, 0)\n",
        "        x_train_std[x_train_std == 0] = 1\n",
        "        x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "        x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "        x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "        x_train = torch.FloatTensor(x_train).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device)\n",
        "        x_test = torch.FloatTensor(x_test).to(device)\n",
        "        y_test = torch.FloatTensor(y_test).to(device)\n",
        "    elif data_name == \"hiv\":\n",
        "        x_train_hiv = np.load('data/hiv/x_train.npy')\n",
        "        y_train_hiv = np.load('data/hiv/y_train.npy')\n",
        "        x_test_hiv = np.load('data/hiv/x_test.npy')\n",
        "        y_test_hiv = np.load('data/hiv/y_test.npy')\n",
        "\n",
        "        # Reshape the array to have 2 dimensions, where the second dimension is 1\n",
        "        # y_train_hiv = y_train_hiv.reshape(-1, 1)\n",
        "        # y_test_hiv = y_test_hiv.reshape(-1, 1)\n",
        "\n",
        "        # Convert to a PyTorch tensor\n",
        "        y_train_hiv = torch.tensor(y_train_hiv, dtype=torch.float32)\n",
        "        y_test_hiv = torch.tensor(y_test_hiv, dtype=torch.float32)\n",
        "\n",
        "        # Convert NumPy arrays to PyTorch tensors and send them to the specified device\n",
        "        x_train = torch.FloatTensor(x_train_hiv).to(device)\n",
        "        y_train = y_train_hiv.type(torch.LongTensor).to(device)\n",
        "        x_test = torch.FloatTensor(x_test_hiv).to(device)\n",
        "        y_test = y_test_hiv.type(torch.LongTensor).to(device)\n",
        "    elif data_name == \"hiv_resample\":\n",
        "        x_train_hiv = np.load('data/hiv/x_train_resample.npy')\n",
        "        y_train_hiv = np.load('data/hiv/y_train_resample.npy')\n",
        "        x_test_hiv = np.load('data/hiv/x_test.npy')\n",
        "        y_test_hiv = np.load('data/hiv/y_test.npy')\n",
        "\n",
        "        # Reshape the array to have 2 dimensions, where the second dimension is 1\n",
        "        # y_train_hiv = y_train_hiv.reshape(-1, 1)\n",
        "        # y_test_hiv = y_test_hiv.reshape(-1, 1)\n",
        "\n",
        "        # Convert to a PyTorch tensor\n",
        "        y_train_hiv = torch.tensor(y_train_hiv, dtype=torch.float32)\n",
        "        y_test_hiv = torch.tensor(y_test_hiv, dtype=torch.float32)\n",
        "\n",
        "        # Convert NumPy arrays to PyTorch tensors and send them to the specified device\n",
        "        x_train = torch.FloatTensor(x_train_hiv).to(device)\n",
        "        y_train = y_train_hiv.type(torch.LongTensor).to(device)\n",
        "        x_test = torch.FloatTensor(x_test_hiv).to(device)\n",
        "        y_test = y_test_hiv.type(torch.LongTensor).to(device)\n",
        "    elif data_name == \"hiv_pca\":\n",
        "        x_train_hiv = np.load('data/hiv/x_train_pca.npy')\n",
        "        y_train_hiv = np.load('data/hiv/y_train.npy')\n",
        "        x_test_hiv = np.load('data/hiv/x_test_pca.npy')\n",
        "        y_test_hiv = np.load('data/hiv/y_test.npy')\n",
        "\n",
        "        # Reshape the array to have 2 dimensions, where the second dimension is 1\n",
        "        # y_train_hiv = y_train_hiv.reshape(-1, 1)\n",
        "        # y_test_hiv = y_test_hiv.reshape(-1, 1)\n",
        "\n",
        "        # Convert to a PyTorch tensor\n",
        "        y_train_hiv = torch.tensor(y_train_hiv, dtype=torch.float32)\n",
        "        y_test_hiv = torch.tensor(y_test_hiv, dtype=torch.float32)\n",
        "\n",
        "        # Convert NumPy arrays to PyTorch tensors and send them to the specified device\n",
        "        x_train = torch.FloatTensor(x_train_hiv).to(device)\n",
        "        y_train = y_train_hiv.type(torch.LongTensor).to(device)\n",
        "        x_test = torch.FloatTensor(x_test_hiv).to(device)\n",
        "        y_test = y_test_hiv.type(torch.LongTensor).to(device)\n",
        "    elif data_name == \"hiv_pca_resample\":\n",
        "        x_train_hiv = np.load('data/hiv/x_train_resample_pca.npy')\n",
        "        y_train_hiv = np.load('data/hiv/y_train_resample.npy')\n",
        "        x_test_hiv = np.load('data/hiv/x_test_pca.npy')\n",
        "        y_test_hiv = np.load('data/hiv/y_test.npy')\n",
        "\n",
        "        # Reshape the array to have 2 dimensions, where the second dimension is 1\n",
        "        # y_train_hiv = y_train_hiv.reshape(-1, 1)\n",
        "        # y_test_hiv = y_test_hiv.reshape(-1, 1)\n",
        "\n",
        "        # Convert to a PyTorch tensor\n",
        "        y_train_hiv = torch.tensor(y_train_hiv, dtype=torch.float32)\n",
        "        y_test_hiv = torch.tensor(y_test_hiv, dtype=torch.float32)\n",
        "\n",
        "        # Convert NumPy arrays to PyTorch tensors and send them to the specified device\n",
        "        x_train = torch.FloatTensor(x_train_hiv).to(device)\n",
        "        y_train = y_train_hiv.type(torch.LongTensor).to(device)\n",
        "        x_test = torch.FloatTensor(x_test_hiv).to(device)\n",
        "        y_test = y_test_hiv.type(torch.LongTensor).to(device)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej1sGX-19HZl"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyWf22KZ-w8E",
        "outputId": "23d095d5-9794-4a49-cdc4-47e142469cc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:deepchem.molnet.load_function.molnet_loader:'split' is deprecated.  Use 'splitter' instead.\n",
            "[23:21:43] WARNING: not removing hydrogen atom without neighbors\n",
            "[23:21:43] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        }
      ],
      "source": [
        "import deepchem as dc\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to standardize datasets\n",
        "def standardize_data(X, mean, std):\n",
        "    return (X - mean) / std\n",
        "\n",
        "# Load HIV dataset\n",
        "hiv_tasks, hiv_datasets, hiv_transformers = dc.molnet.load_hiv(featurizer='ECFP', split='index')\n",
        "hiv_train, hiv_valid, hiv_test = hiv_datasets\n",
        "\n",
        "# Standardize each dataset separately using training data statistics\n",
        "x_train_mean = np.mean(hiv_train.X, axis=0)\n",
        "x_train_std = np.std(hiv_train.X, axis=0)\n",
        "x_train_std[x_train_std == 0] = 1  # Avoid division by zero\n",
        "\n",
        "# Apply standardization\n",
        "x_train_stdized = standardize_data(hiv_train.X, x_train_mean, x_train_std)\n",
        "x_valid_stdized = standardize_data(hiv_valid.X, x_train_mean, x_train_std)\n",
        "x_test_stdized = standardize_data(hiv_test.X, x_train_mean, x_train_std)\n",
        "\n",
        "# Save standardized datasets to disk as NumPy arrays\n",
        "# Ensure the target directories exist\n",
        "os.makedirs('drive/MyDrive/result/data/hiv', exist_ok=True)\n",
        "\n",
        "np.save('drive/MyDrive/result/data/hiv/x_train.npy', x_train_stdized)\n",
        "np.save('drive/MyDrive/result/data/hiv/y_train.npy', hiv_train.y.squeeze(1).astype(int))\n",
        "np.save('drive/MyDrive/result/data/hiv/x_valid.npy', x_valid_stdized)\n",
        "np.save('drive/MyDrive/result/data/hiv/y_valid.npy', hiv_valid.y.squeeze(1).astype(int))\n",
        "np.save('drive/MyDrive/result/data/hiv/x_test.npy', x_test_stdized)\n",
        "np.save('drive/MyDrive/result/data/hiv/y_test.npy', hiv_test.y.squeeze(1).astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QARhS1veDz2M",
        "outputId": "19243c64-92b6-479a-99ea-eedc2d0f3b25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4113, 1024)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test_stdized.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EnS9H1EP-3aX"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# # Load the standardized data\n",
        "# x_train_stdized = np.load('drive/MyDrive/result/data/hiv/x_train.npy')\n",
        "\n",
        "# # Initializing PCA\n",
        "# pca = PCA()\n",
        "\n",
        "# # Fitting PCA on the training data\n",
        "# pca.fit(x_train_stdized)\n",
        "\n",
        "# # Getting the explained variance ratio\n",
        "# explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# # Plotting the variance explained by the principal components\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.plot(np.cumsum(explained_variance_ratio))\n",
        "# plt.xlabel('Number of Components')\n",
        "# plt.ylabel('Variance (%)') # for each component\n",
        "# plt.title('Explained Variance')\n",
        "\n",
        "# # Optional: Plot with a certain number of components (e.g., top 20 components)\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.bar(range(1, 101), explained_variance_ratio[:100], alpha=0.7, align='center', label='Individual explained variance')\n",
        "# plt.step(range(1, 101), np.cumsum(explained_variance_ratio[:100]), where='mid', label='Cumulative explained variance')\n",
        "# plt.xlabel('Principal Component index')\n",
        "# plt.ylabel('Variance Ratio')\n",
        "# plt.title('Explained Variance Ratio by Components (Top 100)')\n",
        "# plt.legend(loc='best')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GZjzyfBE_1Zk"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import deepchem as dc\n",
        "# from sklearn.ensemble import GradientBoostingClassifier\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# # Load the standardized data\n",
        "# x_train_stdized = np.load('drive/MyDrive/result/data/hiv/x_train.npy')\n",
        "# y_train = np.load('drive/MyDrive/result/data/hiv/y_train.npy')\n",
        "# x_test_stdized = np.load('drive/MyDrive/result/data/hiv/x_test.npy')\n",
        "# y_test = np.load('drive/MyDrive/result/data/hiv/y_test.npy')\n",
        "\n",
        "# # Initialize PCA and keep the top 100 principal components\n",
        "# pca = PCA(n_components=100)\n",
        "# pca.fit(x_train_stdized)\n",
        "# x_train_pca = pca.transform(x_train_stdized)\n",
        "# x_test_pca = pca.transform(x_test_stdized)\n",
        "\n",
        "# # Train Gradient Boosting on the full dataset\n",
        "# gb_full = GradientBoostingClassifier()\n",
        "# gb_full.fit(x_train_stdized, y_train)\n",
        "\n",
        "# # Train Gradient Boosting on the top 100 principal components\n",
        "# gb_pca = GradientBoostingClassifier()\n",
        "# gb_pca.fit(x_train_pca, y_train)\n",
        "\n",
        "# # Predictions and evaluation on the full dataset\n",
        "# y_pred_full = gb_full.predict(x_test_stdized)\n",
        "# y_pred_proba_full = gb_full.predict_proba(x_test_stdized)[:, 1]  # Probability for the positive class\n",
        "# accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "# roc_auc_full = roc_auc_score(y_test, y_pred_proba_full)\n",
        "\n",
        "# # Predictions and evaluation on the dataset with PCA\n",
        "# y_pred_pca = gb_pca.predict(x_test_pca)\n",
        "# y_pred_proba_pca = gb_pca.predict_proba(x_test_pca)[:, 1]  # Probability for the positive class\n",
        "# accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "# roc_auc_pca = roc_auc_score(y_test, y_pred_proba_pca)\n",
        "\n",
        "# # Report accuracy and ROC AUC for both models\n",
        "# print(\"Test Accuracy (Full dataset):\", accuracy_full)\n",
        "# print(\"Test ROC AUC (Full dataset):\", roc_auc_full)\n",
        "# print(\"Test Accuracy (PCA dataset):\", accuracy_pca)\n",
        "# print(\"Test ROC AUC (PCA dataset):\", roc_auc_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4PfSXf30D_Rv"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA  # Import PCA from scikit-learn=\n",
        "# Initialize PCA and fit it to the standardized training data\n",
        "pca = PCA(n_components=100)\n",
        "pca.fit(x_train_stdized)\n",
        "\n",
        "# Apply PCA transformation to the training, validation and test sets\n",
        "x_train_pca = pca.transform(x_train_stdized)\n",
        "x_valid_pca = pca.transform(x_valid_stdized)\n",
        "x_test_pca = pca.transform(x_test_stdized)\n",
        "\n",
        "# Save PCA-reduced datasets to disk as NumPy arrays\n",
        "# Ensure the target directories exist\n",
        "os.makedirs('drive/MyDrive/result/data/hiv', exist_ok=True)\n",
        "np.save('drive/MyDrive/result/data/hiv/x_train_pca.npy', x_train_pca)\n",
        "np.save('drive/MyDrive/result/data/hiv/y_train.npy', hiv_train.y.squeeze(1).astype(int))\n",
        "np.save('drive/MyDrive/result/data/hiv/x_valid_pca.npy', x_valid_pca)\n",
        "np.save('drive/MyDrive/result/data/hiv/y_valid.npy', hiv_valid.y.squeeze(1).astype(int))\n",
        "np.save('drive/MyDrive/result/data/hiv/x_test_pca.npy', x_test_pca)\n",
        "np.save('drive/MyDrive/result/data/hiv/y_test.npy', hiv_test.y.squeeze(1).astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EiRxQVWoHwTL"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "def undersample(x, y):\n",
        "    # Load the datasets\n",
        "    x_train_hiv = np.load(x)\n",
        "    y_train_hiv = np.load(y)\n",
        "\n",
        "    # Apply RandomUnderSampler to decrease the size of the dataset and balance the classes\n",
        "    rus = RandomUnderSampler(random_state=42)\n",
        "    x_train_resampled, y_train_resampled = rus.fit_resample(x_train_hiv, y_train_hiv)\n",
        "\n",
        "    # Save the resampled training dataset\n",
        "    np.save(x.replace(\"train\",\"train_resample\"), x_train_resampled)\n",
        "    np.save(y.replace(\"train\",\"train_resample\"), y_train_resampled)\n",
        "\n",
        "undersample('drive/MyDrive/result/data/hiv/x_train.npy', 'drive/MyDrive/result/data/hiv/y_train.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OYVSRquYIN-A"
      },
      "outputs": [],
      "source": [
        "undersample('drive/MyDrive/result/data/hiv/x_train_pca.npy', 'drive/MyDrive/result/data/hiv/y_train.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hXzKVlgYk5jO"
      },
      "outputs": [],
      "source": [
        "!cp -r drive/MyDrive/result/data ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0huG9fsieG-"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLIeP3G2PKZq",
        "outputId": "328b5579-f454-4fd6-f5b3-d3200aeb3b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running command: python kstonet.py --data_name hiv_pca_resample --base_path ./drive/MyDrive/result/ --regression_flag 0 --confidence_interval_flag 0 --layer 0 --unit 5 --sigma 0.0001 --C 0.1 --epsilon 0.1 --lr 5e-05 --alpha 0.1 --nepoch 50 --model_path model_layer_0_unit_5_sigma_0.0001_C_0.1_epsilon_0.1_lr_5e-05_alpha_0.1_nepoch_50_seed_4/ --seed 4\n",
            "Output:\n",
            "epoch: 0\n",
            "train_loss: tensor(0.7059, device='cuda:0') train_accuracy: 0.4709457816678823 train AUC-ROC: 0.5202030745043236 test loss: tensor(0.7172, device='cuda:0') test accuracy: 0.4709457816678823 test AUC-ROC: 0.5686496211721503\n",
            "epoch: 1\n",
            "train_loss: tensor(0.6991, device='cuda:0') train_accuracy: 0.44517383904692437 train AUC-ROC: 0.5215948991178269 test loss: tensor(0.7170, device='cuda:0') test accuracy: 0.44517383904692437 test AUC-ROC: 0.5333168013844627\n",
            "epoch: 2\n",
            "train_loss: tensor(0.6967, device='cuda:0') train_accuracy: 0.30099683929005594 train AUC-ROC: 0.5216442484059743 test loss: tensor(0.7211, device='cuda:0') test accuracy: 0.30099683929005594 test AUC-ROC: 0.5191060084840555\n",
            "epoch: 3\n",
            "train_loss: tensor(0.6966, device='cuda:0') train_accuracy: 0.11475808412351082 train AUC-ROC: 0.5246030220979998 test loss: tensor(0.7299, device='cuda:0') test accuracy: 0.11475808412351082 test AUC-ROC: 0.5014731021674441\n",
            "epoch: 4\n",
            "train_loss: tensor(0.6981, device='cuda:0') train_accuracy: 0.060782883539995135 train AUC-ROC: 0.505824962878854 test loss: tensor(0.7419, device='cuda:0') test accuracy: 0.060782883539995135 test AUC-ROC: 0.4580872886900365\n",
            "epoch: 5\n",
            "train_loss: tensor(0.6997, device='cuda:0') train_accuracy: 0.05956722586919524 train AUC-ROC: 0.47409206044195995 test loss: tensor(0.7524, device='cuda:0') test accuracy: 0.05956722586919524 test AUC-ROC: 0.41074647025304434\n",
            "epoch: 6\n",
            "train_loss: tensor(0.6989, device='cuda:0') train_accuracy: 0.05956722586919524 train AUC-ROC: 0.46599222639531834 test loss: tensor(0.7505, device='cuda:0') test accuracy: 0.05956722586919524 test AUC-ROC: 0.3881919675833105\n",
            "epoch: 7\n",
            "train_loss: tensor(0.6966, device='cuda:0') train_accuracy: 0.05956722586919524 train AUC-ROC: 0.46748405974320895 test loss: tensor(0.7230, device='cuda:0') test accuracy: 0.05956722586919524 test AUC-ROC: 0.40868349408015536\n",
            "epoch: 8\n",
            "train_loss: tensor(0.6950, device='cuda:0') train_accuracy: 0.9404327741308047 train AUC-ROC: 0.4785902698925671 test loss: tensor(0.6788, device='cuda:0') test accuracy: 0.9404327741308047 test AUC-ROC: 0.4235047379861976\n",
            "\n",
            "Errors:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/_compressed.py\", line 95, in __init__\n",
            "    minor_dim = self.indices.max() + 1\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\", line 40, in _amax\n",
            "    return umr_maximum(a, axis, None, out, keepdims, initial, where)\n",
            "ValueError: zero-size array to reduction operation maximum which has no identity\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/kstonet.py\", line 373, in <module>\n",
            "    main()\n",
            "  File \"/content/kstonet.py\", line 235, in main\n",
            "    svr_list[i].fit(x_train.cpu(), hidden_list[0][:, i].cpu().detach())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thundersvm/thundersvm.py\", line 132, in fit\n",
            "    self.support_vectors_ = sp.csr_matrix((self.data, self.col, self.row))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/_compressed.py\", line 97, in __init__\n",
            "    raise ValueError('unable to infer matrix dimensions') from e\n",
            "ValueError: unable to infer matrix dimensions\n",
            "\n",
            "Running command: python kstonet.py --data_name hiv_pca_resample --base_path ./drive/MyDrive/result/ --regression_flag 0 --confidence_interval_flag 0 --layer 0 --unit 10 --sigma 0.0001 --C 0.1 --epsilon 0.1 --lr 5e-05 --alpha 0.1 --nepoch 50 --model_path model_layer_0_unit_10_sigma_0.0001_C_0.1_epsilon_0.1_lr_5e-05_alpha_0.1_nepoch_50_seed_4/ --seed 4\n",
            "Output:\n",
            "epoch: 0\n",
            "train_loss: tensor(0.7176, device='cuda:0') train_accuracy: 0.8978847556528081 train AUC-ROC: 0.48502008909075034 test loss: tensor(0.5850, device='cuda:0') test accuracy: 0.8978847556528081 test AUC-ROC: 0.5069708545258849\n",
            "epoch: 1\n",
            "train_loss: tensor(0.7123, device='cuda:0') train_accuracy: 0.9224410406029662 train AUC-ROC: 0.4817381430692637 test loss: tensor(0.5817, device='cuda:0') test accuracy: 0.9224410406029662 test AUC-ROC: 0.5121066627271385\n",
            "epoch: 2\n",
            "train_loss: tensor(0.7086, device='cuda:0') train_accuracy: 0.9348407488451252 train AUC-ROC: 0.47889466328937025 test loss: tensor(0.5787, device='cuda:0') test accuracy: 0.9348407488451252 test AUC-ROC: 0.5126390266551295\n",
            "epoch: 3\n",
            "train_loss: tensor(0.7058, device='cuda:0') train_accuracy: 0.9401896425966448 train AUC-ROC: 0.4698663638745742 test loss: tensor(0.5785, device='cuda:0') test accuracy: 0.9401896425966448 test AUC-ROC: 0.5130706160437287\n",
            "epoch: 4\n",
            "train_loss: tensor(0.7037, device='cuda:0') train_accuracy: 0.9404327741308047 train AUC-ROC: 0.4578631321512796 test loss: tensor(0.5827, device='cuda:0') test accuracy: 0.9404327741308047 test AUC-ROC: 0.5074425426840851\n",
            "epoch: 5\n",
            "train_loss: tensor(0.7023, device='cuda:0') train_accuracy: 0.9404327741308047 train AUC-ROC: 0.4337295833697266 test loss: tensor(0.5940, device='cuda:0') test accuracy: 0.9404327741308047 test AUC-ROC: 0.45997931747673215\n",
            "epoch: 6\n",
            "train_loss: tensor(0.7010, device='cuda:0') train_accuracy: 0.9404327741308047 train AUC-ROC: 0.3954568084548869 test loss: tensor(0.6094, device='cuda:0') test accuracy: 0.9404327741308047 test AUC-ROC: 0.4027736741025262\n",
            "epoch: 7\n",
            "train_loss: tensor(0.6989, device='cuda:0') train_accuracy: 0.9404327741308047 train AUC-ROC: 0.3675399598218185 test loss: tensor(0.6232, device='cuda:0') test accuracy: 0.9404327741308047 test AUC-ROC: 0.3827675537640082\n",
            "epoch: 8\n",
            "train_loss: tensor(0.6962, device='cuda:0') train_accuracy: 0.9404327741308047 train AUC-ROC: 0.3621198357935191 test loss: tensor(0.6408, device='cuda:0') test accuracy: 0.9404327741308047 test AUC-ROC: 0.3882647785070595\n",
            "epoch: 9\n",
            "train_loss: tensor(0.6942, device='cuda:0') train_accuracy: 0.9404327741308047 train AUC-ROC: 0.39363830902262203 test loss: tensor(0.6619, device='cuda:0') test accuracy: 0.9404327741308047 test AUC-ROC: 0.4140118818985712\n",
            "\n",
            "Errors:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/_compressed.py\", line 95, in __init__\n",
            "    minor_dim = self.indices.max() + 1\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\", line 40, in _amax\n",
            "    return umr_maximum(a, axis, None, out, keepdims, initial, where)\n",
            "ValueError: zero-size array to reduction operation maximum which has no identity\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/kstonet.py\", line 373, in <module>\n",
            "    main()\n",
            "  File \"/content/kstonet.py\", line 235, in main\n",
            "    svr_list[i].fit(x_train.cpu(), hidden_list[0][:, i].cpu().detach())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thundersvm/thundersvm.py\", line 132, in fit\n",
            "    self.support_vectors_ = sp.csr_matrix((self.data, self.col, self.row))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/_compressed.py\", line 97, in __init__\n",
            "    raise ValueError('unable to infer matrix dimensions') from e\n",
            "ValueError: unable to infer matrix dimensions\n",
            "\n",
            "Running command: python kstonet.py --data_name hiv_pca_resample --base_path ./drive/MyDrive/result/ --regression_flag 0 --confidence_interval_flag 0 --layer 0 --unit 20 --sigma 0.0001 --C 0.1 --epsilon 0.1 --lr 5e-05 --alpha 0.1 --nepoch 50 --model_path model_layer_0_unit_20_sigma_0.0001_C_0.1_epsilon_0.1_lr_5e-05_alpha_0.1_nepoch_50_seed_4/ --seed 4\n",
            "Output:\n",
            "epoch: 0\n",
            "train_loss: tensor(0.7075, device='cuda:0') train_accuracy: 0.3527838560661318 train AUC-ROC: 0.4854520045418813 test loss: tensor(0.7162, device='cuda:0') test accuracy: 0.3527838560661318 test AUC-ROC: 0.5272745499440729\n",
            "epoch: 1\n",
            "train_loss: tensor(0.7021, device='cuda:0') train_accuracy: 0.32214928276197424 train AUC-ROC: 0.4898716045069438 test loss: tensor(0.7125, device='cuda:0') test accuracy: 0.32214928276197424 test AUC-ROC: 0.5416948061541059\n",
            "epoch: 2\n",
            "train_loss: tensor(0.6974, device='cuda:0') train_accuracy: 0.3026987600291758 train AUC-ROC: 0.499463271901476 test loss: tensor(0.7084, device='cuda:0') test accuracy: 0.3026987600291758 test AUC-ROC: 0.5585943270793322\n",
            "epoch: 3\n",
            "train_loss: tensor(0.6942, device='cuda:0') train_accuracy: 0.2825188426938974 train AUC-ROC: 0.5166721984452791 test loss: tensor(0.7058, device='cuda:0') test accuracy: 0.2825188426938974 test AUC-ROC: 0.5824367389148007\n",
            "epoch: 4\n",
            "train_loss: tensor(0.6924, device='cuda:0') train_accuracy: 0.274981765134938 train AUC-ROC: 0.529546248580662 test loss: tensor(0.7026, device='cuda:0') test accuracy: 0.274981765134938 test AUC-ROC: 0.587031213726442\n",
            "epoch: 5\n",
            "train_loss: tensor(0.6918, device='cuda:0') train_accuracy: 0.23437879893022126 train AUC-ROC: 0.5453367106297493 test loss: tensor(0.7025, device='cuda:0') test accuracy: 0.23437879893022126 test AUC-ROC: 0.5909983538399848\n",
            "epoch: 6\n",
            "train_loss: tensor(0.6923, device='cuda:0') train_accuracy: 0.10406029662047167 train AUC-ROC: 0.5494292077910734 test loss: tensor(0.7082, device='cuda:0') test accuracy: 0.10406029662047167 test AUC-ROC: 0.5860920583331575\n",
            "epoch: 7\n",
            "train_loss: tensor(0.6927, device='cuda:0') train_accuracy: 0.06005348893751519 train AUC-ROC: 0.5449414796052057 test loss: tensor(0.7179, device='cuda:0') test accuracy: 0.06005348893751519 test AUC-ROC: 0.5675690648544837\n",
            "\n",
            "Errors:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/_compressed.py\", line 95, in __init__\n",
            "    minor_dim = self.indices.max() + 1\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\", line 40, in _amax\n",
            "    return umr_maximum(a, axis, None, out, keepdims, initial, where)\n",
            "ValueError: zero-size array to reduction operation maximum which has no identity\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/kstonet.py\", line 373, in <module>\n",
            "    main()\n",
            "  File \"/content/kstonet.py\", line 235, in main\n",
            "    svr_list[i].fit(x_train.cpu(), hidden_list[0][:, i].cpu().detach())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thundersvm/thundersvm.py\", line 132, in fit\n",
            "    self.support_vectors_ = sp.csr_matrix((self.data, self.col, self.row))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/_compressed.py\", line 97, in __init__\n",
            "    raise ValueError('unable to infer matrix dimensions') from e\n",
            "ValueError: unable to infer matrix dimensions\n",
            "\n",
            "Running command: python kstonet.py --data_name hiv_pca_resample --base_path ./drive/MyDrive/result/ --regression_flag 0 --confidence_interval_flag 0 --layer 1 --unit 5 --sigma 0.0001 --C 0.1 --epsilon 0.1 --lr 5e-05 --alpha 0.1 --nepoch 50 --model_path model_layer_1_unit_5_sigma_0.0001_C_0.1_epsilon_0.1_lr_5e-05_alpha_0.1_nepoch_50_seed_4/ --seed 4\n",
            "Output:\n",
            "epoch: 0\n",
            "train_loss: tensor(9.6611, device='cuda:0') train_accuracy: 0.5686846584001946 train AUC-ROC: 0.7242999388592891 test loss: tensor(9.0988, device='cuda:0') test accuracy: 0.5686846584001946 test AUC-ROC: 0.6254774919274846\n",
            "epoch: 1\n",
            "train_loss: tensor(4.9276, device='cuda:0') train_accuracy: 0.600534889375152 train AUC-ROC: 0.793605991789676 test loss: tensor(4.5543, device='cuda:0') test accuracy: 0.600534889375152 test AUC-ROC: 0.6562596289808581\n",
            "epoch: 2\n",
            "train_loss: tensor(3.0115, device='cuda:0') train_accuracy: 0.637004619499149 train AUC-ROC: 0.8178648790287362 test loss: tensor(2.5132, device='cuda:0') test accuracy: 0.637004619499149 test AUC-ROC: 0.6624849629613996\n",
            "epoch: 3\n",
            "train_loss: tensor(1.9465, device='cuda:0') train_accuracy: 0.6722586919523462 train AUC-ROC: 0.8473630884793432 test loss: tensor(1.5066, device='cuda:0') test accuracy: 0.6722586919523462 test AUC-ROC: 0.6710861490407952\n",
            "epoch: 4\n",
            "train_loss: tensor(1.3780, device='cuda:0') train_accuracy: 0.7230731825917821 train AUC-ROC: 0.8759996506245087 test loss: tensor(0.9639, device='cuda:0') test accuracy: 0.7230731825917821 test AUC-ROC: 0.6772149294050609\n",
            "epoch: 5\n",
            "train_loss: tensor(0.9549, device='cuda:0') train_accuracy: 0.7549234135667396 train AUC-ROC: 0.9055258101144203 test loss: tensor(0.7384, device='cuda:0') test accuracy: 0.7549234135667396 test AUC-ROC: 0.6876110630394867\n",
            "epoch: 6\n",
            "train_loss: tensor(0.6366, device='cuda:0') train_accuracy: 0.838560661317773 train AUC-ROC: 0.9373792470958162 test loss: tensor(0.5330, device='cuda:0') test accuracy: 0.838560661317773 test AUC-ROC: 0.6985633032944305\n",
            "epoch: 7\n",
            "train_loss: tensor(0.4038, device='cuda:0') train_accuracy: 0.8560661317772915 train AUC-ROC: 0.9607336885317496 test loss: tensor(0.4984, device='cuda:0') test accuracy: 0.8560661317772915 test AUC-ROC: 0.7106604689445581\n",
            "epoch: 8\n",
            "train_loss: tensor(0.2963, device='cuda:0') train_accuracy: 0.8701677607585704 train AUC-ROC: 0.9751436806708009 test loss: tensor(0.4393, device='cuda:0') test accuracy: 0.8701677607585704 test AUC-ROC: 0.7070040942954224\n",
            "epoch: 9\n",
            "train_loss: tensor(0.2258, device='cuda:0') train_accuracy: 0.8665207877461707 train AUC-ROC: 0.9860289108219059 test loss: tensor(0.4306, device='cuda:0') test accuracy: 0.8665207877461707 test AUC-ROC: 0.7146144186733638\n",
            "epoch: 10\n",
            "train_loss: tensor(0.2391, device='cuda:0') train_accuracy: 0.8760029175784099 train AUC-ROC: 0.9870067254782077 test loss: tensor(0.4075, device='cuda:0') test accuracy: 0.8760029175784099 test AUC-ROC: 0.7083020281535571\n",
            "epoch: 11\n",
            "train_loss: tensor(0.2689, device='cuda:0') train_accuracy: 0.8743009968392901 train AUC-ROC: 0.9833666695781291 test loss: tensor(0.4171, device='cuda:0') test accuracy: 0.8743009968392901 test AUC-ROC: 0.7116281155688748\n",
            "epoch: 12\n",
            "train_loss: tensor(0.2033, device='cuda:0') train_accuracy: 0.8679795769511306 train AUC-ROC: 0.987372696305354 test loss: tensor(0.4497, device='cuda:0') test accuracy: 0.8679795769511306 test AUC-ROC: 0.7106815735601376\n",
            "epoch: 13\n",
            "train_loss: tensor(0.1603, device='cuda:0') train_accuracy: 0.8701677607585704 train AUC-ROC: 0.9910503100707485 test loss: tensor(0.4080, device='cuda:0') test accuracy: 0.8701677607585704 test AUC-ROC: 0.7147895869826731\n",
            "epoch: 14\n",
            "train_loss: tensor(0.1938, device='cuda:0') train_accuracy: 0.8653051300753708 train AUC-ROC: 0.9887326404052755 test loss: tensor(0.4591, device='cuda:0') test accuracy: 0.8653051300753708 test AUC-ROC: 0.7120713124960428\n",
            "epoch: 15\n",
            "train_loss: tensor(0.2095, device='cuda:0') train_accuracy: 0.8728422076343302 train AUC-ROC: 0.9882697178792907 test loss: tensor(0.4027, device='cuda:0') test accuracy: 0.8728422076343302 test AUC-ROC: 0.7190569402528333\n",
            "epoch: 16\n",
            "train_loss: tensor(0.2270, device='cuda:0') train_accuracy: 0.8740578653051301 train AUC-ROC: 0.9869508253995982 test loss: tensor(0.4287, device='cuda:0') test accuracy: 0.8740578653051301 test AUC-ROC: 0.7133692463541776\n",
            "epoch: 17\n",
            "train_loss: tensor(0.2286, device='cuda:0') train_accuracy: 0.8767323121808899 train AUC-ROC: 0.9870084723556642 test loss: tensor(0.4036, device='cuda:0') test accuracy: 0.8767323121808899 test AUC-ROC: 0.7103386235569719\n",
            "epoch: 18\n",
            "train_loss: tensor(0.2774, device='cuda:0') train_accuracy: 0.8828106005348894 train AUC-ROC: 0.9843278888985938 test loss: tensor(0.3831, device='cuda:0') test accuracy: 0.8828106005348894 test AUC-ROC: 0.7128870058881878\n",
            "epoch: 19\n",
            "train_loss: tensor(0.3605, device='cuda:0') train_accuracy: 0.887186968149769 train AUC-ROC: 0.9782199318717791 test loss: tensor(0.3886, device='cuda:0') test accuracy: 0.887186968149769 test AUC-ROC: 0.7051500538167697\n",
            "epoch: 20\n",
            "train_loss: tensor(0.2498, device='cuda:0') train_accuracy: 0.87575978604425 train AUC-ROC: 0.9829303869333565 test loss: tensor(0.4183, device='cuda:0') test accuracy: 0.87575978604425 test AUC-ROC: 0.7098690458603297\n",
            "epoch: 21\n",
            "train_loss: tensor(0.2703, device='cuda:0') train_accuracy: 0.8835399951373694 train AUC-ROC: 0.9860974757620753 test loss: tensor(0.3914, device='cuda:0') test accuracy: 0.8835399951373694 test AUC-ROC: 0.7144339742101599\n",
            "epoch: 22\n",
            "train_loss: tensor(0.1963, device='cuda:0') train_accuracy: 0.8645757354728908 train AUC-ROC: 0.987305441523277 test loss: tensor(0.4392, device='cuda:0') test accuracy: 0.8645757354728908 test AUC-ROC: 0.7151937403710191\n",
            "epoch: 23\n",
            "train_loss: tensor(0.2137, device='cuda:0') train_accuracy: 0.8815949428640895 train AUC-ROC: 0.9896523713861471 test loss: tensor(0.3936, device='cuda:0') test accuracy: 0.8815949428640895 test AUC-ROC: 0.7182866217841841\n",
            "epoch: 24\n",
            "train_loss: tensor(0.2091, device='cuda:0') train_accuracy: 0.8670070508144906 train AUC-ROC: 0.9871997554371561 test loss: tensor(0.4452, device='cuda:0') test accuracy: 0.8670070508144906 test AUC-ROC: 0.7094849418567841\n",
            "epoch: 25\n",
            "train_loss: tensor(0.2318, device='cuda:0') train_accuracy: 0.8825674690007294 train AUC-ROC: 0.9885116604070224 test loss: tensor(0.3871, device='cuda:0') test accuracy: 0.8825674690007294 test AUC-ROC: 0.7173263617753203\n",
            "epoch: 26\n",
            "train_loss: tensor(0.2097, device='cuda:0') train_accuracy: 0.8679795769511306 train AUC-ROC: 0.9869202550441085 test loss: tensor(0.4309, device='cuda:0') test accuracy: 0.8679795769511306 test AUC-ROC: 0.7138446278201042\n",
            "epoch: 27\n",
            "train_loss: tensor(0.3191, device='cuda:0') train_accuracy: 0.8901045465596887 train AUC-ROC: 0.9834789064547123 test loss: tensor(0.3772, device='cuda:0') test accuracy: 0.8901045465596887 test AUC-ROC: 0.7105095709431652\n",
            "epoch: 28\n",
            "train_loss: tensor(0.2546, device='cuda:0') train_accuracy: 0.8760029175784099 train AUC-ROC: 0.9842728622587125 test loss: tensor(0.4057, device='cuda:0') test accuracy: 0.8760029175784099 test AUC-ROC: 0.7121937192664035\n",
            "epoch: 29\n",
            "train_loss: tensor(0.2860, device='cuda:0') train_accuracy: 0.8898614150255288 train AUC-ROC: 0.9846650362477073 test loss: tensor(0.3704, device='cuda:0') test accuracy: 0.8898614150255288 test AUC-ROC: 0.7118486588016799\n",
            "epoch: 30\n",
            "train_loss: tensor(0.3324, device='cuda:0') train_accuracy: 0.8857281789448092 train AUC-ROC: 0.980198270591318 test loss: tensor(0.3931, device='cuda:0') test accuracy: 0.8857281789448092 test AUC-ROC: 0.7085542283097315\n",
            "epoch: 31\n",
            "train_loss: tensor(0.3257, device='cuda:0') train_accuracy: 0.8908339411621687 train AUC-ROC: 0.9824534893877195 test loss: tensor(0.3740, device='cuda:0') test accuracy: 0.8908339411621687 test AUC-ROC: 0.7029952725661103\n",
            "epoch: 32\n",
            "train_loss: tensor(0.2796, device='cuda:0') train_accuracy: 0.8774617067833698 train AUC-ROC: 0.9826207529041837 test loss: tensor(0.4072, device='cuda:0') test accuracy: 0.8774617067833698 test AUC-ROC: 0.7101170250933879\n",
            "epoch: 33\n",
            "train_loss: tensor(0.3404, device='cuda:0') train_accuracy: 0.8939946511062484 train AUC-ROC: 0.9807764870294348 test loss: tensor(0.3708, device='cuda:0') test accuracy: 0.8939946511062484 test AUC-ROC: 0.7068078213705338\n",
            "epoch: 34\n",
            "train_loss: tensor(0.3815, device='cuda:0') train_accuracy: 0.8937515195720885 train AUC-ROC: 0.9764311293562757 test loss: tensor(0.3782, device='cuda:0') test accuracy: 0.8937515195720885 test AUC-ROC: 0.7003097102336281\n",
            "epoch: 35\n",
            "train_loss: tensor(0.4061, device='cuda:0') train_accuracy: 0.8925358619012886 train AUC-ROC: 0.9708734387282734 test loss: tensor(0.3755, device='cuda:0') test accuracy: 0.8925358619012886 test AUC-ROC: 0.7031619990291877\n",
            "epoch: 36\n",
            "train_loss: tensor(0.4353, device='cuda:0') train_accuracy: 0.8981278871869681 train AUC-ROC: 0.969820945060704 test loss: tensor(0.3570, device='cuda:0') test accuracy: 0.8981278871869681 test AUC-ROC: 0.7029804993352047\n",
            "epoch: 37\n",
            "train_loss: tensor(0.5582, device='cuda:0') train_accuracy: 0.8986141502552881 train AUC-ROC: 0.9545191719800856 test loss: tensor(0.3643, device='cuda:0') test accuracy: 0.8986141502552881 test AUC-ROC: 0.6959188949623283\n",
            "epoch: 38\n",
            "train_loss: tensor(0.4400, device='cuda:0') train_accuracy: 0.888645757354729 train AUC-ROC: 0.9598392872739977 test loss: tensor(0.4062, device='cuda:0') test accuracy: 0.888645757354729 test AUC-ROC: 0.6959188949623283\n",
            "epoch: 39\n",
            "train_loss: tensor(0.5045, device='cuda:0') train_accuracy: 0.8978847556528081 train AUC-ROC: 0.9605948117739541 test loss: tensor(0.3626, device='cuda:0') test accuracy: 0.8978847556528081 test AUC-ROC: 0.70108741531773\n",
            "epoch: 40\n",
            "train_loss: tensor(0.3086, device='cuda:0') train_accuracy: 0.8815949428640895 train AUC-ROC: 0.9757856581360818 test loss: tensor(0.4052, device='cuda:0') test accuracy: 0.8815949428640895 test AUC-ROC: 0.702745182871494\n",
            "epoch: 41\n",
            "train_loss: tensor(0.3050, device='cuda:0') train_accuracy: 0.8901045465596887 train AUC-ROC: 0.9823757533409031 test loss: tensor(0.3595, device='cuda:0') test accuracy: 0.8901045465596887 test AUC-ROC: 0.7110667327944621\n",
            "epoch: 42\n",
            "train_loss: tensor(0.3696, device='cuda:0') train_accuracy: 0.886457573547289 train AUC-ROC: 0.969936238972836 test loss: tensor(0.3941, device='cuda:0') test accuracy: 0.886457573547289 test AUC-ROC: 0.7027198573327986\n",
            "epoch: 43\n",
            "train_loss: tensor(0.3289, device='cuda:0') train_accuracy: 0.8905908096280087 train AUC-ROC: 0.9786680059393832 test loss: tensor(0.3597, device='cuda:0') test accuracy: 0.8905908096280087 test AUC-ROC: 0.7073649832218307\n",
            "epoch: 44\n",
            "train_loss: tensor(0.4484, device='cuda:0') train_accuracy: 0.8939946511062484 train AUC-ROC: 0.9650117914228318 test loss: tensor(0.3811, device='cuda:0') test accuracy: 0.8939946511062484 test AUC-ROC: 0.6979343857501635\n",
            "epoch: 45\n",
            "train_loss: tensor(0.5772, device='cuda:0') train_accuracy: 0.899586676391928 train AUC-ROC: 0.9531653419512621 test loss: tensor(0.3648, device='cuda:0') test accuracy: 0.899586676391928 test AUC-ROC: 0.6936765295570142\n",
            "epoch: 46\n",
            "train_loss: tensor(0.4227, device='cuda:0') train_accuracy: 0.8857281789448092 train AUC-ROC: 0.9602803738317759 test loss: tensor(0.4136, device='cuda:0') test accuracy: 0.8857281789448092 test AUC-ROC: 0.6942421332545428\n",
            "epoch: 47\n",
            "train_loss: tensor(0.3966, device='cuda:0') train_accuracy: 0.8944809141745684 train AUC-ROC: 0.9713302471831601 test loss: tensor(0.3678, device='cuda:0') test accuracy: 0.8944809141745684 test AUC-ROC: 0.7050397822003671\n",
            "epoch: 48\n",
            "train_loss: tensor(0.4328, device='cuda:0') train_accuracy: 0.8964259664478483 train AUC-ROC: 0.9706297493230851 test loss: tensor(0.3557, device='cuda:0') test accuracy: 0.8964259664478483 test AUC-ROC: 0.6965098241985521\n",
            "epoch: 49\n",
            "train_loss: tensor(0.3309, device='cuda:0') train_accuracy: 0.8876732312180889 train AUC-ROC: 0.9753541794043148 test loss: tensor(0.3888, device='cuda:0') test accuracy: 0.8876732312180889 test AUC-ROC: 0.7057937445919423\n",
            "\n",
            "Running command: python kstonet.py --data_name hiv_pca_resample --base_path ./drive/MyDrive/result/ --regression_flag 0 --confidence_interval_flag 0 --layer 1 --unit 10 --sigma 0.0001 --C 0.1 --epsilon 0.1 --lr 5e-05 --alpha 0.1 --nepoch 50 --model_path model_layer_1_unit_10_sigma_0.0001_C_0.1_epsilon_0.1_lr_5e-05_alpha_0.1_nepoch_50_seed_4/ --seed 4\n",
            "Output:\n",
            "epoch: 0\n",
            "train_loss: tensor(2.3409, device='cuda:0') train_accuracy: 0.7930950644298566 train AUC-ROC: 0.8090628002445628 test loss: tensor(0.9410, device='cuda:0') test accuracy: 0.7930950644298566 test AUC-ROC: 0.6605000738661546\n",
            "epoch: 1\n",
            "train_loss: tensor(2.1204, device='cuda:0') train_accuracy: 0.7858011184050572 train AUC-ROC: 0.8311232422045594 test loss: tensor(0.8976, device='cuda:0') test accuracy: 0.7858011184050572 test AUC-ROC: 0.6657983876073696\n",
            "epoch: 2\n",
            "train_loss: tensor(1.7608, device='cuda:0') train_accuracy: 0.7819110138584975 train AUC-ROC: 0.8534828369289893 test loss: tensor(0.8255, device='cuda:0') test accuracy: 0.7819110138584975 test AUC-ROC: 0.6724716670535846\n",
            "epoch: 3\n",
            "train_loss: tensor(1.4077, device='cuda:0') train_accuracy: 0.7799659615852176 train AUC-ROC: 0.8721224561097038 test loss: tensor(0.7408, device='cuda:0') test accuracy: 0.7799659615852176 test AUC-ROC: 0.6783060380305173\n",
            "epoch: 4\n",
            "train_loss: tensor(1.0749, device='cuda:0') train_accuracy: 0.7843423292000973 train AUC-ROC: 0.8941597519434011 test loss: tensor(0.6652, device='cuda:0') test accuracy: 0.7843423292000973 test AUC-ROC: 0.6851281050165672\n",
            "epoch: 5\n",
            "train_loss: tensor(0.8320, device='cuda:0') train_accuracy: 0.8001458789204959 train AUC-ROC: 0.9129613940082103 test loss: tensor(0.5958, device='cuda:0') test accuracy: 0.8001458789204959 test AUC-ROC: 0.6912579406116117\n",
            "epoch: 6\n",
            "train_loss: tensor(0.6842, device='cuda:0') train_accuracy: 0.8232433746656942 train AUC-ROC: 0.9262271814132237 test loss: tensor(0.5500, device='cuda:0') test accuracy: 0.8232433746656942 test AUC-ROC: 0.6933029778612583\n",
            "epoch: 7\n",
            "train_loss: tensor(0.5269, device='cuda:0') train_accuracy: 0.838317529783613 train AUC-ROC: 0.9431417591055987 test loss: tensor(0.5219, device='cuda:0') test accuracy: 0.838317529783613 test AUC-ROC: 0.69835858852331\n",
            "epoch: 8\n",
            "train_loss: tensor(0.3938, device='cuda:0') train_accuracy: 0.8708971553610503 train AUC-ROC: 0.9612686697528169 test loss: tensor(0.4599, device='cuda:0') test accuracy: 0.8708971553610503 test AUC-ROC: 0.7053642656649011\n",
            "epoch: 9\n",
            "train_loss: tensor(0.2399, device='cuda:0') train_accuracy: 0.8519328956965718 train AUC-ROC: 0.9780710105686087 test loss: tensor(0.4848, device='cuda:0') test accuracy: 0.8519328956965718 test AUC-ROC: 0.7067455627545743\n",
            "epoch: 10\n",
            "train_loss: tensor(0.2255, device='cuda:0') train_accuracy: 0.8696814976902504 train AUC-ROC: 0.9841278714298193 test loss: tensor(0.4398, device='cuda:0') test accuracy: 0.8696814976902504 test AUC-ROC: 0.7146207500580377\n",
            "epoch: 11\n",
            "train_loss: tensor(0.1284, device='cuda:0') train_accuracy: 0.8468271334792122 train AUC-ROC: 0.9913381081317144 test loss: tensor(0.4780, device='cuda:0') test accuracy: 0.8468271334792122 test AUC-ROC: 0.71472099698204\n",
            "epoch: 12\n",
            "train_loss: tensor(0.1580, device='cuda:0') train_accuracy: 0.8764891806467299 train AUC-ROC: 0.9914245785658136 test loss: tensor(0.3948, device='cuda:0') test accuracy: 0.8764891806467299 test AUC-ROC: 0.720318996264483\n",
            "epoch: 13\n",
            "train_loss: tensor(0.1413, device='cuda:0') train_accuracy: 0.8417213712618526 train AUC-ROC: 0.9903882435147174 test loss: tensor(0.5014, device='cuda:0') test accuracy: 0.8417213712618526 test AUC-ROC: 0.7123335373446174\n",
            "epoch: 14\n",
            "train_loss: tensor(0.2083, device='cuda:0') train_accuracy: 0.8845125212740093 train AUC-ROC: 0.9894016944711329 test loss: tensor(0.3722, device='cuda:0') test accuracy: 0.8845125212740093 test AUC-ROC: 0.7171606905430219\n",
            "epoch: 15\n",
            "train_loss: tensor(0.1958, device='cuda:0') train_accuracy: 0.8611718939946511 train AUC-ROC: 0.9868481963490262 test loss: tensor(0.4535, device='cuda:0') test accuracy: 0.8611718939946511 test AUC-ROC: 0.7099408015532997\n",
            "epoch: 16\n",
            "train_loss: tensor(0.2509, device='cuda:0') train_accuracy: 0.8825674690007294 train AUC-ROC: 0.9865285177744781 test loss: tensor(0.3778, device='cuda:0') test accuracy: 0.8825674690007294 test AUC-ROC: 0.7075908026085305\n",
            "epoch: 17\n",
            "train_loss: tensor(0.2185, device='cuda:0') train_accuracy: 0.8730853391684902 train AUC-ROC: 0.9862092759192943 test loss: tensor(0.4162, device='cuda:0') test accuracy: 0.8730853391684902 test AUC-ROC: 0.7123150708059852\n",
            "epoch: 18\n",
            "train_loss: tensor(0.2605, device='cuda:0') train_accuracy: 0.8847556528081693 train AUC-ROC: 0.9850982618569308 test loss: tensor(0.3796, device='cuda:0') test accuracy: 0.8847556528081693 test AUC-ROC: 0.7126063145009813\n",
            "epoch: 19\n",
            "train_loss: tensor(0.3275, device='cuda:0') train_accuracy: 0.8840262582056893 train AUC-ROC: 0.9795082539959823 test loss: tensor(0.3868, device='cuda:0') test accuracy: 0.8840262582056893 test AUC-ROC: 0.7034110334930249\n",
            "epoch: 20\n",
            "train_loss: tensor(0.1615, device='cuda:0') train_accuracy: 0.8594699732555312 train AUC-ROC: 0.989456721111014 test loss: tensor(0.4447, device='cuda:0') test accuracy: 0.8594699732555312 test AUC-ROC: 0.7181030116286431\n",
            "epoch: 21\n",
            "train_loss: tensor(0.2043, device='cuda:0') train_accuracy: 0.8913202042304887 train AUC-ROC: 0.9907205869508255 test loss: tensor(0.3478, device='cuda:0') test accuracy: 0.8913202042304887 test AUC-ROC: 0.7140197961294135\n",
            "epoch: 22\n",
            "train_loss: tensor(0.2115, device='cuda:0') train_accuracy: 0.8699246292244104 train AUC-ROC: 0.9860459428771071 test loss: tensor(0.4232, device='cuda:0') test accuracy: 0.8699246292244104 test AUC-ROC: 0.7130030812738746\n",
            "epoch: 23\n",
            "train_loss: tensor(0.3682, device='cuda:0') train_accuracy: 0.8961828349136883 train AUC-ROC: 0.9791379159751943 test loss: tensor(0.3471, device='cuda:0') test accuracy: 0.8961828349136883 test AUC-ROC: 0.7027747293333052\n",
            "epoch: 24\n",
            "train_loss: tensor(0.3387, device='cuda:0') train_accuracy: 0.8835399951373694 train AUC-ROC: 0.9748768451393135 test loss: tensor(0.3901, device='cuda:0') test accuracy: 0.8835399951373694 test AUC-ROC: 0.7066031065994133\n",
            "epoch: 25\n",
            "train_loss: tensor(0.2791, device='cuda:0') train_accuracy: 0.8908339411621687 train AUC-ROC: 0.9832684077211983 test loss: tensor(0.3664, device='cuda:0') test accuracy: 0.8908339411621687 test AUC-ROC: 0.7106910706371484\n",
            "epoch: 26\n",
            "train_loss: tensor(0.2367, device='cuda:0') train_accuracy: 0.8745441283734501 train AUC-ROC: 0.9851541619355403 test loss: tensor(0.3921, device='cuda:0') test accuracy: 0.8745441283734501 test AUC-ROC: 0.7099207521684991\n",
            "epoch: 27\n",
            "train_loss: tensor(0.3852, device='cuda:0') train_accuracy: 0.8966690979820082 train AUC-ROC: 0.9780229714385535 test loss: tensor(0.3512, device='cuda:0') test accuracy: 0.8966690979820082 test AUC-ROC: 0.7076710001477324\n",
            "epoch: 28\n",
            "train_loss: tensor(0.3034, device='cuda:0') train_accuracy: 0.8840262582056893 train AUC-ROC: 0.9799388592890209 test loss: tensor(0.3820, device='cuda:0') test accuracy: 0.8840262582056893 test AUC-ROC: 0.7033192284152544\n",
            "epoch: 29\n",
            "train_loss: tensor(0.3217, device='cuda:0') train_accuracy: 0.8935083880379285 train AUC-ROC: 0.981579177220718 test loss: tensor(0.3586, device='cuda:0') test accuracy: 0.8935083880379285 test AUC-ROC: 0.7080878163054259\n",
            "epoch: 30\n",
            "train_loss: tensor(0.2823, device='cuda:0') train_accuracy: 0.8849987843423291 train AUC-ROC: 0.9833352257839113 test loss: tensor(0.3749, device='cuda:0') test accuracy: 0.8849987843423291 test AUC-ROC: 0.7091156110841441\n",
            "epoch: 31\n",
            "train_loss: tensor(0.3208, device='cuda:0') train_accuracy: 0.8944809141745684 train AUC-ROC: 0.9811354703467552 test loss: tensor(0.3567, device='cuda:0') test accuracy: 0.8944809141745684 test AUC-ROC: 0.7092095266234725\n",
            "epoch: 32\n",
            "train_loss: tensor(0.2444, device='cuda:0') train_accuracy: 0.8777048383175298 train AUC-ROC: 0.9851707572713774 test loss: tensor(0.3884, device='cuda:0') test accuracy: 0.8777048383175298 test AUC-ROC: 0.7067170715235421\n",
            "epoch: 33\n",
            "train_loss: tensor(0.3754, device='cuda:0') train_accuracy: 0.8956965718453683 train AUC-ROC: 0.9760599178967596 test loss: tensor(0.3564, device='cuda:0') test accuracy: 0.8956965718453683 test AUC-ROC: 0.7037328788806111\n",
            "epoch: 34\n",
            "train_loss: tensor(0.3674, device='cuda:0') train_accuracy: 0.8893751519572088 train AUC-ROC: 0.9740335400471658 test loss: tensor(0.3783, device='cuda:0') test accuracy: 0.8893751519572088 test AUC-ROC: 0.7023515817909377\n",
            "epoch: 35\n",
            "train_loss: tensor(0.3372, device='cuda:0') train_accuracy: 0.8910770726963287 train AUC-ROC: 0.9780234081579177 test loss: tensor(0.3627, device='cuda:0') test accuracy: 0.8910770726963287 test AUC-ROC: 0.7069724373720533\n",
            "epoch: 36\n",
            "train_loss: tensor(0.2888, device='cuda:0') train_accuracy: 0.8845125212740093 train AUC-ROC: 0.980009607826011 test loss: tensor(0.3903, device='cuda:0') test accuracy: 0.8845125212740093 test AUC-ROC: 0.707239410759133\n",
            "epoch: 37\n",
            "train_loss: tensor(0.2675, device='cuda:0') train_accuracy: 0.8879163627522489 train AUC-ROC: 0.9838806882697179 test loss: tensor(0.3674, device='cuda:0') test accuracy: 0.8879163627522489 test AUC-ROC: 0.7081120866133424\n",
            "epoch: 38\n",
            "train_loss: tensor(0.3267, device='cuda:0') train_accuracy: 0.8847556528081693 train AUC-ROC: 0.9757777971875272 test loss: tensor(0.3895, device='cuda:0') test accuracy: 0.8847556528081693 test AUC-ROC: 0.7053727075111327\n",
            "epoch: 39\n",
            "train_loss: tensor(0.4193, device='cuda:0') train_accuracy: 0.8947240457087284 train AUC-ROC: 0.9686994497336012 test loss: tensor(0.3572, device='cuda:0') test accuracy: 0.8947240457087284 test AUC-ROC: 0.6994824093029146\n",
            "epoch: 40\n",
            "train_loss: tensor(0.3687, device='cuda:0') train_accuracy: 0.8937515195720885 train AUC-ROC: 0.9720988732640404 test loss: tensor(0.3624, device='cuda:0') test accuracy: 0.8937515195720885 test AUC-ROC: 0.7009365173163371\n",
            "epoch: 41\n",
            "train_loss: tensor(0.4554, device='cuda:0') train_accuracy: 0.8944809141745684 train AUC-ROC: 0.9594431828107258 test loss: tensor(0.3677, device='cuda:0') test accuracy: 0.8944809141745684 test AUC-ROC: 0.6968300867399699\n",
            "epoch: 42\n",
            "train_loss: tensor(0.4616, device='cuda:0') train_accuracy: 0.8903476780938487 train AUC-ROC: 0.9510455061577431 test loss: tensor(0.3855, device='cuda:0') test accuracy: 0.8903476780938487 test AUC-ROC: 0.6918309309245931\n",
            "epoch: 43\n",
            "train_loss: tensor(0.3902, device='cuda:0') train_accuracy: 0.886700705081449 train AUC-ROC: 0.9610524936675693 test loss: tensor(0.3894, device='cuda:0') test accuracy: 0.886700705081449 test AUC-ROC: 0.6969002595867716\n",
            "epoch: 44\n",
            "train_loss: tensor(0.5001, device='cuda:0') train_accuracy: 0.8956965718453683 train AUC-ROC: 0.9484120883919994 test loss: tensor(0.3756, device='cuda:0') test accuracy: 0.8956965718453683 test AUC-ROC: 0.691579785999198\n",
            "epoch: 45\n",
            "train_loss: tensor(0.5488, device='cuda:0') train_accuracy: 0.8947240457087284 train AUC-ROC: 0.9422517250414884 test loss: tensor(0.3767, device='cuda:0') test accuracy: 0.8947240457087284 test AUC-ROC: 0.6917043032311166\n",
            "epoch: 46\n",
            "train_loss: tensor(0.3835, device='cuda:0') train_accuracy: 0.8813518113299295 train AUC-ROC: 0.9589920517075727 test loss: tensor(0.4212, device='cuda:0') test accuracy: 0.8813518113299295 test AUC-ROC: 0.6952530443407973\n",
            "epoch: 47\n",
            "train_loss: tensor(0.3185, device='cuda:0') train_accuracy: 0.8835399951373694 train AUC-ROC: 0.97531356450345 test loss: tensor(0.3852, device='cuda:0') test accuracy: 0.8835399951373694 test AUC-ROC: 0.7032348099529366\n",
            "epoch: 48\n",
            "train_loss: tensor(0.2482, device='cuda:0') train_accuracy: 0.8762460491125699 train AUC-ROC: 0.9798226919381605 test loss: tensor(0.4123, device='cuda:0') test accuracy: 0.8762460491125699 test AUC-ROC: 0.7064379629825043\n",
            "epoch: 49\n",
            "train_loss: tensor(0.2678, device='cuda:0') train_accuracy: 0.8803792851932896 train AUC-ROC: 0.9807310682155647 test loss: tensor(0.3854, device='cuda:0') test accuracy: 0.8803792851932896 test AUC-ROC: 0.7102383766329696\n",
            "\n",
            "Running command: python kstonet.py --data_name hiv_pca_resample --base_path ./drive/MyDrive/result/ --regression_flag 0 --confidence_interval_flag 0 --layer 1 --unit 20 --sigma 0.0001 --C 0.1 --epsilon 0.1 --lr 5e-05 --alpha 0.1 --nepoch 50 --model_path model_layer_1_unit_20_sigma_0.0001_C_0.1_epsilon_0.1_lr_5e-05_alpha_0.1_nepoch_50_seed_4/ --seed 4\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "from itertools import product\n",
        "\n",
        "def run_experiments(parameters):\n",
        "    # Extract the individual hyperparameter lists\n",
        "    data_names = parameters['data_name']\n",
        "    base_path = parameters[\"base_path\"]\n",
        "    regression_flags = parameters['regression_flag']\n",
        "    confidence_interval_flags = parameters['confidence_interval_flag']\n",
        "    layers = parameters['layer']\n",
        "    units = parameters['unit']\n",
        "    sigmas = parameters['sigma']\n",
        "    Cs = parameters['C']\n",
        "    epsilons = parameters['epsilon']\n",
        "    lrs = parameters['lr']\n",
        "    alphas = parameters['alpha']\n",
        "    nepochs = parameters['nepoch']\n",
        "    seeds = parameters['seed']\n",
        "\n",
        "    # Create all combinations using product from itertools\n",
        "    all_combinations = product(\n",
        "        data_names,\n",
        "        base_path,\n",
        "        regression_flags,\n",
        "        confidence_interval_flags,\n",
        "        layers,\n",
        "        units,\n",
        "        sigmas,\n",
        "        Cs,\n",
        "        epsilons,\n",
        "        lrs,\n",
        "        alphas,\n",
        "        nepochs,\n",
        "        seeds\n",
        "    )\n",
        "\n",
        "    # Iterate through every combination and call the system command\n",
        "    for combination in all_combinations:\n",
        "        model_path = f\"{parameters['model_path']}_layer_{str(combination[4])}_unit_{str(combination[5])}_sigma_{str(combination[6])}_C_{str(combination[7])}_epsilon_{str(combination[8])}_lr_{str(combination[9])}_alpha_{str(combination[10])}_nepoch_{str(combination[11])}_seed_{str(combination[12])}/\"\n",
        "        # Construct the command\n",
        "        cmd = [\n",
        "            \"python\", \"kstonet.py\",\n",
        "            \"--data_name\", combination[0],\n",
        "            '--base_path', combination[1],\n",
        "            \"--regression_flag\", str(combination[2]),\n",
        "            \"--confidence_interval_flag\", str(combination[3]),\n",
        "            \"--layer\", str(combination[4]),\n",
        "            \"--unit\", str(combination[5]),\n",
        "            \"--sigma\", str(combination[6]),\n",
        "            \"--C\", str(combination[7]),\n",
        "            \"--epsilon\", str(combination[8]),\n",
        "            \"--lr\", str(combination[9]),\n",
        "            \"--alpha\", str(combination[10]),\n",
        "            \"--nepoch\", str(combination[11]),\n",
        "            \"--model_path\", model_path,\n",
        "            \"--seed\", str(combination[12])\n",
        "        ]\n",
        "\n",
        "        print(f\"Running command: {' '.join(cmd)}\")\n",
        "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "        # Print standard output\n",
        "        if result.stdout:\n",
        "            print(\"Output:\")\n",
        "            print(result.stdout)\n",
        "\n",
        "        # Print any error messages\n",
        "        if result.stderr:\n",
        "            print(\"Errors:\")\n",
        "            print(result.stderr)\n",
        "\n",
        "# Sample parameter ranges\n",
        "parameter_ranges = {\n",
        "    'data_name': ['hiv_pca_resample'],\n",
        "    'base_path': ['./drive/MyDrive/result/'],\n",
        "    'regression_flag': [0],\n",
        "    'confidence_interval_flag': [0],\n",
        "    'layer': [0, 1],\n",
        "    'unit': [5, 10, 20],\n",
        "    'sigma': [0.0001],\n",
        "    'C': [0.1],\n",
        "    'epsilon': [0.1],\n",
        "    'lr': [0.00005],\n",
        "    'alpha': [0.1],\n",
        "    'nepoch': [50],\n",
        "    'model_path': \"model\",\n",
        "    'seed': [4]\n",
        "}\n",
        "\n",
        "# Run the experiments with the specified parameter ranges\n",
        "run_experiments(parameter_ranges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VXaBSR3tmUNu"
      },
      "outputs": [],
      "source": [
        "!mv /content/drive/MyDrive/result/hiv_pca_resample/0/ /content/drive/MyDrive/result/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "AJUJlQK0sUuj"
      },
      "outputs": [],
      "source": [
        "!rm -r drive/MyDrive/result/hiv_pca_resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAExS4sKZr1N",
        "outputId": "096b1f89-6db7-4e25-9e98-1407a7e2df9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DNN with layers: [5, 5]\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 8s 17ms/step - loss: 0.6945 - auc: 0.6574\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.5962 - auc: 0.8095\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.5418 - auc: 0.8655\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.4956 - auc: 0.8975\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.4529 - auc: 0.9142\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.4115 - auc: 0.9313\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.3725 - auc: 0.9396\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.3377 - auc: 0.9498\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.3030 - auc: 0.9580\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.2717 - auc: 0.9651\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2445 - auc: 0.9701\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2207 - auc: 0.9757\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.2003 - auc: 0.9809\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1796 - auc: 0.9851\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1590 - auc: 0.9886\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1434 - auc: 0.9918\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1292 - auc: 0.9933\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1185 - auc: 0.9949\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1054 - auc: 0.9964\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0970 - auc: 0.9971\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0905 - auc: 0.9976\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0860 - auc: 0.9980\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0821 - auc: 0.9983\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0781 - auc: 0.9986\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0751 - auc: 0.9987\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0714 - auc: 0.9989\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0700 - auc: 0.9990\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0671 - auc: 0.9991\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0652 - auc: 0.9991\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 0s 2ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0626 - auc: 0.9992\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0608 - auc: 0.9993\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0596 - auc: 0.9993\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0571 - auc: 0.9994\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0560 - auc: 0.9994\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0564 - auc: 0.9993\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0541 - auc: 0.9995\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0544 - auc: 0.9995\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0570 - auc: 0.9988\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0529 - auc: 0.9994\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0517 - auc: 0.9994\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0510 - auc: 0.9995\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0503 - auc: 0.9994\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0488 - auc: 0.9995\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0522 - auc: 0.9989\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0611 - auc: 0.9983\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.0596 - auc: 0.9984\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0559 - auc: 0.9994\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0501 - auc: 0.9996\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0487 - auc: 0.9996\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0536 - auc: 0.9988\n",
            "Training DNN with layers: [10, 5]\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 2s 15ms/step - loss: 0.7394 - auc_1: 0.6211\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.5685 - auc_1: 0.8374\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.4903 - auc_1: 0.8941\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.4210 - auc_1: 0.9290\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.3594 - auc_1: 0.9498\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.3058 - auc_1: 0.9678\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2590 - auc_1: 0.9801\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2195 - auc_1: 0.9879\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1880 - auc_1: 0.9930\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1600 - auc_1: 0.9961\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1395 - auc_1: 0.9975\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1227 - auc_1: 0.9985\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1066 - auc_1: 0.9993\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0968 - auc_1: 0.9995\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0864 - auc_1: 0.9998\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0803 - auc_1: 0.9998\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0755 - auc_1: 0.9998\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0716 - auc_1: 0.9998\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0674 - auc_1: 0.9999\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0635 - auc_1: 1.0000\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0599 - auc_1: 1.0000\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0579 - auc_1: 1.0000\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0564 - auc_1: 1.0000\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0543 - auc_1: 1.0000\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0530 - auc_1: 1.0000\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0521 - auc_1: 1.0000\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0513 - auc_1: 1.0000\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0509 - auc_1: 1.0000\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0503 - auc_1: 0.9999\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0489 - auc_1: 1.0000\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0463 - auc_1: 1.0000\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0453 - auc_1: 1.0000\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0444 - auc_1: 1.0000\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0438 - auc_1: 1.0000\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0457 - auc_1: 0.9999\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0422 - auc_1: 1.0000\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0429 - auc_1: 1.0000\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0407 - auc_1: 1.0000\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0402 - auc_1: 1.0000\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0387 - auc_1: 1.0000\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0379 - auc_1: 1.0000\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0373 - auc_1: 1.0000\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0364 - auc_1: 1.0000\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0359 - auc_1: 1.0000\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0355 - auc_1: 1.0000\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0347 - auc_1: 1.0000\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0340 - auc_1: 1.0000\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0334 - auc_1: 1.0000\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0325 - auc_1: 1.0000\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0320 - auc_1: 1.0000\n",
            "Training DNN with layers: [50, 5]\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 2s 14ms/step - loss: 0.8669 - auc_2: 0.6944\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.5411 - auc_2: 0.9353\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.4287 - auc_2: 0.9816\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.3580 - auc_2: 0.9937\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.3082 - auc_2: 0.9973\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2717 - auc_2: 0.9983\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2437 - auc_2: 0.9995\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2281 - auc_2: 0.9994\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2125 - auc_2: 0.9997\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2031 - auc_2: 0.9991\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1917 - auc_2: 0.9999\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1832 - auc_2: 0.9999\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1729 - auc_2: 1.0000\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1654 - auc_2: 0.9999\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1570 - auc_2: 1.0000\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.1498 - auc_2: 1.0000\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1450 - auc_2: 0.9998\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1420 - auc_2: 0.9993\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1317 - auc_2: 1.0000\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 0s 2ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.1250 - auc_2: 1.0000\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1194 - auc_2: 1.0000\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1127 - auc_2: 1.0000\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1096 - auc_2: 1.0000\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1032 - auc_2: 1.0000\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0986 - auc_2: 1.0000\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0970 - auc_2: 0.9994\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0945 - auc_2: 1.0000\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0957 - auc_2: 0.9993\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0925 - auc_2: 1.0000\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.1141 - auc_2: 0.9985\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.2322 - auc_2: 0.9876\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.3238 - auc_2: 0.9766\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.2185 - auc_2: 0.9935\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1578 - auc_2: 0.9997\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 2ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.1392 - auc_2: 0.9994\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1275 - auc_2: 1.0000\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1206 - auc_2: 1.0000\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1157 - auc_2: 1.0000\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1100 - auc_2: 1.0000\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1044 - auc_2: 1.0000\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1002 - auc_2: 1.0000\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0971 - auc_2: 1.0000\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0927 - auc_2: 1.0000\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0903 - auc_2: 1.0000\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0891 - auc_2: 1.0000\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0888 - auc_2: 1.0000\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0814 - auc_2: 1.0000\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0792 - auc_2: 1.0000\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0757 - auc_2: 1.0000\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0732 - auc_2: 1.0000\n",
            "Training DNN with layers: [15, 10, 5, 5]\n",
            "Epoch 1/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 2s 16ms/step - loss: 0.7404 - auc_3: 0.6296\n",
            "Epoch 2/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.6442 - auc_3: 0.8019\n",
            "Epoch 3/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.5392 - auc_3: 0.8828\n",
            "Epoch 4/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.4261 - auc_3: 0.9317\n",
            "Epoch 5/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.3282 - auc_3: 0.9628\n",
            "Epoch 6/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.2538 - auc_3: 0.9825\n",
            "Epoch 7/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.1957 - auc_3: 0.9913\n",
            "Epoch 8/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 2ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.1546 - auc_3: 0.9952\n",
            "Epoch 9/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.1302 - auc_3: 0.9974\n",
            "Epoch 10/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1074 - auc_3: 0.9992\n",
            "Epoch 11/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0959 - auc_3: 0.9995\n",
            "Epoch 12/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.0897 - auc_3: 0.9997\n",
            "Epoch 13/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0826 - auc_3: 0.9998\n",
            "Epoch 14/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0789 - auc_3: 0.9999\n",
            "Epoch 15/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0774 - auc_3: 0.9999\n",
            "Epoch 16/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0762 - auc_3: 0.9994\n",
            "Epoch 17/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0747 - auc_3: 0.9998\n",
            "Epoch 18/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0718 - auc_3: 0.9994\n",
            "Epoch 19/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0679 - auc_3: 0.9999\n",
            "Epoch 20/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0645 - auc_3: 1.0000\n",
            "Epoch 21/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0613 - auc_3: 1.0000\n",
            "Epoch 22/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0598 - auc_3: 1.0000\n",
            "Epoch 23/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0577 - auc_3: 1.0000\n",
            "Epoch 24/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0567 - auc_3: 1.0000\n",
            "Epoch 25/50\n",
            "67/67 [==============================] - 0s 2ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0548 - auc_3: 1.0000\n",
            "Epoch 26/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 2ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.0525 - auc_3: 1.0000\n",
            "Epoch 27/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0506 - auc_3: 1.0000\n",
            "Epoch 28/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0493 - auc_3: 1.0000\n",
            "Epoch 29/50\n",
            "67/67 [==============================] - 0s 2ms/step\n",
            "129/129 [==============================] - 0s 2ms/step\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.0478 - auc_3: 1.0000\n",
            "Epoch 30/50\n",
            "67/67 [==============================] - 0s 2ms/step\n",
            "129/129 [==============================] - 0s 2ms/step\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.0465 - auc_3: 1.0000\n",
            "Epoch 31/50\n",
            "67/67 [==============================] - 0s 2ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.0455 - auc_3: 1.0000\n",
            "Epoch 32/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0445 - auc_3: 1.0000\n",
            "Epoch 33/50\n",
            "67/67 [==============================] - 0s 2ms/step\n",
            "129/129 [==============================] - 0s 2ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.0422 - auc_3: 1.0000\n",
            "Epoch 34/50\n",
            "67/67 [==============================] - 0s 2ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.0409 - auc_3: 1.0000\n",
            "Epoch 35/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0396 - auc_3: 1.0000\n",
            "Epoch 36/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0385 - auc_3: 1.0000\n",
            "Epoch 37/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0377 - auc_3: 1.0000\n",
            "Epoch 38/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.0369 - auc_3: 1.0000\n",
            "Epoch 39/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0356 - auc_3: 1.0000\n",
            "Epoch 40/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0346 - auc_3: 1.0000\n",
            "Epoch 41/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0336 - auc_3: 1.0000\n",
            "Epoch 42/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0327 - auc_3: 1.0000\n",
            "Epoch 43/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0318 - auc_3: 1.0000\n",
            "Epoch 44/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0311 - auc_3: 1.0000\n",
            "Epoch 45/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.0408 - auc_3: 0.9995\n",
            "Epoch 46/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.7160 - auc_3: 0.9027\n",
            "Epoch 47/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.3957 - auc_3: 0.9379\n",
            "Epoch 48/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.2070 - auc_3: 0.9885\n",
            "Epoch 49/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.1486 - auc_3: 0.9968\n",
            "Epoch 50/50\n",
            "67/67 [==============================] - 0s 1ms/step\n",
            "129/129 [==============================] - 0s 1ms/step\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 0.1184 - auc_3: 0.9989\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "# Define custom callback for roc-auc\n",
        "class RocAucCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, train_data, valid_data, test_data, result_dicts):\n",
        "        self.X_train, self.y_train, self.w_train = train_data\n",
        "        self.X_valid, self.y_valid, self.w_valid = valid_data\n",
        "        self.X_test, self.y_test, self.w_test = test_data\n",
        "        self.result_dicts = result_dicts\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_time = time.time()  # Store start time\n",
        "\n",
        "        # Predict probabilities for training set\n",
        "        train_y_pred = self.model.predict(self.X_train).flatten()\n",
        "        # Calculate ROC-AUC for training set\n",
        "        train_auc = roc_auc_score(self.y_train, train_y_pred, sample_weight=self.w_train)\n",
        "\n",
        "        # Predict probabilities for test set\n",
        "        test_y_pred = self.model.predict(self.X_test).flatten()\n",
        "        # Calculate ROC-AUC for test set\n",
        "        test_auc = roc_auc_score(self.y_test, test_y_pred, sample_weight=self.w_test)\n",
        "\n",
        "        elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
        "\n",
        "        self.result_dicts.append({\n",
        "            'train_loss_path': logs.get('loss'),  # Training loss for this epoch\n",
        "            'test_loss_path': None,  # Test loss is not computed within this callback\n",
        "            'train_accuracy_path': logs.get('auc'),  # Training AUC for this epoch\n",
        "            'test_accuracy_path': test_auc,  # Test AUC computed within the callback\n",
        "            'test_auc_roc_path': test_auc,  # Redundant with 'test_accuracy_path' for compatibility\n",
        "            'train_auc_roc_path': train_auc,  # Redundant with 'train_accuracy_path' for compatibility\n",
        "            'time_used_path': elapsed_time\n",
        "        })\n",
        "        # Call parent method if defined\n",
        "        super().on_epoch_end(epoch, logs)\n",
        "\n",
        "\n",
        "# Define function to create feed-forward DNN model with L1 regularization\n",
        "def create_dnn_model(layers, input_shape, l1_penalty):\n",
        "    model = tf.keras.Sequential()\n",
        "    for layer_size in layers:\n",
        "        model.add(tf.keras.layers.Dense(layer_size, activation='relu',\n",
        "                                        kernel_regularizer=tf.keras.regularizers.l1(l1_penalty)))\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # Assume binary classification\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
        "    return model\n",
        "\n",
        "# Define the main function to load data, train, validate and evaluate DNN models\n",
        "def train_and_evaluate_dnn_models(layer_configurations, data_paths, l1_penalty, dataset):\n",
        "    train_data_path, train_labels_path, valid_data_path, valid_labels_path, test_data_path, test_labels_path = data_paths\n",
        "\n",
        "    # Load data from paths\n",
        "    X_train = np.load(train_data_path)\n",
        "    y_train = np.load(train_labels_path)\n",
        "    X_valid = np.load(valid_data_path)\n",
        "    y_valid = np.load(valid_labels_path)\n",
        "    X_test = np.load(test_data_path)\n",
        "    y_test = np.load(test_labels_path)\n",
        "\n",
        "    # Sample weights for all datasets\n",
        "    w_train = np.ones_like(y_train)\n",
        "    w_valid = np.ones_like(y_valid)\n",
        "    w_test = np.ones_like(y_test)\n",
        "\n",
        "    # Number of features\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    # Directory to store results\n",
        "    results_directory = os.path.join(\"drive\", \"MyDrive\", \"result\", dataset, \"0\")\n",
        "    if not os.path.exists(results_directory):\n",
        "        os.makedirs(results_directory)\n",
        "\n",
        "    # List to store results across all epochs\n",
        "    results = []\n",
        "\n",
        "    # Train, validate and evaluate DNN configurations\n",
        "    for layers in layer_configurations:\n",
        "        print(\"Training DNN with layers:\", layers)\n",
        "\n",
        "        results_subdirectory = os.path.join(results_directory, \"DNN-{}\".format('-'.join(str(l) for l in layers)))\n",
        "        os.makedirs(results_subdirectory, exist_ok=True)\n",
        "        results_file = os.path.join(results_subdirectory, \"result.txt\")\n",
        "\n",
        "        # Create DNN model with L1 regularization\n",
        "        model = create_dnn_model(layers, input_shape=(n_features,), l1_penalty=l1_penalty)\n",
        "\n",
        "        # Define callbacks\n",
        "        roc_auc_callback = RocAucCallback(\n",
        "            train_data=(X_train, y_train, w_train),\n",
        "            valid_data=(X_valid, y_valid, w_valid),\n",
        "            test_data=(X_test, y_test, w_test),\n",
        "            result_dicts=results\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train, sample_weight=w_train, batch_size=50, epochs=50, verbose=1, callbacks=[roc_auc_callback])\n",
        "\n",
        "        # Save results after model training\n",
        "        results_subdirectory = os.path.join(results_directory, \"DNN-{}\".format('-'.join(str(l) for l in layers)))\n",
        "        os.makedirs(results_subdirectory, exist_ok=True)\n",
        "\n",
        "        # Before saving, convert the result dictionaries to a list of tuples\n",
        "        # Expected tuple format: (train_loss, test_loss, train_accuracy, test_accuracy, test_auc, train_auc, time_used)\n",
        "        results_to_save = [\n",
        "            [result_dict['train_loss_path'] for result_dict in results],\n",
        "            [None]*len(results),  # Placeholder for test_loss_path which is not available\n",
        "            [result_dict['train_accuracy_path'] for result_dict in results],\n",
        "            [result_dict['test_accuracy_path'] for result_dict in results],\n",
        "            [result_dict['test_auc_roc_path'] for result_dict in results],\n",
        "            [result_dict['train_auc_roc_path'] for result_dict in results],\n",
        "            [result_dict['time_used_path'] for result_dict in results],\n",
        "        ]\n",
        "\n",
        "        with open(results_file, 'wb') as file:\n",
        "            pickle.dump(results_to_save, file)\n",
        "\n",
        "        results.clear()  # Clear results for the next configuration (Optional)\n",
        "\n",
        "# Example usage of the function with datapaths and L1 penalization\n",
        "layer_configurations = [[5, 5], [10, 5], [50, 5], [15, 10, 5, 5]]\n",
        "datapaths = [\n",
        "    'drive/MyDrive/result/data/hiv/x_train_resample.npy',\n",
        "    'drive/MyDrive/result/data/hiv/y_train_resample.npy',\n",
        "    'drive/MyDrive/result/data/hiv/x_valid.npy',\n",
        "    'drive/MyDrive/result/data/hiv/y_valid.npy',\n",
        "    'drive/MyDrive/result/data/hiv/x_test.npy',\n",
        "    'drive/MyDrive/result/data/hiv/y_test.npy'\n",
        "]\n",
        "l1_penalty = 0.0001\n",
        "dataset = 'hiv_pca_resample_DNN+1_layer'\n",
        "train_and_evaluate_dnn_models(layer_configurations, datapaths, l1_penalty, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6iUjeEiPrHL",
        "outputId": "5ca1d20f-50e0-4a39-c1f4-ee7ea688633a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/kstonet.py\", line 352, in <module>\n",
            "    main()\n",
            "  File \"/content/kstonet.py\", line 89, in main\n",
            "    x_train, y_train, x_test, y_test = preprocess_data(data_name, cross_validate_index, seed = args.seed)\n",
            "  File \"/content/process_data.py\", line 692, in preprocess_data\n",
            "    return x_train, y_train, x_test, y_test\n",
            "UnboundLocalError: local variable 'x_train' referenced before assignment\n"
          ]
        }
      ],
      "source": [
        " !python kstonet.py --data_name 'hiv' --regression_flag 0 --confidence_interval_flag 1 --layer 1 --unit 10 --sigma 0.01 --C 1 --epsilon 0.05 --lr 5e-05 --alpha 0.1 --nepoch 2 --model_path '10_unit_with_auc/' --seed 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8RXTyegAGPi"
      },
      "source": [
        "## Analyzing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sggLaIQN5tTZ",
        "outputId": "cda8f6c7-2cbe-44be-f977-b73e3d8cf86b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([0.69577289, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.76245421, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]), array([0.05956723, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.62647046, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.61643681, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.81738988, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ])]\n",
            "[array([0.69437206, 0.69484091, 0.69523352, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.76008099, 0.76011437, 0.76061314, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.5, 0.5, 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
            "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]), array([0.05956723, 0.05956723, 0.05956723, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.61784026, 0.62310744, 0.6295702 , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.59470216, 0.63429601, 0.65883483, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.7923255 , 0.39169689, 0.19883601, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ])]\n",
            "[array([0.69239819, 0.69260681, 0.6928947 , 0.69321513, 0.69342542,\n",
            "       0.69354397, 0.6936785 , 0.69386047, 0.69409877, 0.69441229,\n",
            "       0.69482017, 0.69515729, 0.69536275, 0.69549876, 0.69565839,\n",
            "       0.69592768, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.76698124, 0.767142  , 0.76740116, 0.76850617, 0.76982081,\n",
            "       0.77110481, 0.77226645, 0.77332848, 0.77420413, 0.77494717,\n",
            "       0.77569431, 0.77631909, 0.77661723, 0.77735764, 0.77926612,\n",
            "       0.78212816, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.50654206, 0.50186916, 0.49953271, 0.49672897, 0.49579439,\n",
            "       0.49672897, 0.49626168, 0.4953271 , 0.49672897, 0.49719626,\n",
            "       0.49719626, 0.49859813, 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.0994408 , 0.09141746, 0.08266472, 0.07755896, 0.07221007,\n",
            "       0.06977875, 0.06613178, 0.0632142 , 0.06078288, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.59767322, 0.59987179, 0.60421037, 0.60600585, 0.60695239,\n",
            "       0.60724944, 0.60786727, 0.60920161, 0.61049005, 0.61171675,\n",
            "       0.6115247 , 0.61377815, 0.61858947, 0.62394688, 0.62821951,\n",
            "       0.63179147, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.55371386, 0.55394532, 0.55373395, 0.55365578, 0.55513145,\n",
            "       0.55778714, 0.56082016, 0.56546467, 0.56863088, 0.57047035,\n",
            "       0.57073107, 0.57362521, 0.57730631, 0.58917809, 0.6025452 ,\n",
            "       0.61117696, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.91346378, 0.96450786, 0.81078881, 0.59517024, 0.38889713,\n",
            "       0.34318634, 0.31169488, 0.31640922, 0.2862006 , 0.26442794,\n",
            "       0.27105647, 0.25002552, 0.23682906, 0.2345924 , 0.22646374,\n",
            "       0.22060787, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ])]\n",
            "[array([0.69021267, 0.69001108, 0.68979478, 0.68961203, 0.6894843 ,\n",
            "       0.68942416, 0.68934816, 0.68932438, 0.68928909, 0.68926656,\n",
            "       0.68923497, 0.68922877, 0.68921667, 0.68919003, 0.68911922,\n",
            "       0.68902594, 0.68898267, 0.68904394, 0.68918031, 0.6893152 ,\n",
            "       0.68944746, 0.68967283, 0.68992352, 0.6902166 , 0.69049335,\n",
            "       0.6907618 , 0.69106925, 0.69148451, 0.69201601, 0.69267172,\n",
            "       0.69342947, 0.69421709, 0.6950736 , 0.69577241, 0.69652307,\n",
            "       0.69743919, 0.69842196, 0.69948143, 0.70071775, 0.70200318,\n",
            "       0.70336276, 0.70471662, 0.70614052, 0.70758402, 0.70909029,\n",
            "       0.71050096, 0.71161723, 0.71257418, 0.71355039, 0.71455014,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.76444018, 0.76401359, 0.76365203, 0.76315629, 0.76255369,\n",
            "       0.76215547, 0.76189148, 0.76172519, 0.761594  , 0.76159751,\n",
            "       0.76164097, 0.76177353, 0.76191956, 0.76213229, 0.76236373,\n",
            "       0.76263481, 0.76304072, 0.76381642, 0.76477569, 0.76582587,\n",
            "       0.76697642, 0.76847285, 0.77031845, 0.77272487, 0.77546227,\n",
            "       0.77822971, 0.78115624, 0.78455496, 0.78859574, 0.79317766,\n",
            "       0.79811615, 0.80312091, 0.80853397, 0.81322581, 0.81816399,\n",
            "       0.82381058, 0.82961494, 0.83570755, 0.84261644, 0.84962368,\n",
            "       0.85672194, 0.86355275, 0.87057596, 0.87740833, 0.88457555,\n",
            "       0.89114058, 0.89604801, 0.90004921, 0.90409774, 0.90822983,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.52897196, 0.53130841, 0.53271028, 0.53130841, 0.52897196,\n",
            "       0.52429907, 0.52196262, 0.52149533, 0.51962617, 0.51962617,\n",
            "       0.52102804, 0.52336449, 0.52009346, 0.51588785, 0.51635514,\n",
            "       0.51682243, 0.51448598, 0.51121495, 0.51074766, 0.50981308,\n",
            "       0.50607477, 0.50654206, 0.5046729 , 0.50420561, 0.50560748,\n",
            "       0.50607477, 0.50747664, 0.50560748, 0.50373832, 0.5046729 ,\n",
            "       0.50186916, 0.50140187, 0.50140187, 0.50046729, 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.2112813 , 0.20982251, 0.20860686, 0.20471675, 0.20179917,\n",
            "       0.19742281, 0.19474836, 0.1896426 , 0.18453683, 0.17870168,\n",
            "       0.17262339, 0.16678823, 0.16070994, 0.15609044, 0.14758084,\n",
            "       0.14271821, 0.13761245, 0.12837345, 0.12059324, 0.11378556,\n",
            "       0.10989545, 0.1033309 , 0.09822514, 0.09238998, 0.08801362,\n",
            "       0.08290785, 0.07974714, 0.07464138, 0.07148067, 0.06613178,\n",
            "       0.06370046, 0.06078288, 0.06053975, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.59947397, 0.60015459, 0.600277  , 0.60008073, 0.59990661,\n",
            "       0.59937583, 0.59864772, 0.59788954, 0.59747853, 0.59732393,\n",
            "       0.59736456, 0.59731285, 0.59741785, 0.59718887, 0.59732288,\n",
            "       0.59830952, 0.59975888, 0.60086476, 0.60140504, 0.6023041 ,\n",
            "       0.60321582, 0.60408005, 0.60517274, 0.60664268, 0.60794219,\n",
            "       0.6102083 , 0.61304476, 0.61507397, 0.61726674, 0.6196294 ,\n",
            "       0.62183484, 0.62302725, 0.6233549 , 0.62355908, 0.6234467 ,\n",
            "       0.62319978, 0.62260568, 0.6221208 , 0.62179737, 0.62140641,\n",
            "       0.62077222, 0.6202847 , 0.62018551, 0.62070838, 0.62092417,\n",
            "       0.62122438, 0.62166442, 0.62237195, 0.62322668, 0.62438902,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.55722596, 0.5579972 , 0.55873526, 0.55911608, 0.5590925 ,\n",
            "       0.55862739, 0.55841427, 0.55757533, 0.55713337, 0.55636169,\n",
            "       0.55571185, 0.55457551, 0.55364923, 0.55267316, 0.55231243,\n",
            "       0.55226614, 0.55168923, 0.55037645, 0.54835095, 0.54610577,\n",
            "       0.54408682, 0.54207442, 0.54028605, 0.5389034 , 0.53841689,\n",
            "       0.53817932, 0.53844746, 0.5389213 , 0.53973229, 0.53998471,\n",
            "       0.53957813, 0.5392602 , 0.53954363, 0.54202288, 0.54462704,\n",
            "       0.54701066, 0.54895406, 0.55112455, 0.55327452, 0.55562014,\n",
            "       0.55650406, 0.5576094 , 0.55855708, 0.558354  , 0.5591401 ,\n",
            "       0.56079963, 0.56292995, 0.56473797, 0.56703773, 0.56936938,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([1.33108299, 1.10242764, 0.877144  , 0.7810023 , 0.72138971,\n",
            "       0.66989564, 0.60553132, 0.57335393, 0.53791749, 0.52345966,\n",
            "       0.49644367, 0.48179481, 0.48493331, 0.44921954, 0.43510071,\n",
            "       0.43990519, 0.40866744, 0.41056328, 0.38562626, 0.37809604,\n",
            "       0.39662543, 0.367953  , 0.36462607, 0.34049769, 0.33636048,\n",
            "       0.32948966, 0.31365199, 0.3012219 , 0.29117225, 0.28542252,\n",
            "       0.28859565, 0.29461282, 0.28002801, 0.2711646 , 0.26923451,\n",
            "       0.27368811, 0.27567641, 0.26693706, 0.25540963, 0.25662842,\n",
            "       0.25340181, 0.25000191, 0.25100932, 0.28734997, 0.25187489,\n",
            "       0.24734786, 0.24104389, 0.23757762, 0.24075927, 0.2295803 ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ])]\n",
            "[array([0.69024158, 0.69007891, 0.68992335, 0.68974721, 0.68964529,\n",
            "       0.68964523, 0.68963718, 0.68959999, 0.68958724, 0.68953627,\n",
            "       0.68950826, 0.68950039, 0.68944573, 0.68934792, 0.68922627,\n",
            "       0.6890828 , 0.68900132, 0.68897331, 0.68904293, 0.68912822,\n",
            "       0.68920112, 0.68932164, 0.68947804, 0.68970555, 0.68992603,\n",
            "       0.69012314, 0.69041324, 0.69074535, 0.69116104, 0.69166136,\n",
            "       0.69212615, 0.69256747, 0.69295996, 0.69329834, 0.69359243,\n",
            "       0.69391739, 0.69428569, 0.69463438, 0.69503176, 0.69541109,\n",
            "       0.69592196, 0.69646114, 0.69700038, 0.6975261 , 0.69808185,\n",
            "       0.69870329, 0.69931895, 0.69995153, 0.70058548, 0.70129114,\n",
            "       0.70207661, 0.70293647, 0.70376235, 0.70451826, 0.70520145,\n",
            "       0.7058084 , 0.70635527, 0.70689201, 0.7072835 , 0.70746434,\n",
            "       0.70757371, 0.70764333, 0.70770001, 0.70783353, 0.70794922,\n",
            "       0.70808274, 0.7082324 , 0.70829058, 0.70837808, 0.70841712,\n",
            "       0.70848989, 0.7083106 , 0.70808989, 0.70788592, 0.70765024,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.76667118, 0.76635998, 0.76610118, 0.76575106, 0.76529098,\n",
            "       0.76500809, 0.76476353, 0.76458699, 0.76456624, 0.76454002,\n",
            "       0.76462531, 0.76485586, 0.76511818, 0.7654044 , 0.76577353,\n",
            "       0.76628625, 0.76715267, 0.76814133, 0.76934379, 0.77060705,\n",
            "       0.7719115 , 0.77344173, 0.77530092, 0.77758825, 0.77987772,\n",
            "       0.78214169, 0.78464311, 0.78740126, 0.79050863, 0.79395318,\n",
            "       0.79724085, 0.80032969, 0.80325699, 0.80583251, 0.80820775,\n",
            "       0.81050706, 0.81289893, 0.81519526, 0.81763554, 0.82000601,\n",
            "       0.8226878 , 0.82542992, 0.82808447, 0.83061254, 0.83312851,\n",
            "       0.83585536, 0.83846194, 0.84125268, 0.84413016, 0.84737551,\n",
            "       0.85096866, 0.85479397, 0.85814619, 0.86099851, 0.86344719,\n",
            "       0.86545944, 0.86729032, 0.86916733, 0.87024188, 0.87032324,\n",
            "       0.8699947 , 0.86948335, 0.86895013, 0.86894286, 0.86886442,\n",
            "       0.8689853 , 0.86920381, 0.86890984, 0.86884284, 0.8684842 ,\n",
            "       0.86831599, 0.86674517, 0.86497223, 0.8633244 , 0.86147982,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.52850467, 0.53271028, 0.53364486, 0.5317757 , 0.53130841,\n",
            "       0.52570093, 0.52196262, 0.52102804, 0.52009346, 0.52336449,\n",
            "       0.52523364, 0.52570093, 0.5228972 , 0.52102804, 0.5228972 ,\n",
            "       0.52056075, 0.51682243, 0.51401869, 0.51448598, 0.51495327,\n",
            "       0.51261682, 0.51028037, 0.50981308, 0.5088785 , 0.51028037,\n",
            "       0.51074766, 0.5088785 , 0.50981308, 0.50654206, 0.50560748,\n",
            "       0.50420561, 0.50607477, 0.5046729 , 0.5046729 , 0.50373832,\n",
            "       0.50233645, 0.50186916, 0.50140187, 0.50186916, 0.50186916,\n",
            "       0.50046729, 0.5       , 0.5       , 0.49953271, 0.5       ,\n",
            "       0.5       , 0.49953271, 0.49953271, 0.49953271, 0.49953271,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.21687333, 0.21419888, 0.21176757, 0.21055191, 0.20763433,\n",
            "       0.20593241, 0.20398736, 0.20058352, 0.19572088, 0.19377583,\n",
            "       0.18842694, 0.18259178, 0.17845855, 0.17189399, 0.1665451 ,\n",
            "       0.15998055, 0.15657671, 0.15001216, 0.14101629, 0.13372234,\n",
            "       0.12740092, 0.12083637, 0.11500122, 0.1086798 , 0.10260151,\n",
            "       0.09555069, 0.09044493, 0.08485291, 0.07901775, 0.07293946,\n",
            "       0.06953562, 0.06637491, 0.06442986, 0.06272794, 0.06175541,\n",
            "       0.06053975, 0.06005349, 0.05981036, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.60110958, 0.6019411 , 0.60232309, 0.60194215, 0.60173005,\n",
            "       0.60122671, 0.60035034, 0.59935947, 0.59870048, 0.59846886,\n",
            "       0.59810164, 0.59790273, 0.59775869, 0.59754553, 0.59774497,\n",
            "       0.5984926 , 0.59931779, 0.60011449, 0.60069592, 0.60144303,\n",
            "       0.60255946, 0.60315461, 0.60424625, 0.60525241, 0.60649178,\n",
            "       0.60781082, 0.60905282, 0.61046103, 0.61170779, 0.6124512 ,\n",
            "       0.61359929, 0.61483549, 0.61655182, 0.61827343, 0.62026043,\n",
            "       0.62243948, 0.62366302, 0.62467024, 0.62546852, 0.62628738,\n",
            "       0.62649421, 0.62700863, 0.62764705, 0.62846116, 0.62891332,\n",
            "       0.62920246, 0.62956493, 0.62990946, 0.63091088, 0.63253224,\n",
            "       0.63437203, 0.6366946 , 0.6383645 , 0.63951628, 0.64029715,\n",
            "       0.64074615, 0.64166157, 0.64295845, 0.64343488, 0.64413767,\n",
            "       0.64513169, 0.64552107, 0.64593894, 0.64644071, 0.64653568,\n",
            "       0.64702267, 0.64738883, 0.64811008, 0.64865775, 0.64920225,\n",
            "       0.64978051, 0.65082836, 0.65153536, 0.65229091, 0.65272091,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.55673989, 0.55731156, 0.5579413 , 0.55831863, 0.55837016,\n",
            "       0.55777186, 0.5571456 , 0.55658747, 0.55598131, 0.55555594,\n",
            "       0.55520657, 0.55449559, 0.55402961, 0.55420997, 0.55428247,\n",
            "       0.5546301 , 0.55481396, 0.55477334, 0.55440737, 0.55400428,\n",
            "       0.55391694, 0.55386453, 0.55391082, 0.55369334, 0.55369858,\n",
            "       0.55429077, 0.554334  , 0.55445847, 0.5541353 , 0.55372871,\n",
            "       0.55389423, 0.55475282, 0.55638702, 0.55873177, 0.56175605,\n",
            "       0.56472487, 0.56766661, 0.57099703, 0.57418727, 0.57767403,\n",
            "       0.5798668 , 0.58193554, 0.58435147, 0.58728273, 0.58947987,\n",
            "       0.59069875, 0.59161979, 0.59338501, 0.59621714, 0.59938685,\n",
            "       0.60185256, 0.60366539, 0.60384575, 0.60390864, 0.60333435,\n",
            "       0.60150974, 0.60016814, 0.59963185, 0.59819635, 0.59796008,\n",
            "       0.59739191, 0.59721067, 0.59692244, 0.59502315, 0.59199188,\n",
            "       0.59229496, 0.59025417, 0.58914665, 0.59021268, 0.5908599 ,\n",
            "       0.59088043, 0.5918399 , 0.59451743, 0.59838152, 0.6005459 ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([1.47818753, 1.1190205 , 0.87453967, 0.79264375, 0.71085627,\n",
            "       0.67849467, 0.63480303, 0.58701388, 0.54389122, 0.6729757 ,\n",
            "       0.53693615, 0.49720864, 0.48453675, 0.45455022, 0.46293186,\n",
            "       0.44395068, 0.43512827, 0.43465289, 0.42367055, 0.41602227,\n",
            "       0.3973589 , 0.37811559, 0.37745756, 0.36345897, 0.36026657,\n",
            "       0.35479468, 0.32761157, 0.32651507, 0.32310251, 0.31573256,\n",
            "       0.31952839, 0.30514083, 0.30781895, 0.30309929, 0.29108519,\n",
            "       0.29846144, 0.28946263, 0.29296113, 0.29146064, 0.28544806,\n",
            "       0.28383487, 0.28873379, 0.28022682, 0.27544359, 0.28132355,\n",
            "       0.27414677, 0.27736021, 0.2667821 , 0.277834  , 0.26641929,\n",
            "       0.27015537, 0.2644849 , 0.251347  , 0.25171439, 0.25154029,\n",
            "       0.25083692, 0.25800198, 0.24903392, 0.25260532, 0.26451921,\n",
            "       0.25741657, 0.25424219, 0.24697308, 0.252759  , 0.24707535,\n",
            "       0.24848334, 0.24335011, 0.23527316, 0.23819534, 0.23699532,\n",
            "       0.23356444, 0.23491037, 0.24376364, 0.29067939, 0.23253038,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ])]\n",
            "[array([0.69024158, 0.69007891, 0.68992335, 0.68974721, 0.68964529,\n",
            "       0.68964523, 0.68963718, 0.68959999, 0.68958724, 0.68953627,\n",
            "       0.68950826, 0.68950039, 0.68944573, 0.68934792, 0.68922627,\n",
            "       0.6890828 , 0.68900132, 0.68897331, 0.68904293, 0.68912822,\n",
            "       0.68920112, 0.68932164, 0.68947804, 0.68970555, 0.68992603,\n",
            "       0.69012314, 0.69041324, 0.69074535, 0.69116104, 0.69166136,\n",
            "       0.69212615, 0.69256747, 0.69295996, 0.69329834, 0.69359243,\n",
            "       0.69391739, 0.69428569, 0.69463438, 0.69503176, 0.69541109,\n",
            "       0.69592196, 0.69646114, 0.69700038, 0.6975261 , 0.69808185,\n",
            "       0.69870329, 0.69931895, 0.69995153, 0.70058548, 0.70129114,\n",
            "       0.70207661, 0.70293647, 0.70376235, 0.70451826, 0.70520145,\n",
            "       0.7058084 , 0.70635527, 0.70689201, 0.7072835 , 0.70746434,\n",
            "       0.70757371, 0.70764333, 0.70770001, 0.70783353, 0.70794922,\n",
            "       0.70808274, 0.7082324 , 0.70829058, 0.70837808, 0.70841712,\n",
            "       0.70848989, 0.7083106 , 0.70808989, 0.70788592, 0.70765024,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.76667118, 0.76635998, 0.76610118, 0.76575106, 0.76529098,\n",
            "       0.76500809, 0.76476353, 0.76458699, 0.76456624, 0.76454002,\n",
            "       0.76462531, 0.76485586, 0.76511818, 0.7654044 , 0.76577353,\n",
            "       0.76628625, 0.76715267, 0.76814133, 0.76934379, 0.77060705,\n",
            "       0.7719115 , 0.77344173, 0.77530092, 0.77758825, 0.77987772,\n",
            "       0.78214169, 0.78464311, 0.78740126, 0.79050863, 0.79395318,\n",
            "       0.79724085, 0.80032969, 0.80325699, 0.80583251, 0.80820775,\n",
            "       0.81050706, 0.81289893, 0.81519526, 0.81763554, 0.82000601,\n",
            "       0.8226878 , 0.82542992, 0.82808447, 0.83061254, 0.83312851,\n",
            "       0.83585536, 0.83846194, 0.84125268, 0.84413016, 0.84737551,\n",
            "       0.85096866, 0.85479397, 0.85814619, 0.86099851, 0.86344719,\n",
            "       0.86545944, 0.86729032, 0.86916733, 0.87024188, 0.87032324,\n",
            "       0.8699947 , 0.86948335, 0.86895013, 0.86894286, 0.86886442,\n",
            "       0.8689853 , 0.86920381, 0.86890984, 0.86884284, 0.8684842 ,\n",
            "       0.86831599, 0.86674517, 0.86497223, 0.8633244 , 0.86147982,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.52850467, 0.53271028, 0.53364486, 0.5317757 , 0.53130841,\n",
            "       0.52570093, 0.52196262, 0.52102804, 0.52009346, 0.52336449,\n",
            "       0.52523364, 0.52570093, 0.5228972 , 0.52102804, 0.5228972 ,\n",
            "       0.52056075, 0.51682243, 0.51401869, 0.51448598, 0.51495327,\n",
            "       0.51261682, 0.51028037, 0.50981308, 0.5088785 , 0.51028037,\n",
            "       0.51074766, 0.5088785 , 0.50981308, 0.50654206, 0.50560748,\n",
            "       0.50420561, 0.50607477, 0.5046729 , 0.5046729 , 0.50373832,\n",
            "       0.50233645, 0.50186916, 0.50140187, 0.50186916, 0.50186916,\n",
            "       0.50046729, 0.5       , 0.5       , 0.49953271, 0.5       ,\n",
            "       0.5       , 0.49953271, 0.49953271, 0.49953271, 0.49953271,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.21687333, 0.21419888, 0.21176757, 0.21055191, 0.20763433,\n",
            "       0.20593241, 0.20398736, 0.20058352, 0.19572088, 0.19377583,\n",
            "       0.18842694, 0.18259178, 0.17845855, 0.17189399, 0.1665451 ,\n",
            "       0.15998055, 0.15657671, 0.15001216, 0.14101629, 0.13372234,\n",
            "       0.12740092, 0.12083637, 0.11500122, 0.1086798 , 0.10260151,\n",
            "       0.09555069, 0.09044493, 0.08485291, 0.07901775, 0.07293946,\n",
            "       0.06953562, 0.06637491, 0.06442986, 0.06272794, 0.06175541,\n",
            "       0.06053975, 0.06005349, 0.05981036, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.05956723, 0.05956723, 0.05956723, 0.05956723, 0.05956723,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.60110958, 0.6019411 , 0.60232309, 0.60194215, 0.60173005,\n",
            "       0.60122671, 0.60035034, 0.59935947, 0.59870048, 0.59846886,\n",
            "       0.59810164, 0.59790273, 0.59775869, 0.59754553, 0.59774497,\n",
            "       0.5984926 , 0.59931779, 0.60011449, 0.60069592, 0.60144303,\n",
            "       0.60255946, 0.60315461, 0.60424625, 0.60525241, 0.60649178,\n",
            "       0.60781082, 0.60905282, 0.61046103, 0.61170779, 0.6124512 ,\n",
            "       0.61359929, 0.61483549, 0.61655182, 0.61827343, 0.62026043,\n",
            "       0.62243948, 0.62366302, 0.62467024, 0.62546852, 0.62628738,\n",
            "       0.62649421, 0.62700863, 0.62764705, 0.62846116, 0.62891332,\n",
            "       0.62920246, 0.62956493, 0.62990946, 0.63091088, 0.63253224,\n",
            "       0.63437203, 0.6366946 , 0.6383645 , 0.63951628, 0.64029715,\n",
            "       0.64074615, 0.64166157, 0.64295845, 0.64343488, 0.64413767,\n",
            "       0.64513169, 0.64552107, 0.64593894, 0.64644071, 0.64653568,\n",
            "       0.64702267, 0.64738883, 0.64811008, 0.64865775, 0.64920225,\n",
            "       0.64978051, 0.65082836, 0.65153536, 0.65229091, 0.65272091,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.55673989, 0.55731156, 0.5579413 , 0.55831863, 0.55837016,\n",
            "       0.55777186, 0.5571456 , 0.55658747, 0.55598131, 0.55555594,\n",
            "       0.55520657, 0.55449559, 0.55402961, 0.55420997, 0.55428247,\n",
            "       0.5546301 , 0.55481396, 0.55477334, 0.55440737, 0.55400428,\n",
            "       0.55391694, 0.55386453, 0.55391082, 0.55369334, 0.55369858,\n",
            "       0.55429077, 0.554334  , 0.55445847, 0.5541353 , 0.55372871,\n",
            "       0.55389423, 0.55475282, 0.55638702, 0.55873177, 0.56175605,\n",
            "       0.56472487, 0.56766661, 0.57099703, 0.57418727, 0.57767403,\n",
            "       0.5798668 , 0.58193554, 0.58435147, 0.58728273, 0.58947987,\n",
            "       0.59069875, 0.59161979, 0.59338501, 0.59621714, 0.59938685,\n",
            "       0.60185256, 0.60366539, 0.60384575, 0.60390864, 0.60333435,\n",
            "       0.60150974, 0.60016814, 0.59963185, 0.59819635, 0.59796008,\n",
            "       0.59739191, 0.59721067, 0.59692244, 0.59502315, 0.59199188,\n",
            "       0.59229496, 0.59025417, 0.58914665, 0.59021268, 0.5908599 ,\n",
            "       0.59088043, 0.5918399 , 0.59451743, 0.59838152, 0.6005459 ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([1.47004791, 1.1311087 , 0.88663229, 0.7915879 , 0.71962487,\n",
            "       0.67710304, 0.63454753, 0.59712994, 0.72021348, 0.5364735 ,\n",
            "       0.50449105, 0.50378648, 0.47782286, 0.45455278, 0.46203713,\n",
            "       0.46029501, 0.44172265, 0.44468848, 0.42872015, 0.42647005,\n",
            "       0.39354334, 0.38154515, 0.38210314, 0.37141825, 0.36140539,\n",
            "       0.36833818, 0.34246951, 0.33204901, 0.31602563, 0.32481694,\n",
            "       0.30591799, 0.30305266, 0.30804257, 0.29432875, 0.28763207,\n",
            "       0.28645048, 0.29585965, 0.29265722, 0.28706898, 0.28962573,\n",
            "       0.27133757, 0.27197259, 0.26857962, 0.26570634, 0.26486797,\n",
            "       0.26518308, 0.26207416, 0.27081853, 0.26866452, 0.26124712,\n",
            "       0.26096216, 0.26233864, 0.26073808, 0.25683192, 0.25240637,\n",
            "       0.25229341, 0.26034263, 0.25428363, 0.25449817, 0.24963642,\n",
            "       0.25226179, 0.25219667, 0.25399065, 0.24463675, 0.24247507,\n",
            "       0.25408599, 0.25475559, 0.24559303, 0.24203318, 0.24395485,\n",
            "       0.23413049, 0.23914028, 0.24069731, 0.22953827, 0.2287908 ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ])]\n",
            "CSV file has been created: drive/MyDrive/result/varying_svr.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "# Replace this path with the appropriate one for your files\n",
        "base_dir = 'drive/MyDrive/result/'\n",
        "\n",
        "datasets = ['hiv_pca_resample/0/'] # 'full-dataset'\n",
        "\n",
        "runs = [\"model_layer_0_unit_10_sigma_0.01_C_0.001_epsilon_0.05_lr_5e-05_alpha_0.1_nepoch_100_seed_0\",\n",
        "        \"model_layer_0_unit_10_sigma_0.01_C_0.01_epsilon_0.05_lr_5e-05_alpha_0.1_nepoch_100_seed_0\",\n",
        "        \"model_layer_0_unit_10_sigma_0.01_C_0.1_epsilon_0.05_lr_5e-05_alpha_0.1_nepoch_100_seed_0\",\n",
        "        \"model_layer_0_unit_10_sigma_0.01_C_1_epsilon_0.05_lr_5e-05_alpha_0.1_nepoch_100_seed_0\",\n",
        "        \"model_layer_0_unit_10_sigma_0.01_C_10_epsilon_0.05_lr_5e-05_alpha_0.1_nepoch_100_seed_0\",\n",
        "        \"model_layer_0_unit_10_sigma_0.01_C_100_epsilon_0.05_lr_5e-05_alpha_0.1_nepoch_100_seed_0\"]\n",
        "\n",
        "# Prepare data for writing into CSV\n",
        "csv_data = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    dataset_path = os.path.join(base_dir, dataset)\n",
        "    # Instead of using a predefined run_names list, find subdirectories in the dataset folder\n",
        "    run_names = next(os.walk(dataset_path))[1]  # List subdirectories\n",
        "    for run_name in run_names:\n",
        "      if run_name in runs:\n",
        "          file_path = os.path.join(dataset_path, run_name, \"result.txt\")\n",
        "          # Load the pickle data\n",
        "          with open(file_path, 'rb') as f:\n",
        "              loaded_data = pickle.load(f)\n",
        "          # Unpack the data\n",
        "          (train_loss_path, test_loss_path,\n",
        "          train_accuracy_path, test_accuracy_path,\n",
        "          test_auc_roc_path, train_auc_roc_path,\n",
        "          time_used_path) = loaded_data\n",
        "\n",
        "          # Go through each epoch and add the relevant data to csv_data\n",
        "          for i in range(len(time_used_path)):\n",
        "              csv_data.append({\n",
        "                  'Dataset': dataset.replace('-', ' ').title(),\n",
        "                  'Run Name': run_name.replace('-', ' ').title(),  # Assume run_name's format is good\n",
        "                  'Epoch': i + 1,\n",
        "                  'Testing ROC-AUC': test_auc_roc_path[i],\n",
        "                  'Training ROC-AUC': train_auc_roc_path[i],\n",
        "                  'Seconds per Epoch': time_used_path[i]\n",
        "              })\n",
        "\n",
        "# Define the CSV file header\n",
        "csv_columns = ['Dataset', 'Run Name', 'Epoch', 'Testing ROC-AUC', 'Training ROC-AUC', 'Seconds per Epoch']\n",
        "\n",
        "# Where to save the CSV file\n",
        "csv_file = \"drive/MyDrive/result/varying_svr.csv\"\n",
        "\n",
        "# Writing CSV data into the file\n",
        "with open(csv_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "    writer.writeheader()\n",
        "    for data in csv_data:\n",
        "        writer.writerow(data)\n",
        "\n",
        "print(f\"CSV file has been created: {csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NM5zuxAgSSG",
        "outputId": "4b9eec43-6ff5-4602-f7ca-f49f0ed2ed56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file has been created: drive/MyDrive/result/nn_removal_3.csv\n"
          ]
        }
      ],
      "source": [
        "# Kernel-only variation\n",
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this path with the appropriate one for your files\n",
        "base_dir = 'drive/MyDrive/result/'\n",
        "\n",
        "datasets = ['hiv_pca_resample/0/'] # 'full-dataset'\n",
        "# Prepare data for writing into CSV\n",
        "csv_data = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    dataset_path = os.path.join(base_dir, dataset)\n",
        "    # Instead of using a predefined run_names list, find subdirectories in the dataset folder\n",
        "    run_names = next(os.walk(dataset_path))[1]  # List subdirectories\n",
        "    for run_name in run_names:\n",
        "      if \"seed_3\" in run_name:\n",
        "          file_path = os.path.join(dataset_path, run_name, \"result.txt\")\n",
        "          # Load the pickle data\n",
        "          with open(file_path, 'rb') as f:\n",
        "              loaded_data = pickle.load(f)\n",
        "          # Unpack the data\n",
        "          (train_loss_path, test_loss_path,\n",
        "          train_accuracy_path, test_accuracy_path,\n",
        "          test_auc_roc_path, train_auc_roc_path,\n",
        "          time_used_path) = loaded_data\n",
        "\n",
        "          # Go through each epoch and add the relevant data to csv_data\n",
        "          for i in range(len(time_used_path)):\n",
        "              csv_data.append({\n",
        "                  'Dataset': dataset.replace('-', ' ').title(),\n",
        "                  'Run Name': run_name.replace('-', ' ').title(),  # Assume run_name's format is good\n",
        "                  'Epoch': i + 1,\n",
        "                  'Testing ROC-AUC': test_auc_roc_path[i],\n",
        "                  'Training ROC-AUC': train_auc_roc_path[i],\n",
        "                  'Seconds per Epoch': time_used_path[i]\n",
        "              })\n",
        "\n",
        "# Define the CSV file header\n",
        "csv_columns = ['Dataset', 'Run Name', 'Epoch', 'Testing ROC-AUC', 'Training ROC-AUC', 'Seconds per Epoch']\n",
        "\n",
        "# Where to save the CSV file\n",
        "csv_file = \"drive/MyDrive/result/nn_removal_3.csv\"\n",
        "\n",
        "# Writing CSV data into the file\n",
        "with open(csv_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "    writer.writeheader()\n",
        "    for data in csv_data:\n",
        "        writer.writerow(data)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(csv_file)\n",
        "df_sub = df[df[\"Training ROC-AUC\"] != 0.0]\n",
        "df_sub = df[df[\"epoch\"] < 10] # remove this if you don't want to subset to low number of epochs\n",
        "df_sub.to_csv(csv_file.replace(\".csv\",\"_no_0.csv\"))\n",
        "\n",
        "### automatically remove 0's #############\n",
        "\n",
        "print(f\"CSV file has been created: {csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "nqcG0Fro6g8C"
      },
      "outputs": [],
      "source": [
        "df_sub = df[df[\"Epoch\"] < 9] # remove this if you don't want to subset to low number of epochs\n",
        "df_sub.to_csv(csv_file.replace(\".csv\",\"_no_0_9_epoch.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J30--LA5jb-7",
        "outputId": "299a0c32-4c59-4e55-f928-44d1ffe47b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file has been created: drive/MyDrive/result/DNN_with_additional_layer.csv\n"
          ]
        }
      ],
      "source": [
        "# DNN's with an additional layer\n",
        "# Kernel-only variation\n",
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this path with the appropriate one for your files\n",
        "base_dir = 'drive/MyDrive/result/'\n",
        "\n",
        "datasets = ['hiv_pca_resample_DNN+1_layer/0/'] # 'full-dataset'\n",
        "# Prepare data for writing into CSV\n",
        "csv_data = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    dataset_path = os.path.join(base_dir, dataset)\n",
        "    # Instead of using a predefined run_names list, find subdirectories in the dataset folder\n",
        "    run_names = next(os.walk(dataset_path))[1]  # List subdirectories\n",
        "    for run_name in run_names:\n",
        "      if \"DNN\" in run_name:\n",
        "          file_path = os.path.join(dataset_path, run_name, \"result.txt\")\n",
        "          # Load the pickle data\n",
        "          with open(file_path, 'rb') as f:\n",
        "              loaded_data = pickle.load(f)\n",
        "          # Unpack the data\n",
        "          (train_loss_path, test_loss_path,\n",
        "          train_accuracy_path, test_accuracy_path,\n",
        "          test_auc_roc_path, train_auc_roc_path,\n",
        "          time_used_path) = loaded_data\n",
        "\n",
        "          # Go through each epoch and add the relevant data to csv_data\n",
        "          for i in range(len(time_used_path)):\n",
        "              csv_data.append({\n",
        "                  'Dataset': dataset.replace('-', ' ').title(),\n",
        "                  'Run Name': run_name.replace('-', ' ').title(),  # Assume run_name's format is good\n",
        "                  'Epoch': i + 1,\n",
        "                  'Testing ROC-AUC': test_auc_roc_path[i],\n",
        "                  'Training ROC-AUC': train_auc_roc_path[i],\n",
        "                  'Seconds per Epoch': time_used_path[i]\n",
        "              })\n",
        "\n",
        "# Define the CSV file header\n",
        "csv_columns = ['Dataset', 'Run Name', 'Epoch', 'Testing ROC-AUC', 'Training ROC-AUC', 'Seconds per Epoch']\n",
        "\n",
        "# Where to save the CSV file\n",
        "csv_file = \"drive/MyDrive/result/DNN_with_additional_layer.csv\"\n",
        "\n",
        "# Writing CSV data into the file\n",
        "with open(csv_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "    writer.writeheader()\n",
        "    for data in csv_data:\n",
        "        writer.writerow(data)\n",
        "\n",
        "### automatically remove 0's #############\n",
        "\n",
        "print(f\"CSV file has been created: {csv_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tsDM9isDq2V2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "csv_name = \"drive/MyDrive/result/varying_svr_100_epoch.csv\"\n",
        "df = pd.read_csv(csv_name)\n",
        "df_sub = df[df[\"Training ROC-AUC\"] != 0.0]\n",
        "df_sub.to_csv(csv_name.replace(\".csv\",\"_no_0.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v1w973sxrD_o"
      },
      "outputs": [],
      "source": [
        "df_sub = df[df[\"Training ROC-AUC\"] != 0.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amkRy4X_rP77",
        "outputId": "a989ff0d-3638-4019-d85e-e2f4d88ad778"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(232, 6)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq1zoZ6EV1SG",
        "outputId": "793a1759-3db6-4d78-fb72-54bfb520e414"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for SPS. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for AvgIpc. Feature removed!\n",
            "WARNING:deepchem.models:Skipped loading some PyTorch models, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n",
            "WARNING:deepchem.molnet.load_function.molnet_loader:'split' is deprecated.  Use 'splitter' instead.\n",
            "[18:37:41] WARNING: not removing hydrogen atom without neighbors\n",
            "[18:37:41] WARNING: not removing hydrogen atom without neighbors\n"
          ]
        }
      ],
      "source": [
        "import deepchem as dc\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to standardize datasets\n",
        "def standardize_data(X, mean, std):\n",
        "    return (X - mean) / std\n",
        "\n",
        "# Load HIV dataset\n",
        "hiv_tasks, hiv_datasets, hiv_transformers = dc.molnet.load_hiv(featurizer='ECFP', split='index')\n",
        "hiv_train, hiv_valid, hiv_test = hiv_datasets\n",
        "\n",
        "# Standardize each dataset separately using training data statistics\n",
        "x_train_mean = np.mean(hiv_train.X, axis=0)\n",
        "x_train_std = np.std(hiv_train.X, axis=0)\n",
        "x_train_std[x_train_std == 0] = 1  # Avoid division by zero\n",
        "\n",
        "# Apply standardization\n",
        "x_train_stdized = standardize_data(hiv_train.X, x_train_mean, x_train_std)\n",
        "x_valid_stdized = standardize_data(hiv_valid.X, x_train_mean, x_train_std)\n",
        "x_test_stdized = standardize_data(hiv_test.X, x_train_mean, x_train_std)\n",
        "\n",
        "# Save standardized datasets to disk as NumPy arrays\n",
        "# Ensure the target directories exist\n",
        "os.makedirs('data/hiv', exist_ok=True)\n",
        "\n",
        "np.save('data/hiv/x_train.npy', x_train_stdized)\n",
        "np.save('data/hiv/y_train.npy', hiv_train.y.squeeze(1).astype(int))\n",
        "np.save('data/hiv/x_valid.npy', x_valid_stdized)\n",
        "np.save('data/hiv/y_valid.npy', hiv_valid.y.squeeze(1).astype(int))\n",
        "np.save('data/hiv/x_test.npy', x_test_stdized)\n",
        "np.save('data/hiv/y_test.npy', hiv_test.y.squeeze(1).astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "Pj8D6NU2k8F1",
        "outputId": "0a4acb4d-a27e-4b26-9edd-b14710be6786"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the standardized data\n",
        "x_train_stdized = np.load('data/hiv/x_train.npy')\n",
        "\n",
        "# Initializing PCA\n",
        "pca = PCA()\n",
        "\n",
        "# Fitting PCA on the training data\n",
        "pca.fit(x_train_stdized)\n",
        "\n",
        "# Getting the explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Plotting the variance explained by the principal components\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(np.cumsum(explained_variance_ratio))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Variance (%)') # for each component\n",
        "plt.title('Explained Variance')\n",
        "\n",
        "# Optional: Plot with a certain number of components (e.g., top 20 components)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(1, 101), explained_variance_ratio[:50], alpha=0.7, align='center', label='Individual explained variance')\n",
        "plt.step(range(1, 101), np.cumsum(explained_variance_ratio[:50]), where='mid', label='Cumulative explained variance')\n",
        "plt.xlabel('Principal Component index')\n",
        "plt.ylabel('Variance Ratio')\n",
        "plt.title('Explained Variance Ratio by Components (Top 20)')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SkEXjOImJ8Z",
        "outputId": "b1ab8661-9f6a-4a86-82ce-649018a81b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy (Full dataset): 0.9397033795283248\n",
            "Test ROC AUC (Full dataset): 0.7135270033556339\n",
            "Test Accuracy (PCA dataset): 0.9423778264040846\n",
            "Test ROC AUC (PCA dataset): 0.6985564442943673\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import deepchem as dc\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# Load the standardized data\n",
        "x_train_stdized = np.load('data/hiv/x_train.npy')\n",
        "y_train = np.load('data/hiv/y_train.npy')\n",
        "x_test_stdized = np.load('data/hiv/x_test.npy')\n",
        "y_test = np.load('data/hiv/y_test.npy')\n",
        "\n",
        "# Initialize PCA and keep the top 100 principal components\n",
        "pca = PCA(n_components=100)\n",
        "pca.fit(x_train_stdized)\n",
        "x_train_pca = pca.transform(x_train_stdized)\n",
        "x_test_pca = pca.transform(x_test_stdized)\n",
        "\n",
        "# Train Gradient Boosting on the full dataset\n",
        "gb_full = GradientBoostingClassifier()\n",
        "gb_full.fit(x_train_stdized, y_train)\n",
        "\n",
        "# Train Gradient Boosting on the top 100 principal components\n",
        "gb_pca = GradientBoostingClassifier()\n",
        "gb_pca.fit(x_train_pca, y_train)\n",
        "\n",
        "# Predictions and evaluation on the full dataset\n",
        "y_pred_full = gb_full.predict(x_test_stdized)\n",
        "y_pred_proba_full = gb_full.predict_proba(x_test_stdized)[:, 1]  # Probability for the positive class\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "roc_auc_full = roc_auc_score(y_test, y_pred_proba_full)\n",
        "\n",
        "# Predictions and evaluation on the dataset with PCA\n",
        "y_pred_pca = gb_pca.predict(x_test_pca)\n",
        "y_pred_proba_pca = gb_pca.predict_proba(x_test_pca)[:, 1]  # Probability for the positive class\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "roc_auc_pca = roc_auc_score(y_test, y_pred_proba_pca)\n",
        "\n",
        "# Report accuracy and ROC AUC for both models\n",
        "print(\"Test Accuracy (Full dataset):\", accuracy_full)\n",
        "print(\"Test ROC AUC (Full dataset):\", roc_auc_full)\n",
        "print(\"Test Accuracy (PCA dataset):\", accuracy_pca)\n",
        "print(\"Test ROC AUC (PCA dataset):\", roc_auc_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1kJ_jNsPXr2",
        "outputId": "88a98042-bea7-4ad4-ab2a-8d2ad4b86dec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0.0: 28659, 1.0: 952}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def count_numbers_in_array(array):\n",
        "    # Get the unique numbers and their counts\n",
        "    unique_numbers, counts = np.unique(array, return_counts=True)\n",
        "\n",
        "    # Combine the unique numbers and their counts into a dictionary\n",
        "    number_counts = dict(zip(unique_numbers, counts))\n",
        "\n",
        "    return number_counts\n",
        "count_numbers_in_array(y_train_hiv.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy8MqknFLTCj"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/results.zip ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX6nCv2DGlSp"
      },
      "outputs": [],
      "source": [
        "!mv DNN*.csv dnn/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QCZMyYpGI0j",
        "outputId": "5bd410ca-d652-4ff0-f0b4-418286ab7165"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n",
            "<ipython-input-115-dd1aa8d44f1e>:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  summary_df = summary_df.append(new_row, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# File with summary information\n",
        "summary_file = 'new_data.csv'\n",
        "\n",
        "# Read the existing data\n",
        "summary_df = pd.read_csv(summary_file)\n",
        "\n",
        "# Directory where the detailed files are located\n",
        "detail_dir_path = Path('./reduced_dataset')\n",
        "\n",
        "# List out all files that match the naming pattern (assuming they all end with '.csv')\n",
        "detailed_files = [f for f in detail_dir_path.iterdir() if f.is_file() and f.suffix == '.csv']\n",
        "\n",
        "# Process each detailed file\n",
        "for detailed_file in detailed_files:\n",
        "    # Extract units information from the filename\n",
        "    match = re.search(r'.*', detailed_file.stem, re.IGNORECASE)\n",
        "    if match:\n",
        "        units = match.group()\n",
        "\n",
        "        # Read the detailed file\n",
        "        detailed_df = pd.read_csv(detailed_file)\n",
        "\n",
        "        # Process the detailed file to match the summary schema\n",
        "        # Assuming 'Epoch 1' corresponds to 'Seconds per Epoch' in summary\n",
        "        for epoch in detailed_df['Epoch'].unique():\n",
        "          seconds_per_epoch = detailed_df.loc[detailed_df['Epoch'] == epoch, 'Time'].iloc[0]\n",
        "          # Get the Testing ROC-AUC and Training ROC-AUC for the first epoch\n",
        "          testing_roc_auc = detailed_df.loc[detailed_df['Epoch'] == epoch, 'Testing_ROC_AUC'].iloc[0]\n",
        "          training_roc_auc = detailed_df.loc[detailed_df['Epoch'] == epoch, 'Training_ROC_AUC'].iloc[0]\n",
        "\n",
        "          # Append to the summary dataframe\n",
        "          new_row = {'Dataset': 'Reduced Dataset',\n",
        "                    'Units/Layers': units,\n",
        "                    'Epoch': epoch,\n",
        "                    'Testing ROC-AUC': testing_roc_auc,\n",
        "                    'Training ROC-AUC': training_roc_auc,\n",
        "                    'Seconds per Epoch': seconds_per_epoch}\n",
        "\n",
        "          summary_df = summary_df.append(new_row, ignore_index=True)\n",
        "\n",
        "# Write the updated summary back to the data.csv file\n",
        "summary_df.to_csv(\"new_new_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnCBUBXKPvoa",
        "outputId": "6b591756-c3bf-4e00-bed2-0a810c12b6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "train_loss: tensor(0.7382, device='cuda:0') train_accuracy: 0.37806953561876977 train AUC-ROC: 0.4617215477334265 test loss: tensor(0.7503, device='cuda:0') test accuracy: 0.37806953561876977 test AUC-ROC: 0.45118133085705847\n",
            "epoch: 1\n",
            "train_loss: tensor(0.7363, device='cuda:0') train_accuracy: 0.37758327255044977 train AUC-ROC: 0.4620132762686697 test loss: tensor(0.7496, device='cuda:0') test accuracy: 0.37758327255044977 test AUC-ROC: 0.45102726716332864\n",
            "epoch: 2\n",
            "train_loss: tensor(0.7346, device='cuda:0') train_accuracy: 0.37515195720885 train AUC-ROC: 0.46147392785396113 test loss: tensor(0.7490, device='cuda:0') test accuracy: 0.37515195720885 test AUC-ROC: 0.45135016778169385\n",
            "epoch: 3\n",
            "train_loss: tensor(0.7329, device='cuda:0') train_accuracy: 0.37053245805981033 train AUC-ROC: 0.46111450781727664 test loss: tensor(0.7480, device='cuda:0') test accuracy: 0.37053245805981033 test AUC-ROC: 0.451640356245911\n",
            "epoch: 4\n",
            "train_loss: tensor(0.7313, device='cuda:0') train_accuracy: 0.3678580111840506 train AUC-ROC: 0.4599694296445105 test loss: tensor(0.7467, device='cuda:0') test accuracy: 0.3678580111840506 test AUC-ROC: 0.45225027963615644\n",
            "epoch: 5\n",
            "train_loss: tensor(0.7295, device='cuda:0') train_accuracy: 0.36712861658157064 train AUC-ROC: 0.4589169359769413 test loss: tensor(0.7452, device='cuda:0') test accuracy: 0.36712861658157064 test AUC-ROC: 0.45218907625097604\n",
            "epoch: 6\n",
            "train_loss: tensor(0.7276, device='cuda:0') train_accuracy: 0.3610503282275711 train AUC-ROC: 0.4585762948729147 test loss: tensor(0.7434, device='cuda:0') test accuracy: 0.3610503282275711 test AUC-ROC: 0.4522729670979043\n",
            "epoch: 7\n",
            "train_loss: tensor(0.7258, device='cuda:0') train_accuracy: 0.3571602236810114 train AUC-ROC: 0.45813782863132146 test loss: tensor(0.7418, device='cuda:0') test accuracy: 0.3571602236810114 test AUC-ROC: 0.4531208450288078\n",
            "epoch: 8\n",
            "train_loss: tensor(0.7238, device='cuda:0') train_accuracy: 0.35399951373693167 train AUC-ROC: 0.4585300026203162 test loss: tensor(0.7401, device='cuda:0') test accuracy: 0.35399951373693167 test AUC-ROC: 0.4537355169575586\n",
            "epoch: 9\n",
            "train_loss: tensor(0.7218, device='cuda:0') train_accuracy: 0.3522975929978118 train AUC-ROC: 0.4585963839636649 test loss: tensor(0.7384, device='cuda:0') test accuracy: 0.3522975929978118 test AUC-ROC: 0.4537787814194964\n",
            "epoch: 10\n",
            "train_loss: tensor(0.7200, device='cuda:0') train_accuracy: 0.3520544614636518 train AUC-ROC: 0.45841558214691236 test loss: tensor(0.7366, device='cuda:0') test accuracy: 0.3520544614636518 test AUC-ROC: 0.4539724162674377\n",
            "epoch: 11\n",
            "train_loss: tensor(0.7182, device='cuda:0') train_accuracy: 0.3518113299294919 train AUC-ROC: 0.45818761463883306 test loss: tensor(0.7349, device='cuda:0') test accuracy: 0.3518113299294919 test AUC-ROC: 0.45384631618935056\n",
            "epoch: 12\n",
            "train_loss: tensor(0.7164, device='cuda:0') train_accuracy: 0.34840748845125213 train AUC-ROC: 0.45796052056948205 test loss: tensor(0.7331, device='cuda:0') test accuracy: 0.34840748845125213 test AUC-ROC: 0.45377614334254907\n",
            "epoch: 13\n",
            "train_loss: tensor(0.7146, device='cuda:0') train_accuracy: 0.3435448577680525 train AUC-ROC: 0.45818499432264825 test loss: tensor(0.7315, device='cuda:0') test accuracy: 0.3435448577680525 test AUC-ROC: 0.45400249034463847\n",
            "epoch: 14\n",
            "train_loss: tensor(0.7127, device='cuda:0') train_accuracy: 0.33819596401653296 train AUC-ROC: 0.4588208577168311 test loss: tensor(0.7298, device='cuda:0') test accuracy: 0.33819596401653296 test AUC-ROC: 0.4541401979612941\n",
            "epoch: 15\n",
            "train_loss: tensor(0.7108, device='cuda:0') train_accuracy: 0.33309020179917337 train AUC-ROC: 0.45983229976417156 test loss: tensor(0.7275, device='cuda:0') test accuracy: 0.33309020179917337 test AUC-ROC: 0.45484456450625754\n",
            "epoch: 16\n",
            "train_loss: tensor(0.7088, device='cuda:0') train_accuracy: 0.3306588864575735 train AUC-ROC: 0.46112280548519524 test loss: tensor(0.7250, device='cuda:0') test accuracy: 0.3306588864575735 test AUC-ROC: 0.4557562838992888\n",
            "epoch: 17\n",
            "train_loss: tensor(0.7069, device='cuda:0') train_accuracy: 0.32847070265013373 train AUC-ROC: 0.4629076775264215 test loss: tensor(0.7225, device='cuda:0') test accuracy: 0.32847070265013373 test AUC-ROC: 0.45740508199143154\n",
            "epoch: 18\n",
            "train_loss: tensor(0.7053, device='cuda:0') train_accuracy: 0.32579625577437393 train AUC-ROC: 0.4631094418726527 test loss: tensor(0.7203, device='cuda:0') test accuracy: 0.32579625577437393 test AUC-ROC: 0.4583284089230315\n",
            "epoch: 19\n",
            "train_loss: tensor(0.7038, device='cuda:0') train_accuracy: 0.3238512035010941 train AUC-ROC: 0.4631832474451917 test loss: tensor(0.7181, device='cuda:0') test accuracy: 0.3238512035010941 test AUC-ROC: 0.45893411139016105\n",
            "epoch: 20\n",
            "train_loss: tensor(0.7024, device='cuda:0') train_accuracy: 0.3236080719669341 train AUC-ROC: 0.4630133636125426 test loss: tensor(0.7158, device='cuda:0') test accuracy: 0.3236080719669341 test AUC-ROC: 0.460170314247726\n",
            "epoch: 21\n",
            "train_loss: tensor(0.7011, device='cuda:0') train_accuracy: 0.32579625577437393 train AUC-ROC: 0.46269412175735875 test loss: tensor(0.7134, device='cuda:0') test accuracy: 0.32579625577437393 test AUC-ROC: 0.4615922377223899\n",
            "epoch: 22\n",
            "train_loss: tensor(0.7000, device='cuda:0') train_accuracy: 0.325309992706054 train AUC-ROC: 0.4612778408594637 test loss: tensor(0.7111, device='cuda:0') test accuracy: 0.325309992706054 test AUC-ROC: 0.46152997910643057\n",
            "epoch: 23\n",
            "train_loss: tensor(0.6992, device='cuda:0') train_accuracy: 0.32725504497933383 train AUC-ROC: 0.4596991003581099 test loss: tensor(0.7092, device='cuda:0') test accuracy: 0.32725504497933383 test AUC-ROC: 0.4601914188633054\n",
            "epoch: 24\n",
            "train_loss: tensor(0.6987, device='cuda:0') train_accuracy: 0.3299294918550936 train AUC-ROC: 0.457701545986549 test loss: tensor(0.7073, device='cuda:0') test accuracy: 0.3299294918550936 test AUC-ROC: 0.4569608298334845\n",
            "epoch: 25\n",
            "train_loss: tensor(0.6981, device='cuda:0') train_accuracy: 0.34548991004133234 train AUC-ROC: 0.45669097737793696 test loss: tensor(0.7047, device='cuda:0') test accuracy: 0.34548991004133234 test AUC-ROC: 0.4542641875778233\n",
            "epoch: 26\n",
            "train_loss: tensor(0.6974, device='cuda:0') train_accuracy: 0.37806953561876977 train AUC-ROC: 0.4578504672897197 test loss: tensor(0.7017, device='cuda:0') test accuracy: 0.37806953561876977 test AUC-ROC: 0.4508077791613025\n",
            "epoch: 27\n",
            "train_loss: tensor(0.6967, device='cuda:0') train_accuracy: 0.4403112083637248 train AUC-ROC: 0.4599502139924885 test loss: tensor(0.6982, device='cuda:0') test accuracy: 0.4403112083637248 test AUC-ROC: 0.4493262351476268\n",
            "epoch: 28\n",
            "train_loss: tensor(0.6962, device='cuda:0') train_accuracy: 0.5292973498662776 train AUC-ROC: 0.46067735173377583 test loss: tensor(0.6949, device='cuda:0') test accuracy: 0.5292973498662776 test AUC-ROC: 0.4507259987759323\n",
            "epoch: 29\n",
            "train_loss: tensor(0.6956, device='cuda:0') train_accuracy: 0.611232676878191 train AUC-ROC: 0.46288758843567124 test loss: tensor(0.6918, device='cuda:0') test accuracy: 0.611232676878191 test AUC-ROC: 0.4548519511217103\n",
            "epoch: 30\n",
            "train_loss: tensor(0.6951, device='cuda:0') train_accuracy: 0.6997325553124241 train AUC-ROC: 0.46495501790549393 test loss: tensor(0.6877, device='cuda:0') test accuracy: 0.6997325553124241 test AUC-ROC: 0.46135428318173183\n",
            "epoch: 31\n",
            "train_loss: tensor(0.6947, device='cuda:0') train_accuracy: 0.787989302212497 train AUC-ROC: 0.46798279325705305 test loss: tensor(0.6831, device='cuda:0') test accuracy: 0.787989302212497 test AUC-ROC: 0.46859527678703333\n",
            "epoch: 32\n",
            "train_loss: tensor(0.6943, device='cuda:0') train_accuracy: 0.8567955263797714 train AUC-ROC: 0.47116822429906546 test loss: tensor(0.6781, device='cuda:0') test accuracy: 0.8567955263797714 test AUC-ROC: 0.4783830698773822\n",
            "epoch: 33\n",
            "train_loss: tensor(0.6939, device='cuda:0') train_accuracy: 0.9015317286652079 train AUC-ROC: 0.4738169272425539 test loss: tensor(0.6734, device='cuda:0') test accuracy: 0.9015317286652079 test AUC-ROC: 0.4897136103665872\n",
            "epoch: 34\n",
            "train_loss: tensor(0.6936, device='cuda:0') train_accuracy: 0.9207391198638464 train AUC-ROC: 0.4776513232596733 test loss: tensor(0.6693, device='cuda:0') test accuracy: 0.9207391198638464 test AUC-ROC: 0.5012314543190595\n",
            "epoch: 35\n",
            "train_loss: tensor(0.6936, device='cuda:0') train_accuracy: 0.9277899343544858 train AUC-ROC: 0.47980522316359503 test loss: tensor(0.6643, device='cuda:0') test accuracy: 0.9277899343544858 test AUC-ROC: 0.5111005001793892\n",
            "epoch: 36\n",
            "train_loss: tensor(0.6938, device='cuda:0') train_accuracy: 0.9297349866277657 train AUC-ROC: 0.4813446589221766 test loss: tensor(0.6592, device='cuda:0') test accuracy: 0.9297349866277657 test AUC-ROC: 0.5180402253972944\n",
            "epoch: 37\n",
            "train_loss: tensor(0.6940, device='cuda:0') train_accuracy: 0.9331388281060053 train AUC-ROC: 0.4841890121407983 test loss: tensor(0.6523, device='cuda:0') test accuracy: 0.9331388281060053 test AUC-ROC: 0.5243832176096913\n",
            "epoch: 38\n",
            "train_loss: tensor(0.6943, device='cuda:0') train_accuracy: 0.9336250911743253 train AUC-ROC: 0.48810551139837544 test loss: tensor(0.6444, device='cuda:0') test accuracy: 0.9336250911743253 test AUC-ROC: 0.5313799252896609\n",
            "epoch: 39\n",
            "train_loss: tensor(0.6948, device='cuda:0') train_accuracy: 0.9358132749817651 train AUC-ROC: 0.4858904707834746 test loss: tensor(0.6376, device='cuda:0') test accuracy: 0.9358132749817651 test AUC-ROC: 0.5352932486334763\n",
            "epoch: 40\n",
            "train_loss: tensor(0.6953, device='cuda:0') train_accuracy: 0.936785801118405 train AUC-ROC: 0.48597650449820945 test loss: tensor(0.6315, device='cuda:0') test accuracy: 0.936785801118405 test AUC-ROC: 0.5395780132114895\n",
            "epoch: 41\n",
            "train_loss: tensor(0.6958, device='cuda:0') train_accuracy: 0.936785801118405 train AUC-ROC: 0.4847550004367194 test loss: tensor(0.6259, device='cuda:0') test accuracy: 0.936785801118405 test AUC-ROC: 0.5428708608572695\n",
            "epoch: 42\n",
            "train_loss: tensor(0.6964, device='cuda:0') train_accuracy: 0.936785801118405 train AUC-ROC: 0.484596908026902 test loss: tensor(0.6204, device='cuda:0') test accuracy: 0.936785801118405 test AUC-ROC: 0.546461811198109\n",
            "epoch: 43\n",
            "train_loss: tensor(0.6970, device='cuda:0') train_accuracy: 0.937515195720885 train AUC-ROC: 0.4867735173377588 test loss: tensor(0.6153, device='cuda:0') test accuracy: 0.937515195720885 test AUC-ROC: 0.5498169174598484\n",
            "epoch: 44\n",
            "train_loss: tensor(0.6976, device='cuda:0') train_accuracy: 0.937515195720885 train AUC-ROC: 0.48845969080269014 test loss: tensor(0.6102, device='cuda:0') test accuracy: 0.937515195720885 test AUC-ROC: 0.5526233037165228\n",
            "epoch: 45\n",
            "train_loss: tensor(0.6982, device='cuda:0') train_accuracy: 0.937515195720885 train AUC-ROC: 0.4900139750196524 test loss: tensor(0.6059, device='cuda:0') test accuracy: 0.937515195720885 test AUC-ROC: 0.5548809699681321\n",
            "epoch: 46\n",
            "train_loss: tensor(0.6987, device='cuda:0') train_accuracy: 0.9377583272550449 train AUC-ROC: 0.49159489911782683 test loss: tensor(0.6025, device='cuda:0') test accuracy: 0.9377583272550449 test AUC-ROC: 0.5572288584513433\n",
            "epoch: 47\n",
            "train_loss: tensor(0.6992, device='cuda:0') train_accuracy: 0.9384877218575249 train AUC-ROC: 0.4953860599178968 test loss: tensor(0.5993, device='cuda:0') test accuracy: 0.9384877218575249 test AUC-ROC: 0.5589473017748982\n",
            "epoch: 48\n",
            "train_loss: tensor(0.6996, device='cuda:0') train_accuracy: 0.9384877218575249 train AUC-ROC: 0.49699187701982706 test loss: tensor(0.5963, device='cuda:0') test accuracy: 0.9384877218575249 test AUC-ROC: 0.5566843593693941\n",
            "epoch: 49\n",
            "train_loss: tensor(0.7001, device='cuda:0') train_accuracy: 0.9384877218575249 train AUC-ROC: 0.49822473578478466 test loss: tensor(0.5936, device='cuda:0') test accuracy: 0.9384877218575249 test AUC-ROC: 0.5555563176666737\n"
          ]
        }
      ],
      "source": [
        "!python kstonet.py --data_name 'hiv_pca_resample' --regression_flag 0 --confidence_interval_flag 1 --layer 0 --unit 5 --sigma 0.01 --C 1 --epsilon 0.05 --lr 0.00005 --alpha 0.1 --nepoch 50 --model_path '5_unit_resampled/' --seed 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZu80t3UYZRs"
      },
      "outputs": [],
      "source": [
        "cross_validate_index = 0.2\n",
        "temp = pd.read_csv('./data/qsar/qsar_androgen_receptor.csv', sep=';', header=None)\n",
        "temp = np.mat(temp)\n",
        "x_data = temp[:, 0:-1].astype('float64')\n",
        "y_data = temp[:, -1]\n",
        "\n",
        "y_data = (y_data == 'positive')\n",
        "\n",
        "y_data = np.array(y_data.astype('int')).reshape(y_data.shape[0])\n",
        "\n",
        "permutation = np.random.choice(range(x_data.shape[0]), x_data.shape[0], replace=False)\n",
        "size_test = np.round(x_data.shape[0] * 0.2).astype(int)\n",
        "divid_index = np.arange(x_data.shape[0])\n",
        "\n",
        "lower_bound = cross_validate_index * size_test\n",
        "upper_bound = (cross_validate_index + 1) * size_test\n",
        "test_index = (divid_index >= lower_bound) * (divid_index < upper_bound)\n",
        "\n",
        "index_train = permutation[[not _ for _ in test_index]]\n",
        "index_test = permutation[test_index]\n",
        "\n",
        "x_train = x_data[index_train, :]\n",
        "y_train = y_data[index_train]\n",
        "\n",
        "x_test = x_data[index_test, :]\n",
        "y_test = y_data[index_test]\n",
        "\n",
        "x_train_std = np.std(x_train, 0)\n",
        "x_train_std[x_train_std == 0] = 1\n",
        "x_train_mean = np.mean(x_train, 0)\n",
        "\n",
        "x_train = (x_train - np.full(x_train.shape, x_train_mean)) / np.full(x_train.shape, x_train_std)\n",
        "\n",
        "x_test = (x_test - np.full(x_test.shape, x_train_mean)) / np.full(x_test.shape, x_train_std)\n",
        "\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test).to(device)\n",
        "y_test = torch.LongTensor(y_test).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the CSV data into a pandas DataFrame\n",
        "df = pd.read_csv('svm_vs_no_svm.csv')\n",
        "\n",
        "# Convert 'Seconds per Epoch' to cumulative seconds for each row\n",
        "df['Cumulative Seconds'] = df.groupby(['Run Name', 'Dataset'])['Seconds per Epoch'].cumsum()\n",
        "\n",
        "# Define the datasets we are interested in\n",
        "datasets = df['Dataset'].unique()\n",
        "units_layers = df['Run Name'].unique()\n",
        "\n",
        "# Create line plots for ROC-AUC vs epoch/total seconds elapsed\n",
        "for dataset in datasets:\n",
        "    dataset_df = df[df['Dataset'] == dataset]\n",
        "    \n",
        "    # Plot ROC-AUC vs Epoch for Testing and Training\n",
        "    for metric in ['Testing ROC-AUC', 'Training ROC-AUC']:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for units in units_layers:\n",
        "            units_df = dataset_df[dataset_df['Run Name'] == units]\n",
        "            if not units_df.empty:\n",
        "                plt.plot(units_df['Epoch'], units_df[metric], label=f'{units}', marker='o')\n",
        "        plt.title(f'{metric} vs Epoch for {dataset}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('ROC-AUC')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    \n",
        "    # Plot ROC-AUC vs Total Seconds for Testing and Training\n",
        "    for metric in ['Testing ROC-AUC', 'Training ROC-AUC']:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for units in units_layers:\n",
        "            units_df = dataset_df[dataset_df['Run Name'] == units]\n",
        "            if not units_df.empty:\n",
        "                plt.plot(units_df['Cumulative Seconds'], units_df[metric], label=f'{units}', marker='o')\n",
        "        plt.title(f'{metric} vs Total Seconds Elapsed for {dataset}')\n",
        "        plt.xlabel('Total Seconds Elapsed')\n",
        "        plt.ylabel('ROC-AUC')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the CSV data into a pandas DataFrame\n",
        "df = pd.read_csv('svm_vs_no_svm.csv')\n",
        "\n",
        "# Convert 'Seconds per Epoch' to cumulative seconds for each row\n",
        "df['Cumulative Seconds']=df.groupby(['Run Name', 'Dataset'])['Seconds per Epoch'].cumsum()\n",
        "\n",
        "# Define the datasets we are interested in\n",
        "datasets = df['Dataset'].unique()\n",
        "units_layers = df['Run Name'].unique()\n",
        "\n",
        "# Create line plots for ROC-AUC vs epoch/total seconds elapsed\n",
        "for dataset in datasets:\n",
        "    dataset_df = df[df['Dataset'] == dataset]\n",
        "\n",
        "    # Plot ROC-AUC vs Epoch for Testing and Training\n",
        "    for metric in ['Testing ROC-AUC', 'Training ROC-AUC']:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for units in units_layers:\n",
        "            units_df = dataset_df[dataset_df['Run Name'] == units]\n",
        "            if not units_df.empty:\n",
        "                plt.plot(units_df['Epoch'], units_df[metric], label=f'{units}', marker='o')\n",
        "        \n",
        "        plt.title(f'{metric} vs Epoch for {dataset}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('ROC-AUC')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.ylim(0.65, 0.75)  # Set y-axis range\n",
        "        plt.show()\n",
        "\n",
        "    # Plot ROC-AUC vs Total Seconds for Testing and Training\n",
        "    for metric in ['Testing ROC-AUC', 'Training ROC-AUC']:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for units in units_layers:\n",
        "            units_df = dataset_df[dataset_df['Run Name'] == units]\n",
        "            if not units_df.empty:\n",
        "                plt.plot(units_df['Cumulative Seconds'], units_df[metric], label=f'{units}', marker='o')\n",
        "        \n",
        "        plt.title(f'{metric} vs Total Seconds Elapsed for {dataset}')\n",
        "        plt.xlabel('Total Seconds Elapsed')\n",
        "        plt.ylabel('ROC-AUC')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.ylim(0.65, 0.75)  # Set y-axis range\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "\n",
        "# Read the CSV data into a pandas DataFrame\n",
        "df = pd.read_csv('varying_svr_100_epoch_no_0.csv')\n",
        "\n",
        "# Function to extract C and Epsilon from run name\n",
        "def extract_params(run_name):\n",
        "    parts = run_name.split('_')\n",
        "    c = float(parts[parts.index('C') + 1])\n",
        "    epsilon = float(parts[parts.index('Epsilon') + 1])\n",
        "    return c, epsilon\n",
        "\n",
        "# Extracting unique C and Epsilon values\n",
        "params = df['Run Name'].apply(extract_params)\n",
        "df['C'], df['Epsilon'] = zip(*params)\n",
        "unique_cs = df['C'].unique()\n",
        "unique_epsilons = df['Epsilon'].unique()\n",
        "\n",
        "# Prepare the data for the 3D plot\n",
        "# We will create an array to hold the maximum ROC-AUC for each C and Epsilon\n",
        "max_auc = np.zeros((len(unique_cs), len(unique_epsilons)))\n",
        "\n",
        "# Iterate over each C and Epsilon to get the highest ROC-AUC\n",
        "for i, c in enumerate(unique_cs):\n",
        "    for j, epsilon in enumerate(unique_epsilons):\n",
        "        subset_df = df[(df['C'] == c) & (df['Epsilon'] == epsilon)]\n",
        "        max_auc[i, j] = subset_df['Testing ROC-AUC'].max()  # or 'Training ROC-AUC'\n",
        "\n",
        "# Logarithmic scale values for C and Epsilon\n",
        "log_cs = np.log10(unique_cs)\n",
        "log_epsilons = np.log10(unique_epsilons)\n",
        "\n",
        "# Meshgrid for the 3D plot axis\n",
        "C, Epsilon = np.meshgrid(log_cs, log_epsilons)\n",
        "\n",
        "# Create the 3D plot\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Make the 3D plot with log scales for x and y axis\n",
        "ax.plot_surface(C, Epsilon, max_auc.T, cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('log10(C)')\n",
        "ax.set_ylabel('log10(Epsilon)')\n",
        "ax.set_zlabel('Max ROC-AUC')\n",
        "ax.set_title('Max ROC-AUC vs C and Epsilon')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DpI4XqTC9BgT",
        "b-kI-NSp1YTg",
        "Ej1sGX-19HZl",
        "W1ha6XGg1fNl"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
